{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import dspy\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the DSPy classifier program.\n",
    "\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "import dspy\n",
    "\n",
    "# Signature for the NLI task\n",
    "class NLISignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Classify the relationship between the premise and hypothesis \n",
    "    to a label: entailment, neutral or contradiction.\n",
    "    \"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "# A class for Parallel processing with progress display\n",
    "class NLIClassifier(dspy.Module):\n",
    "    def __init__(self, predictor_module: dspy.Module, batch_size: int = 20, num_threads: int = 8):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor_module  # Predict, ChainOfThought, etc.\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    def forward(self, examples: dspy.Example) -> list[dspy.Prediction]:\n",
    "        # Display progress with tqdm while processing\n",
    "        results = []\n",
    "        for i in tqdm(range(0, len(examples), self.batch_size), desc=\"Processing\"):\n",
    "            sub_batch = examples[i:i + self.batch_size]\n",
    "            processed = self.predictor.batch( # perform batch processing\n",
    "                sub_batch,\n",
    "                num_threads=self.num_threads\n",
    "            )\n",
    "            results.extend(processed)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": [
    "first we will optimize the model on a train set from \"dev_r3\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf5a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 40\n"
     ]
    }
   ],
   "source": [
    "# prepare the training set\n",
    "import random \n",
    "\n",
    "preprocessed_examples = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['dev_r3']  # Use the 'dev_r3' split for training\n",
    "]\n",
    "\n",
    "train_set_size = 40 # tradeoff between quality and speed after testing, permitted range is 20-100\n",
    "trainset = random.sample(preprocessed_examples, train_set_size)  # pick examples randomly for training to avoid bias\n",
    "print(f\"Total examples: {len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ebc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:37<03:35,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the optimization using few-shot learning - only in task 1.4 we will use CoT\n",
    "\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def exact_match(example, pred, trace=None):\n",
    "    # Ensure both labels are strings and lowercase\n",
    "    ex_label = str(example.label).strip().lower()\n",
    "    pred_label = str(pred.label).strip().lower()\n",
    "\n",
    "    # In case example.label is already an int, use reverse mapping\n",
    "    if ex_label.isdigit():\n",
    "        id2label = {v: k for k, v in label2id.items()}\n",
    "        ex_label = id2label[int(ex_label)]\n",
    "\n",
    "    return label2id.get(pred_label) == label2id.get(ex_label)\n",
    "\n",
    "def compute_metrics(preds, golds):\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=golds)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=golds, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=golds, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=golds, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "model_simple = dspy.Predict(NLISignature)\n",
    "bootstrap = BootstrapFewShot(metric=exact_match)\n",
    "optimized_bootstrap = bootstrap.compile(student=model_simple, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16973a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:19<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 1/60 [00:19<18:45, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 2/60 [00:33<16:01, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 3/60 [00:47<14:25, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 4/60 [00:59<13:00, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 5/60 [01:14<12:59, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 6/60 [01:26<12:19, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 7/60 [01:37<11:13, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 8/60 [01:51<11:30, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 9/60 [02:06<11:34, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 10/60 [02:19<11:18, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 11/60 [02:36<11:53, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 12/60 [02:49<11:07, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 13/60 [03:00<10:23, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 14/60 [03:13<09:56, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:23<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 15/60 [03:36<12:02, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# run the optimized model on 'test_r3' split\n",
    "testset_with_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "testset_no_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "\n",
    "program = NLIClassifier(optimized_bootstrap)\n",
    "predictions = program(testset_no_labels)\n",
    "pred_labels = [label2id[pred.label] for pred in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc417aa",
   "metadata": {},
   "source": [
    "# TASK 1.3 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92357a28",
   "metadata": {},
   "source": [
    "a) Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44ae6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset_with_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# use compute_metrics to evaluate the model and print the results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m gold_labels = [ex.label \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtestset_with_labels\u001b[49m]\n\u001b[32m      3\u001b[39m metrics = compute_metrics(pred_labels, gold_labels)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Print the metrics\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'testset_with_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# use compute_metrics to evaluate the model and print the results\n",
    "gold_labels = [ex.label for ex in testset_with_labels]\n",
    "metrics = compute_metrics(pred_labels, gold_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print(\"Model scores:\")\n",
    "print(f\"F1 score: {metrics['f1']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642731ac",
   "metadata": {},
   "source": [
    "Compare the results with the baseline and provide agreement metrics between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b97ccf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models are correct on 441 out of 1200 samples.\n",
      "Both models are correct on 36.75% of the samples.\n",
      "\n",
      "\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 419 out of 1200 samples.\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 34.92% of the samples.\n",
      "\n",
      "\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 136 out of 1200 samples.\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 11.33% of the samples.\n",
      "\n",
      "\n",
      "Both models are incorrect on 204 out of 1200 samples.\n",
      "Both models are incorrect on 17.00% of the samples.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many samples they are both correct\n",
    "%store -r pred_test_r3\n",
    "optimized_llm_predictions = predictions\n",
    "non_llm_predictions = pred_test_r3 # DeBERTa_v3_predictions\n",
    "TEST_SIZE = len(optimized_llm_predictions)\n",
    "# gold_labels = [label2id[row['gold_label']] for row in pred_test_r3]\n",
    "\n",
    "\n",
    "# on how many samples both models are correct\n",
    "correct_predictions = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] == row['pred_label'])\n",
    ")\n",
    "\n",
    "print(f\"Both models are correct on {correct_predictions} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are correct on {correct_predictions / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples llm is correct and DeBERTa_v3_ is incorrect\n",
    "llm_correct_deberta_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] != row['pred_label'])\n",
    ")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples DeBERTa_v3 is correct and llm is incorrect\n",
    "deberta_correct_llm_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (row['pred_label'] == row['gold_label']) and (llm_pred.label != row['gold_label'])\n",
    ")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# on how many samples both models are incorrect\n",
    "both_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label != row['gold_label']) and (row['pred_label'] != row['gold_label'])\n",
    ")\n",
    "print(f\"Both models are incorrect on {both_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are incorrect on {both_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
