{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9b84927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class BatchedNLIPredictor(dspy.Module):\n",
    "    def __init__(self, model, batch_size=15):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, examples):\n",
    "        # examples: list of dspy.Example(premise=..., hypothesis=...)\n",
    "        \n",
    "        # Build a single prompt with multiple pairs\n",
    "        prompt = \"Choose the correct relationship between the hypothesis and the premise: entailment/neutral/contradiction  .\\n\\n\"\n",
    "        for i, ex in enumerate(examples, start=1):\n",
    "            prompt += f\"Example {i}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "        # Single LLM call\n",
    "        response = self.model(prompt)\n",
    "\n",
    "        # Parse the response line by line\n",
    "        predictions = []\n",
    "        for line in response.splitlines():\n",
    "            if line.strip().lower().startswith((\"entailment\", \"neutral\", \"contradiction\")):\n",
    "                predictions.append(line.strip().lower())\n",
    "\n",
    "        # Ensure same length\n",
    "        while len(predictions) < len(examples):\n",
    "            predictions.append(\"unknown\")\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b262f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "batched_predictor = BatchedNLIPredictor(model=lm, batch_size=15)\n",
    "\n",
    "def process_chunk(chunk_examples, chunk_uids, batch_index):\n",
    "    # Build single prompt for this batch\n",
    "    prompt = \"Classify the hypothesis and premise relationship: entailment / neutral / contradiction. **provide 1 word answer**.\\n\\n\"\n",
    "    for idx, ex in enumerate(chunk_examples, start=1):\n",
    "        prompt += f\"Example {idx}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "    # LLM call (one API call per chunk)\n",
    "    print(f\"Processing batch {batch_index} with {len(chunk_examples)} examples\")\n",
    "    response = batched_predictor.model(prompt)\n",
    "    print(f\"Done with batch {batch_index}\")\n",
    "    \n",
    "\n",
    "    # Handle response text\n",
    "    response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "    lines = response_text.splitlines()\n",
    "\n",
    "    # Extract predictions\n",
    "    predictions = []\n",
    "    for line in lines:\n",
    "        m = re.search(r\"(entailment|neutral|contradiction)\", line, re.IGNORECASE)\n",
    "        if m:\n",
    "            predictions.append(m.group(1).lower())\n",
    "\n",
    "    # Pad missing\n",
    "    while len(predictions) < len(chunk_examples):\n",
    "        predictions.append(\"unknown\")\n",
    "\n",
    "    return batch_index, list(zip(chunk_uids, predictions))\n",
    "\n",
    "\n",
    "def predict_batch_parallel(batch, batch_size=15, max_workers=6):\n",
    "    uids = list(batch[\"uid\"])\n",
    "    premises = list(batch[\"premise\"])\n",
    "    hypotheses = list(batch[\"hypothesis\"])\n",
    "\n",
    "    examples = [\n",
    "        dspy.Example(premise=p, hypothesis=h)\n",
    "        for p, h in zip(premises, hypotheses)\n",
    "    ]\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(examples), batch_size):\n",
    "            chunk = examples[i:i+batch_size]\n",
    "            chunk_uids = uids[i:i+batch_size]\n",
    "            batch_index = i // batch_size\n",
    "            futures.append(executor.submit(process_chunk, chunk, chunk_uids, batch_index))\n",
    "\n",
    "        # Collect results in order\n",
    "        results_by_index = {}\n",
    "        for future in as_completed(futures):\n",
    "            batch_index, batch_result = future.result()\n",
    "            results_by_index[batch_index] = batch_result\n",
    "\n",
    "    # Flatten results in original order\n",
    "    ordered_results = []\n",
    "    for batch_index in sorted(results_by_index.keys()):\n",
    "        ordered_results.extend(results_by_index[batch_index])\n",
    "\n",
    "    return ordered_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with 15 examples\n",
      "Done with batch 0\n",
      "Processing batch 1 with 15 examples\n",
      "Done with batch 1\n",
      "Processing batch 2 with 15 examples\n",
      "Done with batch 2\n",
      "Processing batch 3 with 15 examples\n",
      "Processing batch 4 with 15 examples\n",
      "Processing batch 5 with 15 examples\n",
      "Done with batch 3\n",
      "Done with batch 4\n",
      "Done with batch 5\n",
      "Processing batch 8 with 15 examples\n",
      "Processing batch 6 with 15 examples\n",
      "Processing batch 7 with 15 examples\n",
      "Processing batch 9 with 15 examples\n",
      "Processing batch 10 with 15 examples\n",
      "Processing batch 11 with 15 examples\n",
      "Done with batch 8\n",
      "Processing batch 12 with 15 examples\n",
      "Done with batch 6\n",
      "Processing batch 13 with 15 examples\n",
      "Done with batch 11\n",
      "Processing batch 14 with 15 examples\n",
      "Done with batch 9\n",
      "Processing batch 15 with 15 examples\n",
      "Done with batch 10\n",
      "Processing batch 16 with 15 examples\n",
      "Done with batch 7\n",
      "Processing batch 17 with 15 examples\n",
      "Done with batch 14\n",
      "Processing batch 18 with 15 examples\n",
      "Done with batch 12\n",
      "Processing batch 19 with 15 examples\n",
      "Done with batch 16\n",
      "Processing batch 20 with 15 examples\n",
      "Done with batch 13\n",
      "Processing batch 21 with 15 examples\n",
      "Done with batch 17\n",
      "Processing batch 22 with 15 examples\n",
      "Done with batch 15\n",
      "Processing batch 23 with 15 examples\n",
      "Done with batch 18\n",
      "Processing batch 24 with 15 examples\n",
      "Done with batch 23\n",
      "Processing batch 25 with 15 examples\n",
      "Done with batch 19\n",
      "Processing batch 26 with 15 examples\n",
      "Done with batch 22\n",
      "Processing batch 27 with 15 examples\n",
      "Done with batch 21\n",
      "Processing batch 28 with 15 examples\n",
      "Done with batch 20\n",
      "Processing batch 29 with 15 examples\n",
      "Done with batch 24\n",
      "Processing batch 30 with 15 examples\n",
      "Done with batch 25\n",
      "Processing batch 31 with 15 examples\n",
      "Done with batch 26\n",
      "Processing batch 32 with 15 examples\n",
      "Done with batch 27\n",
      "Processing batch 33 with 15 examples\n",
      "Done with batch 28\n",
      "Processing batch 34 with 15 examples\n",
      "Done with batch 29\n",
      "Processing batch 35 with 15 examples\n",
      "Done with batch 30\n",
      "Processing batch 36 with 15 examples\n",
      "Done with batch 32\n",
      "Processing batch 37 with 15 examples\n",
      "Done with batch 31\n",
      "Processing batch 38 with 15 examples\n",
      "Done with batch 33\n",
      "Processing batch 39 with 15 examples\n",
      "Done with batch 34\n",
      "Processing batch 40 with 15 examples\n",
      "Done with batch 35\n",
      "Processing batch 41 with 15 examples\n",
      "Done with batch 36\n",
      "Processing batch 42 with 15 examples\n",
      "Done with batch 38\n",
      "Processing batch 43 with 15 examples\n",
      "Done with batch 37\n",
      "Processing batch 44 with 15 examples\n",
      "Done with batch 39\n",
      "Processing batch 45 with 15 examples\n",
      "Done with batch 41\n",
      "Processing batch 46 with 15 examples\n",
      "Done with batch 42\n",
      "Processing batch 47 with 15 examples\n",
      "Done with batch 40\n",
      "Processing batch 48 with 15 examples\n",
      "Done with batch 43\n",
      "Processing batch 49 with 15 examples\n",
      "Done with batch 44\n",
      "Processing batch 50 with 15 examples\n",
      "Done with batch 45\n",
      "Processing batch 51 with 15 examples\n",
      "Done with batch 49\n",
      "Processing batch 52 with 15 examples\n",
      "Done with batch 48\n",
      "Processing batch 53 with 15 examples\n",
      "Done with batch 50\n",
      "Processing batch 54 with 15 examples\n",
      "Done with batch 47\n",
      "Processing batch 55 with 15 examples\n",
      "Done with batch 51\n",
      "Processing batch 56 with 15 examples\n",
      "Done with batch 52\n",
      "Processing batch 57 with 15 examples\n",
      "Done with batch 54\n",
      "Processing batch 58 with 15 examples\n",
      "Done with batch 53\n",
      "Processing batch 59 with 15 examples\n",
      "Done with batch 55\n",
      "Processing batch 60 with 15 examples\n",
      "Done with batch 56\n",
      "Processing batch 61 with 15 examples\n",
      "Done with batch 57\n",
      "Processing batch 62 with 15 examples\n",
      "Done with batch 58\n",
      "Processing batch 63 with 15 examples\n",
      "Done with batch 59\n",
      "Processing batch 64 with 15 examples\n",
      "Done with batch 60\n",
      "Processing batch 65 with 15 examples\n",
      "Done with batch 61\n",
      "Processing batch 66 with 15 examples\n",
      "Done with batch 62\n",
      "Processing batch 67 with 15 examples\n",
      "Done with batch 63\n",
      "Processing batch 68 with 15 examples\n",
      "Done with batch 65\n",
      "Processing batch 69 with 15 examples\n",
      "Done with batch 66\n",
      "Processing batch 70 with 15 examples\n",
      "Done with batch 67\n",
      "Processing batch 71 with 15 examples\n",
      "Done with batch 64\n",
      "Processing batch 72 with 15 examples\n",
      "Done with batch 68\n",
      "Processing batch 73 with 15 examples\n",
      "Done with batch 69\n",
      "Processing batch 74 with 15 examples\n",
      "Done with batch 70\n",
      "Processing batch 75 with 15 examples\n",
      "Done with batch 71\n",
      "Processing batch 76 with 15 examples\n",
      "Done with batch 72\n",
      "Processing batch 77 with 15 examples\n",
      "Done with batch 74\n",
      "Processing batch 78 with 15 examples\n",
      "Done with batch 73\n",
      "Processing batch 79 with 15 examples\n",
      "Done with batch 75\n",
      "Done with batch 76\n",
      "Done with batch 77\n",
      "Done with batch 78\n",
      "Done with batch 79\n",
      "Done with batch 46\n"
     ]
    }
   ],
   "source": [
    "test_r3 = dataset['test_r3']\n",
    "ordered_llm_predictions = predict_batch_parallel(test_r3, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6775\n",
      "Precision: 0.6859799390313412\n",
      "Recall: 0.6775\n",
      "F1: 0.6805853334720283\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metrics from the `evaluate` library\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "# Define mapping from string labels to integer IDs\n",
    "# Defined in Cell 11 in anli_baseline.ipynb\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "# Extract predicted labels from ordered_llm_predictions (second element in each tuple)\n",
    "predicted_labels = [label2id[label.lower()] for uid, label in ordered_llm_predictions]\n",
    "\n",
    "# Extract gold labels from test_r3, slice to match predictions length to be safe\n",
    "gold_labels = test_r3['label'][:len(predicted_labels)]\n",
    "\n",
    "\n",
    "# Compute all metrics using integer IDs for predicted and gold labels\n",
    "acc_result = accuracy.compute(predictions=predicted_labels, references=gold_labels)\n",
    "prec_result = precision.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "rec_result = recall.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "f1_result = f1.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "\n",
    "# Print out the evaluation results\n",
    "print(\"Accuracy:\", acc_result[\"accuracy\"])\n",
    "print(\"Precision:\", prec_result[\"precision\"])\n",
    "print(\"Recall:\", rec_result[\"recall\"])\n",
    "print(\"F1:\", f1_result[\"f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38bc89cb",
   "metadata": {},
   "source": [
    "# NOTE FOR MICHAEL: \n",
    "\n",
    "1) in the assignment instructions on git , it says \"Evaluate the model on the \"test_r3\" partition of the ANLI dataset\". not on EACH test parition.\n",
    "So we will compare only to \"test_r3\" parition.\n",
    "\n",
    "2) we define DeBERTa baseline model as \"Model 1\" and the LLM baseline model as \"Model 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62ba011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pred_test_r3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51429f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "842653bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models correct on 433 samples (36.08%).\n",
      "Only Model 1 correct on 144 samples (12.00%).\n",
      "Only Model 2 correct on 380 samples (31.67%).\n",
      "Both models incorrect on 243 samples (20.25%).\n"
     ]
    }
   ],
   "source": [
    "# Model 1 predictions and gold labels (from pred_test_r3)\n",
    "model1_preds = pred_test_r3[:len(ordered_llm_predictions)]\n",
    "\n",
    "# Model 2 predictions from ordered_llm_predictions\n",
    "model2_preds = ordered_llm_predictions\n",
    "\n",
    "n = len(model2_preds)\n",
    "\n",
    "both_correct = 0\n",
    "only_model1_correct = 0\n",
    "only_model2_correct = 0\n",
    "both_incorrect = 0\n",
    "\n",
    "for example, (uid, pred2) in zip(model1_preds, model2_preds):\n",
    "    gold = example['gold_label']\n",
    "    pred1 = example['pred_label']\n",
    "    \n",
    "    model1_correct = (pred1 == gold)\n",
    "    model2_correct = (pred2 == gold)\n",
    "    \n",
    "    if model1_correct and model2_correct:\n",
    "        both_correct += 1\n",
    "    elif model1_correct and not model2_correct:\n",
    "        only_model1_correct += 1\n",
    "    elif not model1_correct and model2_correct:\n",
    "        only_model2_correct += 1\n",
    "    else:\n",
    "        both_incorrect += 1\n",
    "\n",
    "print(f\"Both models correct on {both_correct} samples ({both_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 1 correct on {only_model1_correct} samples ({only_model1_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 2 correct on {only_model2_correct} samples ({only_model2_correct / n * 100:.2f}%).\")\n",
    "print(f\"Both models incorrect on {both_incorrect} samples ({both_incorrect / n * 100:.2f}%).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
