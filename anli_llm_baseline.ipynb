{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1aa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class BatchedNLIPredictor(dspy.Module):\n",
    "    \"\"\"\n",
    "    DSPy program to classify NLI examples in batches using a language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, batch_size=15, max_workers=6):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def _process_chunk(self, chunk_examples, batch_index):\n",
    "        \"\"\"Helper to process a single chunk of examples.\"\"\"\n",
    "        print(f\"[Batch {batch_index}] Starting processing with {len(chunk_examples)} examples\")\n",
    "\n",
    "        # Build a single prompt for the chunk\n",
    "        prompt = (\n",
    "            \"Classify the relationship between the hypothesis and premise: \"\n",
    "            \"entailment / neutral / contradiction. **Provide a one-word answer**.\\n\\n\"\n",
    "        )\n",
    "        for idx, ex in enumerate(chunk_examples, start=1):\n",
    "            prompt += (\n",
    "                f\"Example {idx}:\\n\"\n",
    "                f\"Premise: {ex.premise}\\n\"\n",
    "                f\"Hypothesis: {ex.hypothesis}\\n\"\n",
    "                f\"Answer (entailment/neutral/contradiction):\\n\"\n",
    "            )\n",
    "\n",
    "        # Single LLM call\n",
    "        response = self.model(prompt)\n",
    "        response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "\n",
    "        # Extract predictions\n",
    "        predictions = []\n",
    "        for line in response_text.splitlines():\n",
    "            m = re.search(r\"(entailment|neutral|contradiction)\", line, re.IGNORECASE)\n",
    "            if m:\n",
    "                predictions.append(m.group(1).lower())\n",
    "\n",
    "        # Pad any missing predictions\n",
    "        while len(predictions) < len(chunk_examples):\n",
    "            predictions.append(\"unknown\")\n",
    "\n",
    "        print(f\"[Batch {batch_index}] Finished processing\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, examples):\n",
    "        \"\"\"\n",
    "        Main pipeline: \n",
    "        - Splits examples into batches\n",
    "        - Runs them in parallel\n",
    "        - Returns predictions in the original order\n",
    "        \"\"\"\n",
    "        if not examples:\n",
    "            return []\n",
    "\n",
    "        # Split into batches\n",
    "        chunks = [\n",
    "            examples[i:i+self.batch_size]\n",
    "            for i in range(0, len(examples), self.batch_size)\n",
    "        ]\n",
    "\n",
    "        results_by_index = {}\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit each batch to executor\n",
    "            futures = {\n",
    "                executor.submit(self._process_chunk, chunk, idx): idx\n",
    "                for idx, chunk in enumerate(chunks)\n",
    "            }\n",
    "\n",
    "            # Collect results\n",
    "            for future in as_completed(futures):\n",
    "                batch_index = futures[future]\n",
    "                predictions = future.result()\n",
    "                results_by_index[batch_index] = predictions\n",
    "\n",
    "        # Flatten results in original order\n",
    "        ordered_predictions = []\n",
    "        for idx in sorted(results_by_index.keys()):\n",
    "            ordered_predictions.extend(results_by_index[idx])\n",
    "\n",
    "        return ordered_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] Starting processing with 15 examples\n",
      "[Batch 0] Finished processing\n",
      "[Batch 1] Starting processing with 15 examples\n",
      "[Batch 1] Finished processing\n",
      "[Batch 2] Starting processing with 15 examples\n",
      "[Batch 2] Finished processing\n",
      "[Batch 3] Starting processing with 15 examples\n",
      "[Batch 4] Starting processing with 15 examples\n",
      "[Batch 3] Finished processing\n",
      "[Batch 5] Starting processing with 15 examples\n",
      "[Batch 4] Finished processing\n",
      "[Batch 5] Finished processing\n",
      "[Batch 6] Starting processing with 15 examples\n",
      "[Batch 7] Starting processing with 15 examples\n",
      "[Batch 8] Starting processing with 15 examples\n",
      "[Batch 8] Finished processing\n",
      "[Batch 9] Starting processing with 15 examples\n",
      "[Batch 9] Finished processing\n",
      "[Batch 7] Finished processing\n",
      "[Batch 6] Finished processing\n",
      "[Batch 10] Starting processing with 15 examples\n",
      "[Batch 12] Starting processing with 15 examples\n",
      "[Batch 13] Starting processing with 15 examples\n",
      "[Batch 11] Starting processing with 15 examples\n",
      "[Batch 14] Starting processing with 15 examples\n",
      "[Batch 15] Starting processing with 15 examples\n",
      "[Batch 15] Finished processing\n",
      "[Batch 16] Starting processing with 15 examples\n",
      "[Batch 10] Finished processing\n",
      "[Batch 17] Starting processing with 15 examples\n",
      "[Batch 11] Finished processing\n",
      "[Batch 18] Starting processing with 15 examples\n",
      "[Batch 14] Finished processing\n",
      "[Batch 19] Starting processing with 15 examples\n",
      "[Batch 13] Finished processing\n",
      "[Batch 20] Starting processing with 15 examples\n",
      "[Batch 12] Finished processing\n",
      "[Batch 21] Starting processing with 15 examples\n",
      "[Batch 17] Finished processing\n",
      "[Batch 22] Starting processing with 15 examples\n",
      "[Batch 18] Finished processing\n",
      "[Batch 23] Starting processing with 15 examples\n",
      "[Batch 19] Finished processing\n",
      "[Batch 24] Starting processing with 15 examples\n",
      "[Batch 21] Finished processing\n",
      "[Batch 25] Starting processing with 15 examples\n",
      "[Batch 20] Finished processing\n",
      "[Batch 26] Starting processing with 15 examples\n",
      "[Batch 22] Finished processing\n",
      "[Batch 27] Starting processing with 15 examples\n",
      "[Batch 16] Finished processing\n",
      "[Batch 28] Starting processing with 15 examples\n",
      "[Batch 23] Finished processing\n",
      "[Batch 29] Starting processing with 15 examples\n",
      "[Batch 24] Finished processing\n",
      "[Batch 30] Starting processing with 15 examples\n",
      "[Batch 25] Finished processing\n",
      "[Batch 31] Starting processing with 15 examples\n",
      "[Batch 26] Finished processing\n",
      "[Batch 32] Starting processing with 15 examples\n",
      "[Batch 31] Finished processing\n",
      "[Batch 33] Starting processing with 15 examples\n",
      "[Batch 27] Finished processing\n",
      "[Batch 34] Starting processing with 15 examples\n",
      "[Batch 28] Finished processing\n",
      "[Batch 35] Starting processing with 15 examples\n",
      "[Batch 30] Finished processing\n",
      "[Batch 36] Starting processing with 15 examples\n",
      "[Batch 32] Finished processing\n",
      "[Batch 37] Starting processing with 15 examples\n",
      "[Batch 29] Finished processing\n",
      "[Batch 38] Starting processing with 15 examples\n",
      "[Batch 33] Finished processing\n",
      "[Batch 39] Starting processing with 15 examples\n",
      "[Batch 35] Finished processing\n",
      "[Batch 40] Starting processing with 15 examples\n",
      "[Batch 34] Finished processing\n",
      "[Batch 41] Starting processing with 15 examples\n",
      "[Batch 36] Finished processing\n",
      "[Batch 42] Starting processing with 15 examples\n",
      "[Batch 37] Finished processing\n",
      "[Batch 43] Starting processing with 15 examples\n",
      "[Batch 39] Finished processing\n",
      "[Batch 44] Starting processing with 15 examples\n",
      "[Batch 41] Finished processing\n",
      "[Batch 45] Starting processing with 15 examples\n",
      "[Batch 42] Finished processing\n",
      "[Batch 46] Starting processing with 15 examples\n",
      "[Batch 38] Finished processing\n",
      "[Batch 47] Starting processing with 15 examples\n",
      "[Batch 43] Finished processing\n",
      "[Batch 48] Starting processing with 15 examples\n",
      "[Batch 40] Finished processing\n",
      "[Batch 49] Starting processing with 15 examples\n",
      "[Batch 46] Finished processing\n",
      "[Batch 50] Starting processing with 15 examples\n",
      "[Batch 45] Finished processing\n",
      "[Batch 51] Starting processing with 15 examples\n",
      "[Batch 47] Finished processing\n",
      "[Batch 52] Starting processing with 15 examples\n",
      "[Batch 49] Finished processing\n",
      "[Batch 53] Starting processing with 15 examples\n",
      "[Batch 44] Finished processing\n",
      "[Batch 54] Starting processing with 15 examples\n",
      "[Batch 48] Finished processing\n",
      "[Batch 55] Starting processing with 15 examples\n",
      "[Batch 50] Finished processing\n",
      "[Batch 56] Starting processing with 15 examples\n",
      "[Batch 52] Finished processing\n",
      "[Batch 57] Starting processing with 15 examples\n",
      "[Batch 53] Finished processing\n",
      "[Batch 58] Starting processing with 15 examples\n",
      "[Batch 54] Finished processing\n",
      "[Batch 59] Starting processing with 15 examples\n",
      "[Batch 51] Finished processing\n",
      "[Batch 60] Starting processing with 15 examples\n",
      "[Batch 55] Finished processing\n",
      "[Batch 61] Starting processing with 15 examples\n",
      "[Batch 56] Finished processing\n",
      "[Batch 62] Starting processing with 15 examples\n",
      "[Batch 57] Finished processing\n",
      "[Batch 63] Starting processing with 15 examples\n",
      "[Batch 59] Finished processing\n",
      "[Batch 64] Starting processing with 15 examples\n",
      "[Batch 60] Finished processing\n",
      "[Batch 65] Starting processing with 15 examples\n",
      "[Batch 61] Finished processing\n",
      "[Batch 66] Starting processing with 15 examples\n",
      "[Batch 58] Finished processing\n",
      "[Batch 67] Starting processing with 15 examples\n",
      "[Batch 63] Finished processing\n",
      "[Batch 68] Starting processing with 15 examples\n",
      "[Batch 65] Finished processing\n",
      "[Batch 69] Starting processing with 15 examples\n",
      "[Batch 64] Finished processing\n",
      "[Batch 70] Starting processing with 15 examples\n",
      "[Batch 67] Finished processing\n",
      "[Batch 71] Starting processing with 15 examples\n",
      "[Batch 66] Finished processing\n",
      "[Batch 72] Starting processing with 15 examples\n",
      "[Batch 62] Finished processing\n",
      "[Batch 73] Starting processing with 15 examples\n",
      "[Batch 68] Finished processing\n",
      "[Batch 74] Starting processing with 15 examples\n",
      "[Batch 71] Finished processing\n",
      "[Batch 75] Starting processing with 15 examples\n",
      "[Batch 70] Finished processing\n",
      "[Batch 76] Starting processing with 15 examples\n",
      "[Batch 72] Finished processing\n",
      "[Batch 77] Starting processing with 15 examples\n",
      "[Batch 69] Finished processing\n",
      "[Batch 78] Starting processing with 15 examples\n",
      "[Batch 73] Finished processing\n",
      "[Batch 79] Starting processing with 15 examples\n",
      "[Batch 74] Finished processing\n",
      "[Batch 76] Finished processing\n",
      "[Batch 75] Finished processing\n",
      "[Batch 78] Finished processing\n",
      "[Batch 77] Finished processing[Batch 79] Finished processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Prepare DSPy examples from the test_r3 dataset\n",
    "test_r3 = dataset['test_r3']\n",
    "examples = [\n",
    "    dspy.Example(premise=row[\"premise\"], hypothesis=row[\"hypothesis\"])\n",
    "    for row in test_r3\n",
    "]\n",
    "\n",
    "# 2. Initialize the classification predictor\n",
    "classifier = BatchedNLIPredictor(model=lm, batch_size=15, max_workers=6)\n",
    "\n",
    "# 3. Run the predicted labels with their uids for debugging\n",
    "uids = test_r3['uid']\n",
    "ordered_llm_predictions = list(zip(uids, classifier(examples))) # triggers forward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.7035973636189568\n",
      "Recall: 0.69\n",
      "F1: 0.6941264745053273\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metrics from the `evaluate` library\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "# Define mapping from string labels to integer IDs\n",
    "# Defined in Cell 11 in anli_baseline.ipynb\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "# Extract predicted labels from ordered_llm_predictions (second element in each tuple)\n",
    "predicted_labels = [label2id[label.lower()] for uid, label in ordered_llm_predictions]\n",
    "\n",
    "# Extract gold labels from test_r3, slice to match predictions length to be safe\n",
    "gold_labels = test_r3['label'][:len(predicted_labels)]\n",
    "\n",
    "\n",
    "# Compute all metrics using integer IDs for predicted and gold labels\n",
    "acc_result = accuracy.compute(predictions=predicted_labels, references=gold_labels)\n",
    "prec_result = precision.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "rec_result = recall.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "f1_result = f1.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "\n",
    "# Print out the evaluation results\n",
    "print(\"Accuracy:\", acc_result[\"accuracy\"])\n",
    "print(\"Precision:\", prec_result[\"precision\"])\n",
    "print(\"Recall:\", rec_result[\"recall\"])\n",
    "print(\"F1:\", f1_result[\"f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8657e",
   "metadata": {},
   "source": [
    "# TASK 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38bc89cb",
   "metadata": {},
   "source": [
    "# NOTE FOR MICHAEL: \n",
    "\n",
    "1) in the assignment instructions on git , it says \"Evaluate the model on the \"test_r3\" partition of the ANLI dataset\". not on EACH test parition.\n",
    "So we will compare only to \"test_r3\" parition.\n",
    "\n",
    "2) we define DeBERTa baseline model as \"Model 1\" and the LLM baseline model as \"Model 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62ba011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pred_test_r3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51429f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842653bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models correct on 430 samples (35.83%).\n",
      "Only Model 1 correct on 147 samples (12.25%).\n",
      "Only Model 2 correct on 398 samples (33.17%).\n",
      "Both models incorrect on 225 samples (18.75%).\n"
     ]
    }
   ],
   "source": [
    "# Model 1 predictions and gold labels (from pred_test_r3)\n",
    "model1_preds = pred_test_r3[:len(ordered_llm_predictions)]\n",
    "\n",
    "# Model 2 predictions from ordered_llm_predictions\n",
    "model2_preds = ordered_llm_predictions\n",
    "\n",
    "n = len(model2_preds)\n",
    "\n",
    "both_correct = 0\n",
    "only_model1_correct = 0\n",
    "only_model2_correct = 0\n",
    "both_incorrect = 0\n",
    "\n",
    "for example, (uid, pred2) in zip(model1_preds, model2_preds):\n",
    "    gold = example['gold_label']\n",
    "    pred1 = example['pred_label']\n",
    "    \n",
    "    model1_correct = (pred1 == gold)\n",
    "    model2_correct = (pred2 == gold)\n",
    "    \n",
    "    if model1_correct and model2_correct:\n",
    "        both_correct += 1\n",
    "    elif model1_correct and not model2_correct:\n",
    "        only_model1_correct += 1\n",
    "    elif not model1_correct and model2_correct:\n",
    "        only_model2_correct += 1\n",
    "    else:\n",
    "        both_incorrect += 1\n",
    "\n",
    "print(f\"Both models correct on {both_correct} samples ({both_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 1 correct on {only_model1_correct} samples ({only_model1_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 2 correct on {only_model2_correct} samples ({only_model2_correct / n * 100:.2f}%).\")\n",
    "print(f\"Both models incorrect on {both_incorrect} samples ({both_incorrect / n * 100:.2f}%).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb567fb",
   "metadata": {},
   "source": [
    "# Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6645ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
