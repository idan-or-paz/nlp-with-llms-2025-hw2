{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import dspy\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the DSPy classifier program.\n",
    "\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "import dspy\n",
    "\n",
    "# Signature for the NLI task\n",
    "class NLISignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Classify the relationship between the premise and hypothesis \n",
    "    to a label: entailment, neutral or contradiction.\n",
    "    \"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "# A class for Parallel processing with progress display\n",
    "class NLIClassifier(dspy.Module):\n",
    "    def __init__(self, predictor_module: dspy.Module, batch_size: int = 20, num_threads: int = 8):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor_module  # Predict, ChainOfThought, etc.\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    def forward(self, examples: dspy.Example) -> list[dspy.Prediction]:\n",
    "        # Display progress with tqdm while processing\n",
    "        results = []\n",
    "        for i in tqdm(range(0, len(examples), self.batch_size), desc=\"Processing\"):\n",
    "            sub_batch = examples[i:i + self.batch_size]\n",
    "            processed = self.predictor.batch( # perform batch processing\n",
    "                sub_batch,\n",
    "                num_threads=self.num_threads\n",
    "            )\n",
    "            results.extend(processed)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": [
    "first we will optimize the model on a train set from \"dev_r3\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 40\n"
     ]
    }
   ],
   "source": [
    "# prepare the training set\n",
    "import random \n",
    "\n",
    "preprocessed_examples = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['dev_r3']  # Use the 'dev_r3' split for training\n",
    "]\n",
    "\n",
    "train_set_size = 40 # tradeoff between quality and speed after testing, permitted range is 20-100\n",
    "trainset = random.sample(preprocessed_examples, train_set_size)  # pick examples randomly for training to avoid bias\n",
    "print(f\"Total examples: {len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ebc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [00:52<02:38,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples for up to 1 rounds, amounting to 10 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the optimization using few-shot learning - only in task 1.4 we will use CoT\n",
    "\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def exact_match(example, pred, trace=None):\n",
    "    # Ensure both labels are strings and lowercase\n",
    "    ex_label = str(example.label).strip().lower()\n",
    "    pred_label = str(pred.label).strip().lower()\n",
    "\n",
    "    # In case example.label is already an int, use reverse mapping\n",
    "    if ex_label.isdigit():\n",
    "        id2label = {v: k for k, v in label2id.items()}\n",
    "        ex_label = id2label[int(ex_label)]\n",
    "\n",
    "    return label2id.get(pred_label) == label2id.get(ex_label)\n",
    "\n",
    "def compute_metrics(preds, golds):\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=golds)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=golds, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=golds, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=golds, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "model_simple = dspy.Predict(NLISignature)\n",
    "bootstrap = BootstrapFewShot(metric=exact_match)\n",
    "optimized_bootstrap = bootstrap.compile(student=model_simple, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16973a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:00<00:00, 1608.68it/s]\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:00<00:00, 2188.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 2/60 [00:00<00:03, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:00<00:00, 2097.89it/s]\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:00<00:00, 1484.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 4/60 [00:00<00:04, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:00<00:00, 1718.24it/s]\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 6/60 [00:13<02:45,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 7/60 [00:25<04:34,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:43<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 8/60 [01:08<13:06, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 9/60 [01:24<12:58, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:27<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 10/60 [01:52<15:37, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 11/60 [02:05<14:06, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 12/60 [02:16<12:14, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 13/60 [02:27<11:05, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 14/60 [02:38<10:08, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 15/60 [02:48<09:16, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 16/60 [03:00<08:51, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 17/60 [03:14<09:01, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 18/60 [03:25<08:31, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 19/60 [03:36<08:02, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 20/60 [03:56<09:37, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 21/60 [04:21<11:21, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:22<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 22/60 [04:43<11:58, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 23/60 [05:00<11:23, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 24/60 [05:17<10:42, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 25/60 [05:31<09:42, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 26/60 [05:49<09:37, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 27/60 [06:09<09:57, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 28/60 [06:30<10:01, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 29/60 [06:41<08:35, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 30/60 [06:53<07:39, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 31/60 [07:06<06:56, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 32/60 [07:17<06:20, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 33/60 [07:29<05:52, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 34/60 [07:41<05:28, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:09<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 35/60 [07:51<04:56, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 36/60 [08:04<04:51, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 37/60 [08:16<04:40, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:22<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 38/60 [08:39<05:38, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 39/60 [08:53<05:15, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:18<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 40/60 [09:11<05:19, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 41/60 [09:24<04:46, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:57<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 42/60 [10:21<08:18, 27.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 43/60 [10:50<07:57, 28.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 44/60 [11:05<06:25, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 45/60 [11:19<05:14, 20.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 46/60 [11:32<04:21, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 47/60 [12:12<05:26, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 48/60 [12:27<04:23, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:18<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 49/60 [12:46<03:50, 21.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 50/60 [12:59<03:06, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 51/60 [13:16<02:44, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 52/60 [13:30<02:15, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 53/60 [13:45<01:54, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 54/60 [13:57<01:29, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 55/60 [14:26<01:35, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 56/60 [14:38<01:08, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▌| 57/60 [14:51<00:47, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 58/60 [15:08<00:32, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 59/60 [15:22<00:15, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 60/60 [15:40<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the optimized model on 'test_r3' split\n",
    "testset_with_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "testset_no_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "\n",
    "program = NLIClassifier(optimized_bootstrap)\n",
    "predictions = program(testset_no_labels)\n",
    "pred_labels = [label2id[pred.label] for pred in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc417aa",
   "metadata": {},
   "source": [
    "# TASK 1.3 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92357a28",
   "metadata": {},
   "source": [
    "a) Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c44ae6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Model scores:\n",
      "F1 score: 0.7204\n",
      "Accuracy: 0.7167\n",
      "Precision: 0.7373\n",
      "Recall: 0.7168\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "# use compute_metrics to evaluate the model and print the results\n",
    "gold_labels = [ex.label for ex in testset_with_labels]\n",
    "metrics = compute_metrics(pred_labels, gold_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print(\"Model scores:\")\n",
    "print(f\"F1 score: {metrics['f1']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642731ac",
   "metadata": {},
   "source": [
    "Compare the results with the baseline and provide agreement metrics between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b97ccf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models are correct on 441 out of 1200 samples.\n",
      "Both models are correct on 36.75% of the samples.\n",
      "\n",
      "\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 419 out of 1200 samples.\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 34.92% of the samples.\n",
      "\n",
      "\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 136 out of 1200 samples.\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 11.33% of the samples.\n",
      "\n",
      "\n",
      "Both models are incorrect on 204 out of 1200 samples.\n",
      "Both models are incorrect on 17.00% of the samples.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many samples they are both correct\n",
    "%store -r pred_test_r3\n",
    "optimized_llm_predictions = predictions\n",
    "non_llm_predictions = pred_test_r3 # DeBERTa_v3_predictions\n",
    "TEST_SIZE = len(optimized_llm_predictions)\n",
    "# gold_labels = [label2id[row['gold_label']] for row in pred_test_r3]\n",
    "\n",
    "\n",
    "# on how many samples both models are correct\n",
    "correct_predictions = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] == row['pred_label'])\n",
    ")\n",
    "\n",
    "print(f\"Both models are correct on {correct_predictions} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are correct on {correct_predictions / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples llm is correct and DeBERTa_v3_ is incorrect\n",
    "llm_correct_deberta_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] != row['pred_label'])\n",
    ")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples DeBERTa_v3 is correct and llm is incorrect\n",
    "deberta_correct_llm_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (row['pred_label'] == row['gold_label']) and (llm_pred.label != row['gold_label'])\n",
    ")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# on how many samples both models are incorrect\n",
    "both_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label != row['gold_label']) and (row['pred_label'] != row['gold_label'])\n",
    ")\n",
    "print(f\"Both models are incorrect on {both_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are incorrect on {both_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
