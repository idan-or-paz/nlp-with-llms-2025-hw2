{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import dspy\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the DSPy classifier program.\n",
    "\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "import dspy\n",
    "\n",
    "# Signature for the NLI task\n",
    "class NLISignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Classify the relationship between the premise and hypothesis \n",
    "    to a label: entailment, neutral or contradiction.\n",
    "    \"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "# A class for Parallel processing with progress display\n",
    "class NLIClassifier(dspy.Module):\n",
    "    def __init__(self, predictor_module: dspy.Module, batch_size: int = 20, num_threads: int = 8):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor_module  # Predict, ChainOfThought, etc.\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    def forward(self, examples: dspy.Example) -> list[dspy.Prediction]:\n",
    "        # Display progress with tqdm while processing\n",
    "        results = []\n",
    "        for i in tqdm(range(0, len(examples), self.batch_size), desc=\"Processing\"):\n",
    "            sub_batch = examples[i:i + self.batch_size]\n",
    "            processed = self.predictor.batch( # perform batch processing\n",
    "                sub_batch,\n",
    "                num_threads=self.num_threads\n",
    "            )\n",
    "            results.extend(processed)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": [
    "first we will optimize the model on a train set from \"dev_r3\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf5a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 40\n"
     ]
    }
   ],
   "source": [
    "# prepare the training set\n",
    "import random \n",
    "\n",
    "preprocessed_examples = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['dev_r3']  # Use the 'dev_r3' split for training\n",
    "]\n",
    "\n",
    "train_set_size = 40 # tradeoff between quality and speed after testing, permitted range is 20-100\n",
    "trainset = random.sample(preprocessed_examples, train_set_size)  # pick examples randomly for training to avoid bias\n",
    "print(f\"Total examples: {len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ebc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:37<03:35,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the optimization using few-shot learning - only in task 1.4 we will use CoT\n",
    "\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def exact_match(example, pred, trace=None):\n",
    "    # Ensure both labels are strings and lowercase\n",
    "    ex_label = str(example.label).strip().lower()\n",
    "    pred_label = str(pred.label).strip().lower()\n",
    "\n",
    "    # In case example.label is already an int, use reverse mapping\n",
    "    if ex_label.isdigit():\n",
    "        id2label = {v: k for k, v in label2id.items()}\n",
    "        ex_label = id2label[int(ex_label)]\n",
    "\n",
    "    return label2id.get(pred_label) == label2id.get(ex_label)\n",
    "\n",
    "def compute_metrics(preds, golds):\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=golds)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=golds, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=golds, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=golds, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "model_simple = dspy.Predict(NLISignature)\n",
    "bootstrap = BootstrapFewShot(metric=exact_match)\n",
    "optimized_bootstrap = bootstrap.compile(student=model_simple, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16973a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:19<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 1/60 [00:19<18:45, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 2/60 [00:33<16:01, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 3/60 [00:47<14:25, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 4/60 [00:59<13:00, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 5/60 [01:14<12:59, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 6/60 [01:26<12:19, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 7/60 [01:37<11:13, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 8/60 [01:51<11:30, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 9/60 [02:06<11:34, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 10/60 [02:19<11:18, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 11/60 [02:36<11:53, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 12/60 [02:49<11:07, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 13/60 [03:00<10:23, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 14/60 [03:13<09:56, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:23<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 15/60 [03:36<12:02, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 16/60 [03:49<11:04, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 17/60 [04:04<10:53, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 18/60 [04:17<10:03, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 19/60 [04:30<09:38, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 20/60 [04:40<08:37, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 21/60 [04:53<08:17, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 22/60 [05:06<08:08, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 23/60 [05:19<08:00, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 24/60 [05:31<07:41, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 25/60 [05:44<07:28, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 26/60 [05:56<07:04, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 27/60 [06:10<07:09, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 28/60 [06:25<07:09, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 29/60 [06:38<06:58, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 30/60 [06:50<06:25, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 31/60 [07:04<06:28, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 32/60 [07:18<06:15, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 33/60 [07:29<05:44, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 34/60 [07:41<05:22, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:19<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 35/60 [08:00<06:00, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:20<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 36/60 [08:20<06:27, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:19<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 37/60 [08:40<06:37, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 38/60 [08:59<06:34, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 39/60 [09:12<05:44, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 40/60 [09:25<05:06, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 41/60 [09:37<04:32, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 42/60 [09:51<04:18, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 43/60 [10:06<04:04, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 44/60 [10:19<03:44, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 45/60 [10:30<03:18, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:34<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 46/60 [11:05<04:36, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 47/60 [11:18<03:47, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 48/60 [11:31<03:16, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 49/60 [11:44<02:48, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 50/60 [11:56<02:23, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 51/60 [12:11<02:09, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 52/60 [12:27<01:58, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 53/60 [12:39<01:38, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 54/60 [12:52<01:23, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 55/60 [13:04<01:05, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:35<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 56/60 [13:39<01:19, 19.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▌| 57/60 [13:51<00:52, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 58/60 [14:05<00:32, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 59/60 [14:19<00:15, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 / 20 examples: 100%|██████████| 20/20 [00:12<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 60/60 [14:31<00:00, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the optimized model on 'test_r3' split\n",
    "testset_with_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"],\n",
    "        label=row[\"label\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "testset_no_labels = [\n",
    "    dspy.Example(\n",
    "        premise=row[\"premise\"],\n",
    "        hypothesis=row[\"hypothesis\"]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")\n",
    "    for row in dataset['test_r3']  # Use the 'test_r3' split for evaluation\n",
    "]\n",
    "\n",
    "\n",
    "program = NLIClassifier(optimized_bootstrap)\n",
    "predictions = program(testset_no_labels)\n",
    "pred_labels = [label2id[pred.label] for pred in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc417aa",
   "metadata": {},
   "source": [
    "# TASK 1.3 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92357a28",
   "metadata": {},
   "source": [
    "a) Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c44ae6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Model scores:\n",
      "F1 score: 0.7141\n",
      "Accuracy: 0.7117\n",
      "Precision: 0.7368\n",
      "Recall: 0.7121\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "# use compute_metrics to evaluate the model and print the results\n",
    "gold_labels = [ex.label for ex in testset_with_labels]\n",
    "metrics = compute_metrics(pred_labels, gold_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print(\"Model scores:\")\n",
    "print(f\"F1 score: {metrics['f1']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642731ac",
   "metadata": {},
   "source": [
    "Compare the results with the baseline and provide agreement metrics between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b97ccf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models are correct on 445 out of 1200 samples.\n",
      "Both models are correct on 37.08% of the samples.\n",
      "\n",
      "\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 409 out of 1200 samples.\n",
      "LLM is correct and DeBERTa_v3 is incorrect on 34.08% of the samples.\n",
      "\n",
      "\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 132 out of 1200 samples.\n",
      "DeBERTa_v3 is correct and LLM is incorrect on 11.00% of the samples.\n",
      "\n",
      "\n",
      "Both models are incorrect on 214 out of 1200 samples.\n",
      "Both models are incorrect on 17.83% of the samples.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many samples they are both correct\n",
    "%store -r pred_test_r3\n",
    "optimized_llm_predictions = predictions\n",
    "non_llm_predictions = pred_test_r3 # DeBERTa_v3_predictions\n",
    "TEST_SIZE = len(optimized_llm_predictions)\n",
    "# gold_labels = [label2id[row['gold_label']] for row in pred_test_r3]\n",
    "\n",
    "\n",
    "# on how many samples both models are correct\n",
    "correct_predictions = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] == row['pred_label'])\n",
    ")\n",
    "\n",
    "print(f\"Both models are correct on {correct_predictions} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are correct on {correct_predictions / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples llm is correct and DeBERTa_v3_ is incorrect\n",
    "llm_correct_deberta_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label == row['gold_label']) and (row['gold_label'] != row['pred_label'])\n",
    ")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"LLM is correct and DeBERTa_v3 is incorrect on {llm_correct_deberta_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# On how many samples DeBERTa_v3 is correct and llm is incorrect\n",
    "deberta_correct_llm_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (row['pred_label'] == row['gold_label']) and (llm_pred.label != row['gold_label'])\n",
    ")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"DeBERTa_v3 is correct and LLM is incorrect on {deberta_correct_llm_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# on how many samples both models are incorrect\n",
    "both_incorrect = sum(\n",
    "    1 for row, llm_pred in zip(non_llm_predictions,optimized_llm_predictions)\n",
    "    if (llm_pred.label != row['gold_label']) and (row['pred_label'] != row['gold_label'])\n",
    ")\n",
    "print(f\"Both models are incorrect on {both_incorrect} out of {TEST_SIZE} samples.\")\n",
    "print(f\"Both models are incorrect on {both_incorrect / TEST_SIZE * 100:.2f}% of the samples.\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
