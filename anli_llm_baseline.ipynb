{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class NLIClassifier(dspy.Signature):\n",
    "    premise = dspy.InputField(desc=\"A factual statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"A statement to evaluate against the premise\")\n",
    "    label = dspy.OutputField(\n",
    "        desc=\"The relationship between premise and hypothesis: entailment, neutral, or contradiction\",\n",
    "        choices=[\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a Predict module\n",
    "nli_predict = dspy.Predict(NLIClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b84927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(batch):\n",
    "    # Extract lists from the dataset batch\n",
    "    uids = list(batch[\"uid\"])  # Keep as list\n",
    "    premises = list(batch[\"premise\"])\n",
    "    hypotheses = list(batch[\"hypothesis\"])\n",
    "    \n",
    "    # Build model inputs\n",
    "    inputs = [\n",
    "        {\"premise\": p, \"hypothesis\": h}\n",
    "        for p, h in zip(premises, hypotheses)\n",
    "    ]\n",
    "    print(inputs[:5])  # Print first 5 inputs for debugging\n",
    "    \n",
    "    # Vectorized DSPy prediction\n",
    "    predictions = nli_predict.batch(inputs)\n",
    "    labels = [pred.label for pred in predictions]\n",
    "    \n",
    "    # Pair each uid with its prediction\n",
    "    return list(zip(uids, labels))\n",
    "\n",
    "\n",
    "def create_batch(dataset, batch_size=32):\n",
    "    # Create batches of the dataset\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\", 'hypothesis': 'The day of the passage is usually when Christians praise the lord together'}, {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.', 'hypothesis': 'No children were killed in the accident.'}, {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.', 'hypothesis': 'Japanese like kit kat. '}, {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.', 'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.'}, {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.', 'hypothesis': 'Pietro Grassano was once the country head for France.'}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m mini_test_r3 = test_r3.select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# batches = create_batch(test_r3, batch_size=32)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m predictions = \u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_test_r3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# for batch in batches:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     batch_predictions = predict_batch(batch)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     predictions.extend(batch_predictions)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#     print(f\"Processed batch with {len(batch)} examples, total predictions: {len(predictions)}\")\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     print(predictions[-5:])  # Print last 5 predictions for debugging\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpredict_batch\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(inputs[:\u001b[32m5\u001b[39m])  \u001b[38;5;66;03m# Print first 5 inputs for debugging\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Vectorized DSPy prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m predictions = \u001b[43mnli_predict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m labels = [pred.label \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Pair each uid with its prediction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wexle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\dspy\\primitives\\program.py:138\u001b[39m, in \u001b[36mModule.batch\u001b[39m\u001b[34m(self, examples, num_threads, max_errors, return_failed_examples, provide_traceback, disable_progress_bar)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mProcesses a list of dspy.Example instances in parallel using the Parallel module.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m    List of results, and optionally failed examples and exceptions.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Create a list of execution pairs (self, example)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m exec_pairs = [(\u001b[38;5;28mself\u001b[39m, \u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m()) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Create an instance of Parallel\u001b[39;00m\n\u001b[32m    141\u001b[39m parallel_executor = Parallel(\n\u001b[32m    142\u001b[39m     num_threads=num_threads,\n\u001b[32m    143\u001b[39m     max_errors=max_errors,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     disable_progress_bar=disable_progress_bar,\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "test_r3 = dataset['test_r3']\n",
    "mini_test_r3 = test_r3.select(range(100))\n",
    "# batches = create_batch(test_r3, batch_size=32)\n",
    "predictions = predict_batch(mini_test_r3)\n",
    "# for batch in batches:\n",
    "#     batch_predictions = predict_batch(batch)\n",
    "#     predictions.extend(batch_predictions)\n",
    "#     print(f\"Processed batch with {len(batch)} examples, total predictions: {len(predictions)}\")\n",
    "#     print(predictions[-5:])  # Print last 5 predictions for debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
