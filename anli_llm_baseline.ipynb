{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9b84927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class BatchedNLIPredictor(dspy.Module):\n",
    "    def __init__(self, model, batch_size=15):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, examples):\n",
    "        # examples: list of dspy.Example(premise=..., hypothesis=...)\n",
    "        \n",
    "        # Build a single prompt with multiple pairs\n",
    "        prompt = \"Choose the correct relationship between the hypothesis and the premise: entailment/neutral/contradiction  .\\n\\n\"\n",
    "        for i, ex in enumerate(examples, start=1):\n",
    "            prompt += f\"Example {i}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "        # Single LLM call\n",
    "        response = self.model(prompt)\n",
    "\n",
    "        # Parse the response line by line\n",
    "        predictions = []\n",
    "        for line in response.splitlines():\n",
    "            if line.strip().lower().startswith((\"entailment\", \"neutral\", \"contradiction\")):\n",
    "                predictions.append(line.strip().lower())\n",
    "\n",
    "        # Ensure same length\n",
    "        while len(predictions) < len(examples):\n",
    "            predictions.append(\"unknown\")\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b262f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "batched_predictor = BatchedNLIPredictor(model=lm, batch_size=15)\n",
    "\n",
    "def process_chunk(chunk_examples, chunk_uids, batch_index):\n",
    "    # Build single prompt for this batch\n",
    "    prompt = \"Classify the hypothesis and premise relationship: entailment / neutral / contradiction. **provide 1 word answer**.\\n\\n\"\n",
    "    for idx, ex in enumerate(chunk_examples, start=1):\n",
    "        prompt += f\"Example {idx}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "    # LLM call (one API call per chunk)\n",
    "    print(f\"Processing batch {batch_index} with {len(chunk_examples)} examples\")\n",
    "    response = batched_predictor.model(prompt)\n",
    "    print(f\"Done with batch {batch_index}\")\n",
    "    \n",
    "\n",
    "    # Handle response text\n",
    "    response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "    lines = response_text.splitlines()\n",
    "\n",
    "    # Extract predictions\n",
    "    predictions = []\n",
    "    for line in lines:\n",
    "        m = re.search(r\"(entailment|neutral|contradiction)\", line, re.IGNORECASE)\n",
    "        if m:\n",
    "            predictions.append(m.group(1).lower())\n",
    "\n",
    "    # Pad missing\n",
    "    while len(predictions) < len(chunk_examples):\n",
    "        predictions.append(\"unknown\")\n",
    "\n",
    "    return batch_index, list(zip(chunk_uids, predictions))\n",
    "\n",
    "\n",
    "def predict_batch_parallel(batch, batch_size=15, max_workers=6):\n",
    "    uids = list(batch[\"uid\"])\n",
    "    premises = list(batch[\"premise\"])\n",
    "    hypotheses = list(batch[\"hypothesis\"])\n",
    "\n",
    "    examples = [\n",
    "        dspy.Example(premise=p, hypothesis=h)\n",
    "        for p, h in zip(premises, hypotheses)\n",
    "    ]\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(examples), batch_size):\n",
    "            chunk = examples[i:i+batch_size]\n",
    "            chunk_uids = uids[i:i+batch_size]\n",
    "            batch_index = i // batch_size\n",
    "            futures.append(executor.submit(process_chunk, chunk, chunk_uids, batch_index))\n",
    "\n",
    "        # Collect results in order\n",
    "        results_by_index = {}\n",
    "        for future in as_completed(futures):\n",
    "            batch_index, batch_result = future.result()\n",
    "            results_by_index[batch_index] = batch_result\n",
    "\n",
    "    # Flatten results in original order\n",
    "    ordered_results = []\n",
    "    for batch_index in sorted(results_by_index.keys()):\n",
    "        ordered_results.extend(results_by_index[batch_index])\n",
    "\n",
    "    return ordered_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with 15 examples\n",
      "Done with batch 0\n",
      "Processing batch 1 with 15 examples\n",
      "Done with batch 1\n",
      "Processing batch 2 with 15 examples\n",
      "Done with batch 2\n",
      "Processing batch 3 with 15 examples\n",
      "Done with batch 3\n",
      "Processing batch 5 with 15 examples\n",
      "Processing batch 4 with 15 examples\n",
      "Processing batch 6 with 10 examples\n",
      "Done with batch 6\n",
      "Done with batch 4\n",
      "Done with batch 5\n"
     ]
    }
   ],
   "source": [
    "test_r3 = dataset['test_r3']\n",
    "ordered_predictions = predict_batch_parallel(test_r3[:100], batch_size=15)\n",
    "\n",
    "# for uid, label in baseline_predictions:\n",
    "#     print(uid, label)\n",
    "\n",
    "        #    'premise': premise,\n",
    "        #     'hypothesis': hypothesis,\n",
    "        #     'prediction': prediction,\n",
    "        #     'pred_label': get_prediction(prediction),\n",
    "        #     'gold_label': label_names[example['label']],\n",
    "        #     'reason': example['reason']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd808dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\", 'hypothesis': 'The day of the passage is usually when Christians praise the lord together', 'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2}, 'pred_label': 'neutral', 'gold_label': 'entailment', 'reason': \"Sunday is considered Lord's Day\", 'section': 'test_r3'}, {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.', 'hypothesis': 'No children were killed in the accident.', 'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0}, 'pred_label': 'neutral', 'gold_label': 'entailment', 'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.', 'section': 'test_r3'}, {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.', 'hypothesis': 'Japanese like kit kat. ', 'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1}, 'pred_label': 'entailment', 'gold_label': 'entailment', 'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '}]\n",
      "pred_test_r3 length: 1200\n"
     ]
    }
   ],
   "source": [
    "%store -r pred_test_r3\n",
    "print(pred_test_r3[:3])\n",
    "print(\"pred_test_r3 length:\", len(pred_test_r3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "Precision: 0.7031539074960128\n",
      "Recall: 0.64\n",
      "F1: 0.6609256625727213\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metrics from the `evaluate` library\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "# Define mapping from string labels to integer IDs\n",
    "# Defined in Cell 11 in anli_baseline.ipynb\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "# Extract predicted labels from ordered_predictions (second element in each tuple)\n",
    "predicted_labels = [label2id[label.lower()] for uid, label in ordered_predictions]\n",
    "\n",
    "# Extract gold labels from test_r3, slice to match predictions length to be safe\n",
    "gold_labels = test_r3['label'][:len(predicted_labels)]\n",
    "\n",
    "\n",
    "# Compute all metrics using integer IDs for predicted and gold labels\n",
    "acc_result = accuracy.compute(predictions=predicted_labels, references=gold_labels)\n",
    "prec_result = precision.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "rec_result = recall.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "f1_result = f1.compute(predictions=predicted_labels, references=gold_labels, average=\"weighted\")\n",
    "\n",
    "# Print out the evaluation results\n",
    "print(\"Accuracy:\", acc_result[\"accuracy\"])\n",
    "print(\"Precision:\", prec_result[\"precision\"])\n",
    "print(\"Recall:\", rec_result[\"recall\"])\n",
    "print(\"F1:\", f1_result[\"f1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38bc89cb",
   "metadata": {},
   "source": [
    "# NOTE FOR MICHAEL: \n",
    "\n",
    "in the assignment instructions on git , it says \"Evaluate the model on the \"test_r3\" partition of the ANLI dataset\". not on EACH test parition.\n",
    "So we will compare only to \"test_r3\" parition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51429f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\", 'hypothesis': 'The day of the passage is usually when Christians praise the lord together', 'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2}, 'pred_label': 'neutral', 'gold_label': 'entailment', 'reason': \"Sunday is considered Lord's Day\", 'section': 'test_r3'}, {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.', 'hypothesis': 'No children were killed in the accident.', 'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0}, 'pred_label': 'neutral', 'gold_label': 'entailment', 'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.', 'section': 'test_r3'}, {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.', 'hypothesis': 'Japanese like kit kat. ', 'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1}, 'pred_label': 'entailment', 'gold_label': 'entailment', 'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '}]\n",
      "pred_test_r3 length: 1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
