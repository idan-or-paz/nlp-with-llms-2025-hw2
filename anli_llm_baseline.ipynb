{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import islice\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0106ea8",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "1) test_nli_predictor -> run the experiment with specific pipeline on specific dataset\n",
    "\n",
    "2) evaluate_predictions -> summarize the metrics of the experiments using \"evaluate\" package\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e39a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_response:\n",
    "    def __init__(self, uid = None, label = None, reason = None):\n",
    "        self.uid = uid\n",
    "        self.label = label\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c272ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nli_predictor(\n",
    "    dataset,\n",
    "    split_name,\n",
    "    model,\n",
    "    predictor_class,\n",
    "    example_extractor,\n",
    "    batch_size=10,\n",
    "    max_workers=6,\n",
    "    max_examples=100,\n",
    "    uid_key='uid',\n",
    "):\n",
    "    data_split = dataset[split_name]\n",
    "\n",
    "    # Iterate only up to max_examples without slicing\n",
    "    subset_iter = islice(data_split, max_examples)\n",
    "    \n",
    "    examples = []\n",
    "    uids = []\n",
    "    for row in subset_iter:\n",
    "        examples.append(example_extractor(row))\n",
    "        uids.append(row[uid_key])\n",
    "\n",
    "    predictor = predictor_class(model=model, batch_size=batch_size, max_workers=max_workers)\n",
    "    predictions = predictor(examples)\n",
    "\n",
    "    results = list(zip(uids, [p[0] for p in predictions], [p[1] for p in predictions]))\n",
    "    return results\n",
    "\n",
    "def extract_example(row):\n",
    "    return dspy.Example(premise=row[\"premise\"], hypothesis=row[\"hypothesis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a8c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(\n",
    "    ordered_llm_predictions,  # list of (uid, label, ...) tuples, label is str\n",
    "    gold_labels,             # list or dataset column of gold labels (int or str)\n",
    "    label2id,                # dict mapping string labels to int ids\n",
    "    gold_label_key=None      # if gold_labels is a dataset dict, the key to use\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate classification predictions and print accuracy, precision, recall, and F1.\n",
    "\n",
    "    Args:\n",
    "        ordered_llm_predictions: List of tuples (uid, label, ...) or (uid, label)\n",
    "        gold_labels: List or Dataset column of gold labels (ints or strings)\n",
    "        label2id: Dict mapping label strings (case insensitive) to ints\n",
    "        gold_label_key: Optional, if gold_labels is a dataset dict, the key to use\n",
    "\n",
    "    Prints:\n",
    "        Accuracy, Precision, Recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    # Load metrics\n",
    "    accuracy = load(\"accuracy\")\n",
    "    precision = load(\"precision\")\n",
    "    recall = load(\"recall\")\n",
    "    f1 = load(\"f1\")\n",
    "\n",
    "    # Extract predicted labels mapped to int IDs\n",
    "    predicted_labels = [\n",
    "        label2id.get(item[1].lower(), -1)  # -1 for unknown labels\n",
    "        for item in ordered_llm_predictions\n",
    "    ]\n",
    "\n",
    "    # Extract gold labels\n",
    "    if gold_label_key and isinstance(gold_labels, dict):\n",
    "        golds = gold_labels[gold_label_key]\n",
    "    else:\n",
    "        golds = gold_labels\n",
    "\n",
    "    # Trim to equal length\n",
    "    min_len = min(len(predicted_labels), len(golds))\n",
    "    predicted_labels = predicted_labels[:min_len]\n",
    "    golds = golds[:min_len]\n",
    "\n",
    "    # Compute metrics\n",
    "    acc_result = accuracy.compute(predictions=predicted_labels, references=golds)\n",
    "    prec_result = precision.compute(predictions=predicted_labels, references=golds, average=\"weighted\")\n",
    "    rec_result = recall.compute(predictions=predicted_labels, references=golds, average=\"weighted\")\n",
    "    f1_result = f1.compute(predictions=predicted_labels, references=golds, average=\"weighted\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"Evaluation results:\")\n",
    "    print(f\"  Accuracy:  {acc_result.get('accuracy')}\")\n",
    "    print(f\"  Precision: {prec_result.get('precision')}\")\n",
    "    print(f\"  Recall:    {rec_result.get('recall')}\")\n",
    "    print(f\"  F1:        {f1_result.get('f1')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58aefb",
   "metadata": {},
   "source": [
    "# TASK 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94506f",
   "metadata": {},
   "source": [
    "Below we define the pipline for task 1.3 , using a DSPy-hybrid approach.\n",
    "1) We use DSPy as a wrapper - because it provides a convenient layout\n",
    "2) Yet we do Manual prompting, since DSPy was unable to batch multiple examples into 1 API, increasing time & usage dramatically.\n",
    "3) Note: DSPy is just a wrapper for prompting. we don't miss a core element by taking this hybrid approach. \n",
    "see https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcf2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedNLIPredictor(dspy.Module):\n",
    "    \"\"\"\n",
    "    DSPy program to classify NLI examples in batches using a language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, batch_size=15, max_workers=6):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def _process_chunk(self, chunk_examples, batch_index):\n",
    "        \"\"\"Helper to process a single chunk of examples.\"\"\"\n",
    "        print(f\"[Batch {batch_index}] Starting processing with {len(chunk_examples)} examples\")\n",
    "\n",
    "        # Build a single prompt for the chunk\n",
    "        prompt = (\n",
    "            \"Classify the relationship between the hypothesis and premise: \"\n",
    "            \"entailment / neutral / contradiction. **Provide a one-word answer**.\\n\\n\"\n",
    "        )\n",
    "        for idx, ex in enumerate(chunk_examples, start=1):\n",
    "            prompt += (\n",
    "                f\"Example {idx}:\\n\"\n",
    "                f\"Premise: {ex.premise}\\n\"\n",
    "                f\"Hypothesis: {ex.hypothesis}\\n\"\n",
    "                f\"Answer (entailment/neutral/contradiction):\\n\"\n",
    "            )\n",
    "\n",
    "        # Single LLM call\n",
    "        response = self.model(prompt)\n",
    "        response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "\n",
    "        # Extract predictions\n",
    "        predictions = []\n",
    "        for line in response_text.splitlines():\n",
    "            m = re.search(r\"(entailment|neutral|contradiction)\", line, re.IGNORECASE)\n",
    "            if m:\n",
    "                # predictions.append(m.group(1).lower())\n",
    "                predictions.append((m.group(1).lower(), None))\n",
    "\n",
    "        # Pad any missing predictions\n",
    "        while len(predictions) < len(chunk_examples):\n",
    "            predictions.append((\"unknown\",None))\n",
    "\n",
    "        print(f\"[Batch {batch_index}] Finished processing\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, examples):\n",
    "        \"\"\"\n",
    "        Main pipeline: \n",
    "        - Splits examples into batches\n",
    "        - Runs them in parallel\n",
    "        - Returns predictions in the original order\n",
    "        \"\"\"\n",
    "        if not examples:\n",
    "            return []\n",
    "\n",
    "        # Split into batches\n",
    "        chunks = [\n",
    "            examples[i:i+self.batch_size]\n",
    "            for i in range(0, len(examples), self.batch_size)\n",
    "        ]\n",
    "\n",
    "        results_by_index = {}\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit each batch to executor\n",
    "            futures = {\n",
    "                executor.submit(self._process_chunk, chunk, idx): idx\n",
    "                for idx, chunk in enumerate(chunks)\n",
    "            }\n",
    "\n",
    "            # Collect results\n",
    "            for future in as_completed(futures):\n",
    "                batch_index = futures[future]\n",
    "                predictions = future.result()\n",
    "                results_by_index[batch_index] = predictions\n",
    "\n",
    "        # Flatten results in original order\n",
    "        ordered_predictions = []\n",
    "        for idx in sorted(results_by_index.keys()):\n",
    "            ordered_predictions.extend(results_by_index[idx])\n",
    "\n",
    "        return ordered_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc89cb",
   "metadata": {},
   "source": [
    "# NOTE FOR MICHAEL: \n",
    "\n",
    "1) in the assignment instructions on git , it says \"Evaluate the model on the \"test_r3\" partition of the ANLI dataset\". not on EACH test parition.\n",
    "So we will compare only to \"test_r3\" parition.\n",
    "\n",
    "2) we define DeBERTa baseline model as \"Model 1\" and the LLM baseline model as \"Model 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a1452",
   "metadata": {},
   "source": [
    "Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] Starting processing with 15 examples\n",
      "[Batch 1] Starting processing with 15 examples\n",
      "[Batch 2] Starting processing with 15 examples\n",
      "[Batch 3] Starting processing with 15 examples\n",
      "[Batch 1] Finished processing\n",
      "[Batch 4] Starting processing with 15 examples\n",
      "[Batch 5] Starting processing with 15 examples\n",
      "[Batch 6] Starting processing with 15 examples\n",
      "[Batch 0] Finished processing\n",
      "[Batch 7] Starting processing with 15 examples\n",
      "[Batch 2] Finished processing\n",
      "[Batch 8] Starting processing with 15 examples\n",
      "[Batch 4] Finished processing\n",
      "[Batch 9] Starting processing with 15 examples\n",
      "[Batch 3] Finished processing\n",
      "[Batch 10] Starting processing with 15 examples\n",
      "[Batch 5] Finished processing\n",
      "[Batch 11] Starting processing with 15 examples\n",
      "[Batch 7] Finished processing\n",
      "[Batch 12] Starting processing with 15 examples\n",
      "[Batch 6] Finished processing\n",
      "[Batch 13] Starting processing with 15 examples\n",
      "[Batch 8] Finished processing\n",
      "[Batch 14] Starting processing with 15 examples\n",
      "[Batch 9] Finished processing\n",
      "[Batch 15] Starting processing with 15 examples\n",
      "[Batch 15] Finished processing\n",
      "[Batch 16] Starting processing with 15 examples\n",
      "[Batch 10] Finished processing\n",
      "[Batch 17] Starting processing with 15 examples\n",
      "[Batch 12] Finished processing\n",
      "[Batch 18] Starting processing with 15 examples\n",
      "[Batch 11] Finished processing\n",
      "[Batch 19] Starting processing with 15 examples\n",
      "[Batch 18] Finished processing\n",
      "[Batch 20] Starting processing with 15 examples\n",
      "[Batch 13] Finished processing\n",
      "[Batch 21] Starting processing with 15 examples\n",
      "[Batch 14] Finished processing\n",
      "[Batch 22] Starting processing with 15 examples\n",
      "[Batch 16] Finished processing\n",
      "[Batch 23] Starting processing with 15 examples\n",
      "[Batch 17] Finished processing\n",
      "[Batch 24] Starting processing with 15 examples\n",
      "[Batch 23] Finished processing\n",
      "[Batch 25] Starting processing with 15 examples\n",
      "[Batch 20] Finished processing\n",
      "[Batch 26] Starting processing with 15 examples\n",
      "[Batch 19] Finished processing\n",
      "[Batch 27] Starting processing with 15 examples\n",
      "[Batch 26] Finished processing\n",
      "[Batch 28] Starting processing with 15 examples\n",
      "[Batch 22] Finished processing\n",
      "[Batch 29] Starting processing with 15 examples\n",
      "[Batch 21] Finished processing\n",
      "[Batch 30] Starting processing with 15 examples\n",
      "[Batch 24] Finished processing\n",
      "[Batch 31] Starting processing with 15 examples\n",
      "[Batch 25] Finished processing\n",
      "[Batch 32] Starting processing with 15 examples\n",
      "[Batch 27] Finished processing\n",
      "[Batch 33] Starting processing with 15 examples\n",
      "[Batch 29] Finished processing\n",
      "[Batch 34] Starting processing with 15 examples\n",
      "[Batch 28] Finished processing\n",
      "[Batch 35] Starting processing with 15 examples\n",
      "[Batch 35] Finished processing\n",
      "[Batch 36] Starting processing with 15 examples\n",
      "[Batch 31] Finished processing\n",
      "[Batch 37] Starting processing with 15 examples\n",
      "[Batch 36] Finished processing\n",
      "[Batch 38] Starting processing with 15 examples\n",
      "[Batch 30] Finished processing\n",
      "[Batch 39] Starting processing with 15 examples\n",
      "[Batch 32] Finished processing\n",
      "[Batch 40] Starting processing with 15 examples\n",
      "[Batch 37] Finished processing\n",
      "[Batch 41] Starting processing with 15 examples\n",
      "[Batch 33] Finished processing\n",
      "[Batch 42] Starting processing with 15 examples\n",
      "[Batch 41] Finished processing\n",
      "[Batch 43] Starting processing with 15 examples\n",
      "[Batch 42] Finished processing\n",
      "[Batch 44] Starting processing with 15 examples\n",
      "[Batch 38] Finished processing\n",
      "[Batch 45] Starting processing with 15 examples\n",
      "[Batch 44] Finished processing\n",
      "[Batch 46] Starting processing with 15 examples\n",
      "[Batch 39] Finished processing\n",
      "[Batch 47] Starting processing with 15 examples\n",
      "[Batch 47] Finished processing\n",
      "[Batch 48] Starting processing with 15 examples\n",
      "[Batch 40] Finished processing\n",
      "[Batch 49] Starting processing with 15 examples\n",
      "[Batch 34] Finished processing\n",
      "[Batch 50] Starting processing with 15 examples\n",
      "[Batch 43] Finished processing\n",
      "[Batch 51] Starting processing with 15 examples\n",
      "[Batch 51] Finished processing\n",
      "[Batch 52] Starting processing with 15 examples\n",
      "[Batch 45] Finished processing\n",
      "[Batch 53] Starting processing with 15 examples\n",
      "[Batch 46] Finished processing\n",
      "[Batch 54] Starting processing with 15 examples\n",
      "[Batch 49] Finished processing\n",
      "[Batch 55] Starting processing with 15 examples\n",
      "[Batch 54] Finished processing\n",
      "[Batch 56] Starting processing with 15 examples\n",
      "[Batch 55] Finished processing\n",
      "[Batch 57] Starting processing with 15 examples\n",
      "[Batch 50] Finished processing\n",
      "[Batch 58] Starting processing with 15 examples\n",
      "[Batch 48] Finished processing\n",
      "[Batch 59] Starting processing with 15 examples\n",
      "[Batch 52] Finished processing\n",
      "[Batch 60] Starting processing with 15 examples\n",
      "[Batch 60] Finished processing\n",
      "[Batch 61] Starting processing with 15 examples\n",
      "[Batch 57] Finished processing\n",
      "[Batch 62] Starting processing with 15 examples\n",
      "[Batch 53] Finished processing\n",
      "[Batch 63] Starting processing with 15 examples\n",
      "[Batch 63] Finished processing\n",
      "[Batch 64] Starting processing with 15 examples\n",
      "[Batch 56] Finished processing\n",
      "[Batch 65] Starting processing with 15 examples\n",
      "[Batch 59] Finished processing\n",
      "[Batch 66] Starting processing with 15 examples\n",
      "[Batch 58] Finished processing\n",
      "[Batch 67] Starting processing with 15 examples\n",
      "[Batch 62] Finished processing\n",
      "[Batch 68] Starting processing with 15 examples\n",
      "[Batch 68] Finished processing\n",
      "[Batch 69] Starting processing with 15 examples\n",
      "[Batch 61] Finished processing\n",
      "[Batch 70] Starting processing with 15 examples\n",
      "[Batch 69] Finished processing\n",
      "[Batch 71] Starting processing with 15 examples\n",
      "[Batch 70] Finished processing\n",
      "[Batch 72] Starting processing with 15 examples\n",
      "[Batch 64] Finished processing\n",
      "[Batch 73] Starting processing with 15 examples\n",
      "[Batch 72] Finished processing\n",
      "[Batch 74] Starting processing with 15 examples\n",
      "[Batch 65] Finished processing\n",
      "[Batch 75] Starting processing with 15 examples\n",
      "[Batch 66] Finished processing\n",
      "[Batch 76] Starting processing with 15 examples\n",
      "[Batch 67] Finished processing\n",
      "[Batch 77] Starting processing with 15 examples\n",
      "[Batch 76] Finished processing\n",
      "[Batch 78] Starting processing with 15 examples\n",
      "[Batch 77] Finished processing\n",
      "[Batch 79] Starting processing with 15 examples\n",
      "[Batch 71] Finished processing\n",
      "[Batch 79] Finished processing\n",
      "[Batch 73] Finished processing\n",
      "[Batch 74] Finished processing\n",
      "[Batch 75] Finished processing\n",
      "[Batch 78] Finished processing\n"
     ]
    }
   ],
   "source": [
    "test_r3_llm_results = test_nli_predictor(\n",
    "    dataset=dataset,\n",
    "    split_name='test_r3',\n",
    "    model=lm,\n",
    "    predictor_class=BatchedNLIPredictor,\n",
    "    example_extractor=extract_example,\n",
    "    batch_size=15,\n",
    "    max_workers=6,\n",
    "    max_examples=1200,\n",
    "    uid_key='uid'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc60c3",
   "metadata": {},
   "source": [
    "Evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3869b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  Accuracy:  0.69\n",
      "  Precision: 0.7035973636189568\n",
      "  Recall:    0.69\n",
      "  F1:        0.6941264745053273\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(\n",
    "    ordered_llm_predictions=test_r3_llm_results,\n",
    "    gold_labels=dataset['test_r3']['label'],\n",
    "    label2id={\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a88fe9",
   "metadata": {},
   "source": [
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ba011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the predictions from anli_baseline.ipynb\n",
    "%store -r pred_test_r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842653bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models correct on 430 samples (35.83%).\n",
      "Only Model 1 correct on 147 samples (12.25%).\n",
      "Only Model 2 correct on 398 samples (33.17%).\n",
      "Both models incorrect on 225 samples (18.75%).\n"
     ]
    }
   ],
   "source": [
    "# Model 1: pred_test_r3 is a list of dicts with keys: pred_label, gold_label\n",
    "model1_preds = pred_test_r3  # [{'pred_label': ..., 'gold_label': ...}, ...]\n",
    "\n",
    "# Model 2: [(uid, label, reason), ...]\n",
    "model2_preds = test_r3_llm_results\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "n = min(len(model1_preds), len(model2_preds))\n",
    "\n",
    "both_correct = 0\n",
    "only_model1_correct = 0\n",
    "only_model2_correct = 0\n",
    "both_incorrect = 0\n",
    "\n",
    "\n",
    "for example, (_uid, pred2, _reason) in zip(model1_preds[:n], model2_preds[:n]):\n",
    "    gold = example['gold_label'].lower()\n",
    "    pred1 = example['pred_label'].lower()\n",
    "    pred2 = pred2.lower()\n",
    "\n",
    "    model1_correct = (pred1 == gold)\n",
    "    model2_correct = (pred2 == gold)\n",
    "\n",
    "    if model1_correct and model2_correct:\n",
    "        both_correct += 1\n",
    "    elif model1_correct and not model2_correct:\n",
    "        only_model1_correct += 1\n",
    "    elif not model1_correct and model2_correct:\n",
    "        only_model2_correct += 1\n",
    "    else:\n",
    "        both_incorrect += 1\n",
    "\n",
    "print(f\"Both models correct on {both_correct} samples ({both_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 1 correct on {only_model1_correct} samples ({only_model1_correct / n * 100:.2f}%).\")\n",
    "print(f\"Only Model 2 correct on {only_model2_correct} samples ({only_model2_correct / n * 100:.2f}%).\")\n",
    "print(f\"Both models incorrect on {both_incorrect} samples ({both_incorrect / n * 100:.2f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb567fb",
   "metadata": {},
   "source": [
    "# Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa75908",
   "metadata": {},
   "source": [
    "Joint prompt pipeline: prompt the LLM to produce at once a CoT explanation and a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb797941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedNLIJointPredictor(dspy.Module):\n",
    "    def __init__(self, model, batch_size=15, max_workers=6):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def _process_chunk(self, examples, batch_index):\n",
    "        print(f\"[Batch {batch_index}] Starting processing with {len(examples)} examples\")\n",
    "\n",
    "        # Build joint prompt for the batch\n",
    "        prompt = (\n",
    "            \"Classify the relationship between the hypothesis and premise. \"\n",
    "            \"Respond for EACH example on a NEW LINE in the format:\\n\"\n",
    "            \"label || reason\\n\"\n",
    "            \"Label ∈ {entailment, neutral, contradiction}. \"\n",
    "            \"Reason: 1-2 sentence justification.\\n\\n\"\n",
    "        )\n",
    "\n",
    "        for idx, ex in enumerate(examples, start=1):\n",
    "            prompt += (\n",
    "                f\"Example {idx}:\\n\"\n",
    "                f\"Premise: {ex.premise}\\n\"\n",
    "                f\"Hypothesis: {ex.hypothesis}\\n\"\n",
    "            )\n",
    "\n",
    "        prompt += (\n",
    "            \"\\nNow output exactly \"\n",
    "            f\"{len(examples)} lines, one per example, in order, like:\\n\"\n",
    "            \"entailment || <reason>\\nneutral || <reason>\\n...\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Single API call for the batch\n",
    "        response = self.model(prompt)\n",
    "        #print(f\"raw response: {response}\")\n",
    "        response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "\n",
    "        # Parse the LLM output into (uid, label, reason)\n",
    "        results = []\n",
    "        lines = [line.strip() for line in response_text.splitlines() if line.strip()]\n",
    "\n",
    "        for ex, line in zip(examples, lines):\n",
    "            match = re.match(r\"(?i)\\s*(entailment|neutral|contradiction)\\s*\\|\\|\\s*(.*)\", line)\n",
    "            if match:\n",
    "                label = match.group(1).lower()\n",
    "                reason = match.group(2).strip()\n",
    "            else:\n",
    "                label, reason = \"unknown\", \"no reasoning\"\n",
    "            results.append((label, reason))\n",
    "\n",
    "        # Pad missing if LLM returned fewer lines\n",
    "        while len(results) < len(examples):\n",
    "            ex = examples[len(results)]\n",
    "            results.append((\"unknown\", \"no reasoning\"))\n",
    "\n",
    "        print(f\"[Batch {batch_index}] Finished processing\")\n",
    "        return results\n",
    "\n",
    "    def forward(self, examples):\n",
    "        if not examples:\n",
    "            return []\n",
    "\n",
    "        # Split into batches\n",
    "        chunks = [\n",
    "            examples[i:i+self.batch_size]\n",
    "            for i in range(0, len(examples), self.batch_size)\n",
    "        ]\n",
    "\n",
    "        results_by_index = {}\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self._process_chunk, chunk, idx): idx\n",
    "                for idx, chunk in enumerate(chunks)\n",
    "            }\n",
    "            for future in as_completed(futures):\n",
    "                batch_index = futures[future]\n",
    "                results_by_index[batch_index] = future.result()\n",
    "\n",
    "        # Flatten results in original order\n",
    "        ordered_results = []\n",
    "        for idx in sorted(results_by_index.keys()):\n",
    "            ordered_results.extend(results_by_index[idx])\n",
    "\n",
    "        return ordered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40cafe",
   "metadata": {},
   "source": [
    "Pipeline model: prompt the LLM to produce a CoT explanation of the relation (premise, hypothesis) - then, given the explanation, produce a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab9eaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BatchedNLIPipelinePredictor(dspy.Module):\n",
    "    \"\"\"\n",
    "    Two-stage NLI predictor:\n",
    "    1. Generate reasoning per example\n",
    "    2. Classify into entailment/neutral/contradiction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, batch_size=15, max_workers=6):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def _generate_reasoning(self, examples, batch_index):\n",
    "        \"\"\"Stage 1: Ask LLM for short reasoning per example\"\"\"\n",
    "        print(f\"[Batch {batch_index}] Generating reasoning for {len(examples)} examples\")\n",
    "\n",
    "        prompt = (\n",
    "            \"For each example, explain briefly (1-2 sentences) whether the hypothesis \"\n",
    "            \"follows from, contradicts, or is unrelated to the premise. \"\n",
    "            \"Output one line per example, reasoning only, no label.\\n\\n\"\n",
    "        )\n",
    "        for idx, ex in enumerate(examples, start=1):\n",
    "            prompt += (\n",
    "                f\"Example {idx}:\\n\"\n",
    "                f\"Premise: {ex.premise}\\n\"\n",
    "                f\"Hypothesis: {ex.hypothesis}\\n\"\n",
    "            )\n",
    "        prompt += (\n",
    "            f\"\\nOutput exactly {len(examples)} lines of reasoning, \"\n",
    "            \"one per example, in order.\"\n",
    "        )\n",
    "\n",
    "        # Single API call\n",
    "        response = self.model(prompt)\n",
    "        response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "\n",
    "        # Extract reasoning lines\n",
    "        reasonings = [line.strip() for line in response_text.splitlines() if line.strip()]\n",
    "\n",
    "        # Pad missing\n",
    "        while len(reasonings) < len(examples):\n",
    "            reasonings.append(\"No reasoning provided.\")\n",
    "\n",
    "        return reasonings[:len(examples)]\n",
    "\n",
    "    def _classify_from_reasoning(self, examples, reasonings, batch_index):\n",
    "        \"\"\"Stage 2: Use reasoning to classify into entailment/neutral/contradiction\"\"\"\n",
    "        print(f\"[Batch {batch_index}] Classifying examples based on reasoning\")\n",
    "\n",
    "        prompt = (\n",
    "            \"Classify the relationship between premise and hypothesis based on the reasoning provided.\\n\"\n",
    "            \"Respond in format: label || reasoning\\n\"\n",
    "            \"Label ∈ {entailment, neutral, contradiction}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "        for idx, (ex, reason) in enumerate(zip(examples, reasonings), start=1):\n",
    "            prompt += (\n",
    "                f\"Example {idx}:\\n\"\n",
    "                f\"Premise: {ex.premise}\\n\"\n",
    "                f\"Hypothesis: {ex.hypothesis}\\n\"\n",
    "                f\"Reasoning: {reason}\\n\"\n",
    "            )\n",
    "\n",
    "        prompt += (\n",
    "            f\"\\nNow output exactly {len(examples)} lines like:\\n\"\n",
    "            \"entailment || <reasoning>\\nneutral || <reasoning>\\n...\"\n",
    "        )\n",
    "\n",
    "        response = self.model(prompt)\n",
    "        response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "\n",
    "        results = []\n",
    "        lines = [line.strip() for line in response_text.splitlines() if line.strip()]\n",
    "        for ex, line in zip(examples, lines):\n",
    "            m = re.match(r\"(?i)\\s*(entailment|neutral|contradiction)\\s*\\|\\|\\s*(.*)\", line)\n",
    "            if m:\n",
    "                label = m.group(1).lower()\n",
    "                reasoning = m.group(2).strip()\n",
    "            else:\n",
    "                label, reasoning = \"unknown\", \"no reasoning\"\n",
    "            results.append((label, reasoning))\n",
    "\n",
    "        # Pad missing\n",
    "        while len(results) < len(examples):\n",
    "            results.append((\"unknown\", \"no reasoning\"))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _process_chunk(self, examples, batch_index):\n",
    "        \"\"\"Full two-stage pipeline for one batch\"\"\"\n",
    "        reasonings = self._generate_reasoning(examples, batch_index)\n",
    "        results = self._classify_from_reasoning(examples, reasonings, batch_index)\n",
    "        return results\n",
    "\n",
    "    def forward(self, examples):\n",
    "        if not examples:\n",
    "            return []\n",
    "\n",
    "        # Split into batches\n",
    "        chunks = [\n",
    "            examples[i:i+self.batch_size]\n",
    "            for i in range(0, len(examples), self.batch_size)\n",
    "        ]\n",
    "\n",
    "        results_by_index = {}\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self._process_chunk, chunk, idx): idx\n",
    "                for idx, chunk in enumerate(chunks)\n",
    "            }\n",
    "            for future in as_completed(futures):\n",
    "                batch_index = futures[future]\n",
    "                results_by_index[batch_index] = future.result()\n",
    "\n",
    "        # Flatten results in original order\n",
    "        ordered_results = []\n",
    "        for idx in sorted(results_by_index.keys()):\n",
    "            ordered_results.extend(results_by_index[idx])\n",
    "\n",
    "        return ordered_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bab6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating JOINT results on dev_r3\n",
      "\n",
      "\n",
      "[Batch 0] Starting processing with 15 examples\n",
      "[Batch 0] Finished processing\n",
      "[Batch 1] Starting processing with 15 examples\n",
      "[Batch 1] Finished processing\n",
      "[Batch 2] Starting processing with 15 examples\n",
      "[Batch 2] Finished processing\n",
      "[Batch 4] Starting processing with 15 examples\n",
      "[Batch 3] Starting processing with 15 examples\n",
      "[Batch 3] Finished processing\n",
      "[Batch 5] Starting processing with 15 examples\n",
      "[Batch 6] Starting processing with 15 examples\n",
      "[Batch 7] Starting processing with 15 examples\n",
      "[Batch 8] Starting processing with 15 examples\n",
      "[Batch 9] Starting processing with 15 examples\n",
      "[Batch 9] Finished processing\n",
      "[Batch 10] Starting processing with 15 examples\n",
      "[Batch 6] Finished processing\n",
      "[Batch 11] Starting processing with 15 examples\n",
      "[Batch 5] Finished processing\n",
      "[Batch 12] Starting processing with 15 examples\n",
      "[Batch 7] Finished processing\n",
      "[Batch 13] Starting processing with 15 examples\n",
      "[Batch 8] Finished processing\n",
      "[Batch 14] Starting processing with 15 examples\n",
      "[Batch 4] Finished processing\n",
      "[Batch 15] Starting processing with 15 examples\n",
      "[Batch 12] Finished processing\n",
      "[Batch 16] Starting processing with 15 examples\n",
      "[Batch 10] Finished processing\n",
      "[Batch 17] Starting processing with 15 examples\n",
      "[Batch 13] Finished processing\n",
      "[Batch 18] Starting processing with 15 examples\n",
      "[Batch 14] Finished processing\n",
      "[Batch 19] Starting processing with 15 examples\n",
      "[Batch 11] Finished processing\n",
      "[Batch 20] Starting processing with 15 examples\n",
      "[Batch 15] Finished processing\n",
      "[Batch 21] Starting processing with 15 examples\n",
      "[Batch 16] Finished processing\n",
      "[Batch 22] Starting processing with 15 examples\n",
      "[Batch 17] Finished processing\n",
      "[Batch 23] Starting processing with 15 examples\n",
      "[Batch 18] Finished processing\n",
      "[Batch 24] Starting processing with 15 examples\n",
      "[Batch 20] Finished processing\n",
      "[Batch 25] Starting processing with 15 examples\n",
      "[Batch 21] Finished processing\n",
      "[Batch 26] Starting processing with 15 examples\n",
      "[Batch 19] Finished processing\n",
      "[Batch 27] Starting processing with 15 examples\n",
      "[Batch 22] Finished processing\n",
      "[Batch 28] Starting processing with 15 examples\n",
      "[Batch 24] Finished processing\n",
      "[Batch 29] Starting processing with 15 examples\n",
      "[Batch 23] Finished processing\n",
      "[Batch 30] Starting processing with 15 examples\n",
      "[Batch 25] Finished processing\n",
      "[Batch 31] Starting processing with 15 examples\n",
      "[Batch 26] Finished processing\n",
      "[Batch 32] Starting processing with 15 examples\n",
      "[Batch 27] Finished processing\n",
      "[Batch 33] Starting processing with 15 examples\n",
      "[Batch 28] Finished processing\n",
      "[Batch 34] Starting processing with 15 examples\n",
      "[Batch 29] Finished processing\n",
      "[Batch 35] Starting processing with 15 examples\n",
      "[Batch 30] Finished processing\n",
      "[Batch 36] Starting processing with 15 examples\n",
      "[Batch 33] Finished processing\n",
      "[Batch 37] Starting processing with 15 examples\n",
      "[Batch 32] Finished processing\n",
      "[Batch 38] Starting processing with 15 examples\n",
      "[Batch 31] Finished processing\n",
      "[Batch 39] Starting processing with 15 examples\n",
      "[Batch 34] Finished processing\n",
      "[Batch 40] Starting processing with 15 examples\n",
      "[Batch 35] Finished processing\n",
      "[Batch 41] Starting processing with 15 examples\n",
      "[Batch 36] Finished processing\n",
      "[Batch 42] Starting processing with 15 examples\n",
      "[Batch 38] Finished processing\n",
      "[Batch 43] Starting processing with 15 examples\n",
      "[Batch 37] Finished processing\n",
      "[Batch 44] Starting processing with 15 examples\n",
      "[Batch 39] Finished processing\n",
      "[Batch 45] Starting processing with 15 examples\n",
      "[Batch 40] Finished processing\n",
      "[Batch 46] Starting processing with 15 examples\n",
      "[Batch 41] Finished processing\n",
      "[Batch 47] Starting processing with 15 examples\n",
      "[Batch 42] Finished processing\n",
      "[Batch 48] Starting processing with 15 examples\n",
      "[Batch 43] Finished processing\n",
      "[Batch 49] Starting processing with 15 examples\n",
      "[Batch 45] Finished processing\n",
      "[Batch 50] Starting processing with 15 examples\n",
      "[Batch 44] Finished processing\n",
      "[Batch 51] Starting processing with 15 examples\n",
      "[Batch 46] Finished processing\n",
      "[Batch 52] Starting processing with 15 examples\n",
      "[Batch 47] Finished processing\n",
      "[Batch 53] Starting processing with 15 examples\n",
      "[Batch 50] Finished processing\n",
      "[Batch 54] Starting processing with 15 examples\n",
      "[Batch 49] Finished processing\n",
      "[Batch 55] Starting processing with 15 examples\n",
      "[Batch 51] Finished processing\n",
      "[Batch 56] Starting processing with 15 examples\n",
      "[Batch 48] Finished processing\n",
      "[Batch 57] Starting processing with 15 examples\n",
      "[Batch 52] Finished processing\n",
      "[Batch 58] Starting processing with 15 examples\n",
      "[Batch 53] Finished processing\n",
      "[Batch 59] Starting processing with 15 examples\n",
      "[Batch 55] Finished processing\n",
      "[Batch 60] Starting processing with 15 examples\n",
      "[Batch 54] Finished processing\n",
      "[Batch 61] Starting processing with 15 examples\n",
      "[Batch 57] Finished processing\n",
      "[Batch 62] Starting processing with 15 examples\n",
      "[Batch 56] Finished processing\n",
      "[Batch 63] Starting processing with 15 examples\n",
      "[Batch 58] Finished processing\n",
      "[Batch 64] Starting processing with 15 examples\n",
      "[Batch 59] Finished processing\n",
      "[Batch 65] Starting processing with 15 examples\n",
      "[Batch 60] Finished processing\n",
      "[Batch 66] Starting processing with 15 examples\n",
      "[Batch 61] Finished processing\n",
      "[Batch 67] Starting processing with 15 examples\n",
      "[Batch 62] Finished processing\n",
      "[Batch 68] Starting processing with 15 examples\n",
      "[Batch 63] Finished processing\n",
      "[Batch 69] Starting processing with 15 examples\n",
      "[Batch 64] Finished processing\n",
      "[Batch 70] Starting processing with 15 examples\n",
      "[Batch 65] Finished processing\n",
      "[Batch 71] Starting processing with 15 examples\n",
      "[Batch 66] Finished processing\n",
      "[Batch 72] Starting processing with 15 examples\n",
      "[Batch 67] Finished processing\n",
      "[Batch 73] Starting processing with 15 examples\n",
      "[Batch 68] Finished processing\n",
      "[Batch 74] Starting processing with 15 examples\n",
      "[Batch 69] Finished processing\n",
      "[Batch 75] Starting processing with 15 examples\n",
      "[Batch 71] Finished processing\n",
      "[Batch 76] Starting processing with 15 examples\n",
      "[Batch 74] Finished processing\n",
      "[Batch 77] Starting processing with 15 examples\n",
      "[Batch 73] Finished processing\n",
      "[Batch 78] Starting processing with 15 examples\n",
      "[Batch 72] Finished processing\n",
      "[Batch 79] Starting processing with 15 examples\n",
      "[Batch 75] Finished processing\n",
      "[Batch 70] Finished processing\n",
      "[Batch 76] Finished processing\n",
      "[Batch 77] Finished processing\n",
      "[Batch 78] Finished processing\n",
      "[Batch 79] Finished processing\n",
      "\n",
      "\n",
      "Evaluating PIPELINE results on dev_r3\n",
      "\n",
      "\n",
      "[Batch 0] Generating reasoning for 15 examples\n",
      "[Batch 1] Generating reasoning for 15 examples\n",
      "[Batch 0] Classifying examples based on reasoning\n",
      "[Batch 1] Classifying examples based on reasoning\n",
      "[Batch 2] Generating reasoning for 15 examples\n",
      "[Batch 2] Classifying examples based on reasoning\n",
      "[Batch 3] Generating reasoning for 15 examples\n",
      "[Batch 3] Classifying examples based on reasoning\n",
      "[Batch 5] Generating reasoning for 15 examples\n",
      "[Batch 4] Generating reasoning for 15 examples\n",
      "[Batch 6] Generating reasoning for 15 examples\n",
      "[Batch 7] Generating reasoning for 15 examples\n",
      "[Batch 8] Generating reasoning for 15 examples\n",
      "[Batch 9] Generating reasoning for 15 examples\n",
      "[Batch 9] Classifying examples based on reasoning\n",
      "[Batch 7] Classifying examples based on reasoning\n",
      "[Batch 4] Classifying examples based on reasoning\n",
      "[Batch 6] Classifying examples based on reasoning\n",
      "[Batch 8] Classifying examples based on reasoning\n",
      "[Batch 5] Classifying examples based on reasoning\n",
      "[Batch 10] Generating reasoning for 15 examples\n",
      "[Batch 11] Generating reasoning for 15 examples\n",
      "[Batch 12] Generating reasoning for 15 examples\n",
      "[Batch 13] Generating reasoning for 15 examples\n",
      "[Batch 14] Generating reasoning for 15 examples\n",
      "[Batch 10] Classifying examples based on reasoning\n",
      "[Batch 15] Generating reasoning for 15 examples\n",
      "[Batch 12] Classifying examples based on reasoning\n",
      "[Batch 13] Classifying examples based on reasoning\n",
      "[Batch 11] Classifying examples based on reasoning\n",
      "[Batch 16] Generating reasoning for 15 examples\n",
      "[Batch 14] Classifying examples based on reasoning\n",
      "[Batch 15] Classifying examples based on reasoning\n",
      "[Batch 17] Generating reasoning for 15 examples\n",
      "[Batch 18] Generating reasoning for 15 examples\n",
      "[Batch 19] Generating reasoning for 15 examples\n",
      "[Batch 20] Generating reasoning for 15 examples\n",
      "[Batch 16] Classifying examples based on reasoning\n",
      "[Batch 21] Generating reasoning for 15 examples\n",
      "[Batch 19] Classifying examples based on reasoning\n",
      "[Batch 20] Classifying examples based on reasoning\n",
      "[Batch 22] Generating reasoning for 15 examples\n",
      "[Batch 18] Classifying examples based on reasoning\n",
      "[Batch 21] Classifying examples based on reasoning\n",
      "[Batch 17] Classifying examples based on reasoning\n",
      "[Batch 23] Generating reasoning for 15 examples\n",
      "[Batch 24] Generating reasoning for 15 examples\n",
      "[Batch 22] Classifying examples based on reasoning\n",
      "[Batch 25] Generating reasoning for 15 examples\n",
      "[Batch 26] Generating reasoning for 15 examples\n",
      "[Batch 27] Generating reasoning for 15 examples\n",
      "[Batch 23] Classifying examples based on reasoning\n",
      "[Batch 28] Generating reasoning for 15 examples\n",
      "[Batch 25] Classifying examples based on reasoning\n",
      "[Batch 24] Classifying examples based on reasoning\n",
      "[Batch 28] Classifying examples based on reasoning\n",
      "[Batch 29] Generating reasoning for 15 examples\n",
      "[Batch 27] Classifying examples based on reasoning\n",
      "[Batch 26] Classifying examples based on reasoning\n",
      "[Batch 30] Generating reasoning for 15 examples\n",
      "[Batch 31] Generating reasoning for 15 examples\n",
      "[Batch 32] Generating reasoning for 15 examples\n",
      "[Batch 29] Classifying examples based on reasoning\n",
      "[Batch 33] Generating reasoning for 15 examples\n",
      "[Batch 31] Classifying examples based on reasoning\n",
      "[Batch 30] Classifying examples based on reasoning\n",
      "[Batch 34] Generating reasoning for 15 examples\n",
      "[Batch 32] Classifying examples based on reasoning\n",
      "[Batch 35] Generating reasoning for 15 examples\n",
      "[Batch 36] Generating reasoning for 15 examples\n",
      "[Batch 33] Classifying examples based on reasoning\n",
      "[Batch 37] Generating reasoning for 15 examples\n",
      "[Batch 34] Classifying examples based on reasoning\n",
      "[Batch 38] Generating reasoning for 15 examples\n",
      "[Batch 35] Classifying examples based on reasoning\n",
      "[Batch 36] Classifying examples based on reasoning\n",
      "[Batch 39] Generating reasoning for 15 examples\n",
      "[Batch 37] Classifying examples based on reasoning\n",
      "[Batch 40] Generating reasoning for 15 examples\n",
      "[Batch 41] Generating reasoning for 15 examples\n",
      "[Batch 38] Classifying examples based on reasoning\n",
      "[Batch 39] Classifying examples based on reasoning\n",
      "[Batch 42] Generating reasoning for 15 examples\n",
      "[Batch 40] Classifying examples based on reasoning\n",
      "[Batch 41] Classifying examples based on reasoning[Batch 43] Generating reasoning for 15 examples\n",
      "\n",
      "[Batch 44] Generating reasoning for 15 examples\n",
      "[Batch 45] Generating reasoning for 15 examples\n",
      "[Batch 42] Classifying examples based on reasoning\n",
      "[Batch 46] Generating reasoning for 15 examples\n",
      "[Batch 47] Generating reasoning for 15 examples\n",
      "[Batch 43] Classifying examples based on reasoning\n",
      "[Batch 44] Classifying examples based on reasoning\n",
      "[Batch 46] Classifying examples based on reasoning\n",
      "[Batch 45] Classifying examples based on reasoning\n",
      "[Batch 48] Generating reasoning for 15 examples\n",
      "[Batch 47] Classifying examples based on reasoning\n",
      "[Batch 49] Generating reasoning for 15 examples\n",
      "[Batch 50] Generating reasoning for 15 examples\n",
      "[Batch 51] Generating reasoning for 15 examples\n",
      "[Batch 52] Generating reasoning for 15 examples\n",
      "[Batch 49] Classifying examples based on reasoning\n",
      "[Batch 48] Classifying examples based on reasoning\n",
      "[Batch 53] Generating reasoning for 15 examples\n",
      "[Batch 50] Classifying examples based on reasoning\n",
      "[Batch 51] Classifying examples based on reasoning\n",
      "[Batch 52] Classifying examples based on reasoning\n",
      "[Batch 54] Generating reasoning for 15 examples\n",
      "[Batch 55] Generating reasoning for 15 examples\n",
      "[Batch 53] Classifying examples based on reasoning\n",
      "[Batch 56] Generating reasoning for 15 examples\n",
      "[Batch 57] Generating reasoning for 15 examples\n",
      "[Batch 58] Generating reasoning for 15 examples\n",
      "[Batch 59] Generating reasoning for 15 examples\n",
      "[Batch 54] Classifying examples based on reasoning\n",
      "[Batch 55] Classifying examples based on reasoning\n",
      "[Batch 56] Classifying examples based on reasoning\n",
      "[Batch 57] Classifying examples based on reasoning\n",
      "[Batch 59] Classifying examples based on reasoning\n",
      "[Batch 58] Classifying examples based on reasoning\n",
      "[Batch 60] Generating reasoning for 15 examples\n",
      "[Batch 61] Generating reasoning for 15 examples\n",
      "[Batch 62] Generating reasoning for 15 examples\n",
      "[Batch 63] Generating reasoning for 15 examples\n",
      "[Batch 64] Generating reasoning for 15 examples\n",
      "[Batch 65] Generating reasoning for 15 examples\n",
      "[Batch 60] Classifying examples based on reasoning\n",
      "[Batch 62] Classifying examples based on reasoning\n",
      "[Batch 61] Classifying examples based on reasoning\n",
      "[Batch 65] Classifying examples based on reasoning\n",
      "[Batch 64] Classifying examples based on reasoning\n",
      "[Batch 66] Generating reasoning for 15 examples\n",
      "[Batch 67] Generating reasoning for 15 examples\n",
      "[Batch 68] Generating reasoning for 15 examples\n",
      "[Batch 63] Classifying examples based on reasoning\n",
      "[Batch 66] Classifying examples based on reasoning\n",
      "[Batch 69] Generating reasoning for 15 examples\n",
      "[Batch 67] Classifying examples based on reasoning\n",
      "[Batch 70] Generating reasoning for 15 examples\n",
      "[Batch 68] Classifying examples based on reasoning\n",
      "[Batch 71] Generating reasoning for 15 examples\n",
      "[Batch 69] Classifying examples based on reasoning\n",
      "[Batch 72] Generating reasoning for 15 examples\n",
      "[Batch 73] Generating reasoning for 15 examples\n",
      "[Batch 74] Generating reasoning for 15 examples\n",
      "[Batch 75] Generating reasoning for 15 examples\n",
      "[Batch 70] Classifying examples based on reasoning\n",
      "[Batch 71] Classifying examples based on reasoning\n",
      "[Batch 72] Classifying examples based on reasoning\n",
      "[Batch 74] Classifying examples based on reasoning\n",
      "[Batch 76] Generating reasoning for 15 examples\n",
      "[Batch 73] Classifying examples based on reasoning\n",
      "[Batch 77] Generating reasoning for 15 examples\n",
      "[Batch 78] Generating reasoning for 15 examples\n",
      "[Batch 79] Generating reasoning for 15 examples\n",
      "[Batch 76] Classifying examples based on reasoning\n",
      "[Batch 75] Classifying examples based on reasoning\n",
      "[Batch 77] Classifying examples based on reasoning\n",
      "[Batch 78] Classifying examples based on reasoning\n",
      "[Batch 79] Classifying examples based on reasoning\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nEvaluating JOINT results on dev_r3\\n\\n\")\n",
    "\n",
    "dev_r3_llm_joint_results = test_nli_predictor(\n",
    "    dataset=dataset,\n",
    "    split_name='dev_r3',\n",
    "    model=lm,\n",
    "    predictor_class=BatchedNLIJointPredictor,\n",
    "    example_extractor=extract_example,\n",
    "    batch_size=15,\n",
    "    max_workers=6,\n",
    "    max_examples=1200,\n",
    "    uid_key='uid'\n",
    ")\n",
    "\n",
    "print(\"\\n\\nEvaluating PIPELINE results on dev_r3\\n\\n\")\n",
    "\n",
    "dev_r3_llm_pipeline_results = test_nli_predictor(\n",
    "    dataset=dataset,\n",
    "    split_name='dev_r3',\n",
    "    model=lm,\n",
    "    predictor_class=BatchedNLIPipelinePredictor,\n",
    "    example_extractor=extract_example,\n",
    "    batch_size=15,\n",
    "    max_workers=6,\n",
    "    max_examples=1200,\n",
    "    uid_key='uid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e6c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating JOINT model results on dev_r3\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "  Accuracy:  0.6458333333333334\n",
      "  Precision: 0.6431048878911784\n",
      "  Recall:    0.6458333333333334\n",
      "  F1:        0.6441062850126622\n",
      "\n",
      "\n",
      "Evaluating PIPELINE model results on dev_r3\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "  Accuracy:  0.625\n",
      "  Precision: 0.6232593331744185\n",
      "  Recall:    0.625\n",
      "  F1:        0.6218908466082799\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nEvaluating JOINT model results on dev_r3\\n\\n\")\n",
    "evaluate_predictions(\n",
    "    ordered_llm_predictions=dev_r3_llm_joint_results,\n",
    "    gold_labels=dataset['dev_r3']['label'],\n",
    "    label2id={\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    ")\n",
    "\n",
    "print(\"\\n\\nEvaluating PIPELINE model results on dev_r3\\n\\n\")\n",
    "evaluate_predictions(\n",
    "    ordered_llm_predictions=dev_r3_llm_pipeline_results,\n",
    "    gold_labels=dataset['dev_r3']['label'],\n",
    "    label2id={\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf690d1",
   "metadata": {},
   "source": [
    "\n",
    "4) Define a similarity threshold, \"acceptable\" - see \"senten\"ce-transformers.ipynb\", \n",
    "https://dspy.ai/tutorials/output_refinement/best-of-n-and-refine/\n",
    "5) Define a similarity test. **lets SUM the number of acceptable reasons, and print that.** \n",
    "6) Perform the 3 passages tests\n",
    "7) print the results in a clean and meaningful manner\n",
    "10) We need to justify the hybrid approach - using dspy as wrapper but manually prompting , and also verify if he will downgrade for that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50b18e",
   "metadata": {},
   "source": [
    "We picked the general purpose model \"all-MiniLM-L6-v2\" over 2 other options:\n",
    "1) all-mpnet-base-v2 : has slightly better performance , but much slower (x5)\n",
    "2) STS models such as distiluse-base-multilingual-cased-v1, distiluse-base-multilingual-cased-v2: trained on less parameters and weaker for english since they are Multilingual\n",
    "\n",
    "See https://sbert.net/docs/sentence_transformer/pretrained_models.html for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6832f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbbb9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load Hugging Face metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n",
    "f1_metric = load(\"f1\")\n",
    "\n",
    "\n",
    "def optimize_reasoning_threshold(examples, model, test_size=0.2, thresholds=None):\n",
    "    \"\"\"\n",
    "    Manually optimize semantic similarity threshold for reason relevance detection.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): Each dict must have keys: 'premise', 'hypothesis', 'reason', 'label'\n",
    "        model: Preloaded SentenceTransformer\n",
    "        test_size (float): Fraction for validation split\n",
    "        thresholds (list[float]): Thresholds to try (defaults to np.arange(0.3,0.91,0.05))\n",
    "\n",
    "    Returns:\n",
    "        (float, int): best threshold, count of items above it\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.3, 0.91, 0.05)\n",
    "\n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(examples, test_size=test_size, random_state=42)\n",
    "    print(f\"[DEBUG] Validation examples: {len(val_data)}\")\n",
    "\n",
    "    # Encode embeddings for validation\n",
    "    val_embeddings = []\n",
    "    for ex in val_data:\n",
    "        ph_text = ex[\"premise\"] + \" \" + ex[\"hypothesis\"]\n",
    "        ph_emb = model.encode(ph_text, convert_to_tensor=True)\n",
    "        r_emb = model.encode(ex[\"reason\"], convert_to_tensor=True)\n",
    "        val_embeddings.append((ph_emb, r_emb, ex[\"label\"]))\n",
    "\n",
    "    # Manual threshold search\n",
    "    best_f1 = -1\n",
    "    best_threshold = None\n",
    "    best_sims = None\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        preds, labels, sims = [], [], []\n",
    "        for ph_emb, r_emb, label in val_embeddings:\n",
    "            sim = util.cos_sim(ph_emb, r_emb).item()\n",
    "            sims.append(sim)\n",
    "            preds.append(int(sim >= thresh))\n",
    "            labels.append(label)\n",
    "\n",
    "        f1 = f1_metric.compute(predictions=preds, references=labels)['f1']\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thresh\n",
    "            best_sims = sims\n",
    "\n",
    "        print(f\"[DEBUG] Threshold {thresh:.2f} → F1={f1:.3f}\")\n",
    "\n",
    "    above_count = sum(s >= best_threshold for s in best_sims)\n",
    "    print(f\"\\nOptimal Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Number of items above threshold: {above_count} / {len(best_sims)}\")\n",
    "    print(f\"Percentage above threshold: {above_count / len(best_sims) * 100:.2f}%\")\n",
    "    print(f\"Best F1 Score: {best_f1:.3f}\")\n",
    "\n",
    "    return best_threshold, above_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45092bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_ph_with_reasons_ordered(test_r3, results):\n",
    "    \"\"\"\n",
    "    Merge PH pairs from test_r3 with results and create binary label\n",
    "    indicating if dataset label matches result label.\n",
    "\n",
    "    Args:\n",
    "        test_r3: Dataset with features ['uid','premise','hypothesis','label',...]\n",
    "        results: List of tuples (uid, label_str, reason_text)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with: 'uid','premise','hypothesis','reason','label' (binary)\n",
    "    \"\"\"\n",
    "    # Each int label maps to a set containing its string and int\n",
    "    label_map = {\n",
    "        0: {\"entailment\", 0},\n",
    "        1: {\"neutral\", 1},\n",
    "        2: {\"contradiction\", 2},\n",
    "    }\n",
    "\n",
    "    merged = []\n",
    "    for row, (res_uid, res_label, reason_text) in zip(test_r3, results):\n",
    "        assert row[\"uid\"] == res_uid, f\"UID mismatch: {row['uid']} != {res_uid}\"\n",
    "\n",
    "        label_set = label_map[row[\"label\"]]\n",
    "        binary_label = 1 if res_label in label_set else 0\n",
    "\n",
    "        merged.append({\n",
    "            \"uid\": row[\"uid\"],\n",
    "            \"premise\": row[\"premise\"],\n",
    "            \"hypothesis\": row[\"hypothesis\"],\n",
    "            \"reason\": reason_text,\n",
    "            \"label\": binary_label\n",
    "        })\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# test1_pipeline = unify_ph_with_reasons_ordered(dataset['dev_r3'], dev_r3_llm_pipeline_results)\n",
    "\n",
    "# for i in range(30):\n",
    "#     print(test1_pipeline[i])  # Print first 3 examples for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5aeb9",
   "metadata": {},
   "source": [
    "# TEST 1: Human reason vs. LLM reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9916d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6e15205",
   "metadata": {},
   "source": [
    "# TEST 2: (premise,hypothesis) vs. LLM reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3a25728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Comparing the joint prompt model with the (premise, hypothesis) pair****\n",
      "[DEBUG] Validation examples: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wexle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Threshold 0.30 → F1=0.762\n",
      "[DEBUG] Threshold 0.35 → F1=0.756\n",
      "[DEBUG] Threshold 0.40 → F1=0.751\n",
      "[DEBUG] Threshold 0.45 → F1=0.737\n",
      "[DEBUG] Threshold 0.50 → F1=0.693\n",
      "[DEBUG] Threshold 0.55 → F1=0.620\n",
      "[DEBUG] Threshold 0.60 → F1=0.537\n",
      "[DEBUG] Threshold 0.65 → F1=0.432\n",
      "[DEBUG] Threshold 0.70 → F1=0.336\n",
      "[DEBUG] Threshold 0.75 → F1=0.202\n",
      "[DEBUG] Threshold 0.80 → F1=0.084\n",
      "[DEBUG] Threshold 0.85 → F1=0.000\n",
      "[DEBUG] Threshold 0.90 → F1=0.000\n",
      "\n",
      "Optimal Threshold: 0.300\n",
      "Number of items above threshold: 214 / 240\n",
      "Percentage above threshold: 89.17%\n",
      "Best F1 Score: 0.762\n",
      "\n",
      "\n",
      "\n",
      "***Comparing the PIPELINE prompt model with the (premise, hypothesis) pair****\n",
      "[DEBUG] Validation examples: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wexle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Threshold 0.30 → F1=0.730\n",
      "[DEBUG] Threshold 0.35 → F1=0.707\n",
      "[DEBUG] Threshold 0.40 → F1=0.679\n",
      "[DEBUG] Threshold 0.45 → F1=0.653\n",
      "[DEBUG] Threshold 0.50 → F1=0.612\n",
      "[DEBUG] Threshold 0.55 → F1=0.544\n",
      "[DEBUG] Threshold 0.60 → F1=0.470\n",
      "[DEBUG] Threshold 0.65 → F1=0.317\n",
      "[DEBUG] Threshold 0.70 → F1=0.171\n",
      "[DEBUG] Threshold 0.75 → F1=0.066\n",
      "[DEBUG] Threshold 0.80 → F1=0.014\n",
      "[DEBUG] Threshold 0.85 → F1=0.000\n",
      "[DEBUG] Threshold 0.90 → F1=0.000\n",
      "\n",
      "Optimal Threshold: 0.300\n",
      "Number of items above threshold: 214 / 240\n",
      "Percentage above threshold: 89.17%\n",
      "Best F1 Score: 0.730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.3), np.int64(214))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_joint = unify_ph_with_reasons_ordered(dataset['dev_r3'], dev_r3_llm_joint_results)\n",
    "print(\"***Comparing the joint prompt model with the (premise, hypothesis) pair****\")\n",
    "optimize_reasoning_threshold(test2_joint, model)\n",
    "print(\"\\n\\n\")\n",
    "test2_pipeline = unify_ph_with_reasons_ordered(dataset['dev_r3'], dev_r3_llm_pipeline_results)\n",
    "print(\"***Comparing the PIPELINE prompt model with the (premise, hypothesis) pair****\")\n",
    "optimize_reasoning_threshold(test2_pipeline, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81796450",
   "metadata": {},
   "source": [
    "# Test 3: (premise,hypothesis) vs. HUMAN reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f570cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Comparing the given HUMAN reasons with the (premise, hypothesis) pair****\n",
      "[DEBUG] Validation examples: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wexle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Threshold 0.30 → F1=0.832\n",
      "[DEBUG] Threshold 0.35 → F1=0.763\n",
      "[DEBUG] Threshold 0.40 → F1=0.689\n",
      "[DEBUG] Threshold 0.45 → F1=0.617\n",
      "[DEBUG] Threshold 0.50 → F1=0.476\n",
      "[DEBUG] Threshold 0.55 → F1=0.416\n",
      "[DEBUG] Threshold 0.60 → F1=0.328\n",
      "[DEBUG] Threshold 0.65 → F1=0.235\n",
      "[DEBUG] Threshold 0.70 → F1=0.147\n",
      "[DEBUG] Threshold 0.75 → F1=0.080\n",
      "[DEBUG] Threshold 0.80 → F1=0.033\n",
      "[DEBUG] Threshold 0.85 → F1=0.008\n",
      "[DEBUG] Threshold 0.90 → F1=0.000\n",
      "\n",
      "Optimal Threshold: 0.300\n",
      "Number of items above threshold: 171 / 240\n",
      "Percentage above threshold: 71.25%\n",
      "Best F1 Score: 0.832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.3), np.int64(171))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_results = [(row['uid'], row['label'], row['reason']) for row in dataset['dev_r3']]\n",
    "test3 = unify_ph_with_reasons_ordered(dataset['dev_r3'], given_results)\n",
    "print(\"***Comparing the given HUMAN reasons with the (premise, hypothesis) pair****\")\n",
    "optimize_reasoning_threshold(test3, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
