{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "\n",
    "# class NLIClassifier(dspy.Signature):\n",
    "#     premise = dspy.InputField(desc=\"A factual statement\")\n",
    "#     hypothesis = dspy.InputField(desc=\"A statement to evaluate against the premise\")\n",
    "#     label = dspy.OutputField(\n",
    "#         desc=\"The relationship between premise and hypothesis: entailment, neutral, or contradiction\",\n",
    "#         choices=[\"entailment\", \"neutral\", \"contradiction\"]\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Create a Predict module\n",
    "# nli_predict = dspy.Predict(NLIClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9b84927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class BatchedNLIPredictor(dspy.Module):\n",
    "    def __init__(self, model, batch_size=15):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, examples):\n",
    "        # examples: list of dspy.Example(premise=..., hypothesis=...)\n",
    "        \n",
    "        # Build a single prompt with multiple pairs\n",
    "        prompt = \"Choose the correct relationship between the hypothesis and the premise: entailment/neutral/contradiction  .\\n\\n\"\n",
    "        for i, ex in enumerate(examples, start=1):\n",
    "            prompt += f\"Example {i}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "        # Single LLM call\n",
    "        response = self.model(prompt)\n",
    "\n",
    "        # Parse the response line by line\n",
    "        predictions = []\n",
    "        for line in response.splitlines():\n",
    "            if line.strip().lower().startswith((\"entailment\", \"neutral\", \"contradiction\")):\n",
    "                predictions.append(line.strip().lower())\n",
    "\n",
    "        # Ensure same length\n",
    "        while len(predictions) < len(examples):\n",
    "            predictions.append(\"unknown\")\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b262f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "batched_predictor = BatchedNLIPredictor(model=lm, batch_size=15)\n",
    "\n",
    "def process_chunk(chunk_examples, chunk_uids, batch_index):\n",
    "    # Build single prompt for this batch\n",
    "    prompt = \"Classify the hypothesis and premise relationship: entailment / neutral / contradiction. **provide 1 word answer**.\\n\\n\"\n",
    "    for idx, ex in enumerate(chunk_examples, start=1):\n",
    "        prompt += f\"Example {idx}:\\nPremise: {ex.premise}\\nHypothesis: {ex.hypothesis}\\nAnswer (entailment/neutral/contradiction):\\n\"\n",
    "\n",
    "    # LLM call (one API call per chunk)\n",
    "    print(f\"Processing batch {batch_index} with {len(chunk_examples)} examples\")\n",
    "    response = batched_predictor.model(prompt)\n",
    "    print(f\"Done with batch {batch_index}\")\n",
    "    \n",
    "\n",
    "    # Handle response text\n",
    "    response_text = \"\\n\".join(response) if isinstance(response, list) else str(response)\n",
    "    lines = response_text.splitlines()\n",
    "\n",
    "    # Extract predictions\n",
    "    predictions = []\n",
    "    for line in lines:\n",
    "        m = re.search(r\"(entailment|neutral|contradiction)\", line, re.IGNORECASE)\n",
    "        if m:\n",
    "            predictions.append(m.group(1).lower())\n",
    "\n",
    "    # Pad missing\n",
    "    while len(predictions) < len(chunk_examples):\n",
    "        predictions.append(\"unknown\")\n",
    "\n",
    "    return batch_index, list(zip(chunk_uids, predictions))\n",
    "\n",
    "\n",
    "def predict_batch_parallel(batch, batch_size=15, max_workers=6):\n",
    "    uids = list(batch[\"uid\"])\n",
    "    premises = list(batch[\"premise\"])\n",
    "    hypotheses = list(batch[\"hypothesis\"])\n",
    "\n",
    "    examples = [\n",
    "        dspy.Example(premise=p, hypothesis=h)\n",
    "        for p, h in zip(premises, hypotheses)\n",
    "    ]\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(examples), batch_size):\n",
    "            chunk = examples[i:i+batch_size]\n",
    "            chunk_uids = uids[i:i+batch_size]\n",
    "            batch_index = i // batch_size\n",
    "            futures.append(executor.submit(process_chunk, chunk, chunk_uids, batch_index))\n",
    "\n",
    "        # Collect results in order\n",
    "        results_by_index = {}\n",
    "        for future in as_completed(futures):\n",
    "            batch_index, batch_result = future.result()\n",
    "            results_by_index[batch_index] = batch_result\n",
    "\n",
    "    # Flatten results in original order\n",
    "    ordered_results = []\n",
    "    for batch_index in sorted(results_by_index.keys()):\n",
    "        ordered_results.extend(results_by_index[batch_index])\n",
    "\n",
    "    return ordered_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with 15 examplesProcessing batch 1 with 3 examples\n",
      "\n",
      "Done with batch 0\n",
      "Done with batch 1\n",
      "b0e63408-53af-4b46-b33d-bf5ba302949f neutral\n",
      "41ac8273-490a-4c14-adc9-28e7992b40e3 entailment\n",
      "9b4b2be0-7f5e-456f-b7af-627309123ad0 neutral\n",
      "db7fef31-4f2f-4b5a-855e-831209eab172 neutral\n",
      "4f73b484-af35-4922-8f90-4881682041cd contradiction\n",
      "769d15ea-f94c-4387-b6db-04f7121e420e entailment\n",
      "6c59f001-b2cc-4a9a-a4a8-e04ccc73e4d3 entailment\n",
      "9cc974da-688c-4fc5-9d4b-475ec410576e entailment\n",
      "641310d4-2120-4fa9-98a2-7f750ae42c72 neutral\n",
      "33fd6df2-0810-49c2-8fe2-662229badebd entailment\n",
      "8fc6bda7-e103-4c8c-8802-b4dab96a1734 entailment\n",
      "7d98f706-81fe-4160-ad9d-4011a6a1dad6 entailment\n",
      "ec8e2a30-c20b-4b7b-af9d-fccbbb2d9906 entailment\n",
      "433e53c4-c555-4e65-bc02-ff8fc7ced582 entailment\n",
      "49d6eafb-5f08-4b5c-996d-106614ee1092 contradiction\n",
      "f1ed4114-24bd-49ae-8026-24464026970d entailment\n",
      "2b6616c8-d20e-49c1-8a44-767570d5baf1 contradiction\n",
      "d38256b1-bd97-4eb0-ac1d-0777bea1fb58 entailment\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_r3 = dataset['test_r3']\n",
    "mini_test_r3 = test_r3.select(range(18))\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Run in an asyncio event loop\n",
    "ordered_predictions = predict_batch_parallel(mini_test_r3, batch_size=15)\n",
    "\n",
    "for uid, label in ordered_predictions:\n",
    "    print(uid, label)\n",
    "\n",
    "# original_uids = mini_test_r3[\"uid\"]\n",
    "# print(mini_test_r3[\"uid\"] == [uid for uid, _ in ordered_predictions])\n",
    "# print(len(mini_test_r3[\"uid\"]) == len([uid for uid, _ in ordered_predictions]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81ea480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of mini_test_r3: 65\n",
      "len of original_uids: 65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"len of mini_test_r3:\", len(mini_test_r3))\n",
    "print(\"len of original_uids:\", len(mini_test_r3[\"uid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
