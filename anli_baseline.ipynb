{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:42<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95a54b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pred_test_r3' (list)\n"
     ]
    }
   ],
   "source": [
    "%store pred_test_r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluate module for combine function (use alias to avoid conflict)\n",
    "import evaluate as eval_lib\n",
    "clf_metrics = eval_lib.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0367154",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19358603",
   "metadata": {},
   "source": [
    "## 1.1. Execute the NLI Notebook\n",
    "\n",
    "**Task 1.1**: Implement baseline NLI evaluation on ANLI dataset with non-empty 'reason' fields using Hugging Face evaluate package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4119c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - test_r1: 1000/1000 samples have non-empty reasons ✅\n",
      "   - test_r2: 1000/1000 samples have non-empty reasons ✅\n",
      "   - test_r3: 1200/1200 samples have non-empty reasons ✅\n",
      "================================================================================\n",
      "\n",
      " Evaluating on test_r1...\n",
      "Number of samples with non-empty reasons: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Results for test_r1 (using evaluate package):\n",
      "- Samples with reasons: 1000\n",
      "- Accuracy: 0.619\n",
      "- F1 (macro): 0.605\n",
      "- Precision (macro): 0.633\n",
      "- Recall (macro): 0.619\n",
      "\n",
      " Per-Class Results for test_r1:\n",
      "    entailment: F1=0.713, Precision=0.697, Recall=0.731\n",
      "       neutral: F1=0.460, Precision=0.656, Recall=0.354\n",
      "  contradiction: F1=0.640, Precision=0.547, Recall=0.772\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick verification that all samples have non-empty reasons\n",
    "for section in ['test_r1', 'test_r2', 'test_r3']:\n",
    "    samples_with_reason = sum(1 for x in dataset[section] if x['reason'] and x['reason'].strip())\n",
    "    print(f\"   - {section}: {samples_with_reason}/{len(dataset[section])} samples have non-empty reasons ✅\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_metrics_with_evaluate(predictions, section_name):\n",
    "    \"\"\"\n",
    "    Compute classification metrics using Hugging Face evaluate package\n",
    "    Task 1.1 implementation - no sklearn usage\n",
    "    \"\"\"\n",
    "    # Extract predictions and gold labels\n",
    "    pred_labels = [p['pred_label'] for p in predictions]\n",
    "    gold_labels = [p['gold_label'] for p in predictions]\n",
    "    \n",
    "    # Map labels to integers for metrics computation\n",
    "    label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    pred_ints = [label_to_int[label] for label in pred_labels]\n",
    "    gold_ints = [label_to_int[label] for label in gold_labels]\n",
    "    \n",
    "    # Use evaluate package metrics with correct parameters for 3-class classification\n",
    "    accuracy_result = accuracy.compute(predictions=pred_ints, references=gold_ints)\n",
    "    f1_result = f1.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    precision_result = precision.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    recall_result = recall.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    \n",
    "    # Get per-class metrics for detailed analysis\n",
    "    f1_per_class = f1.compute(predictions=pred_ints, references=gold_ints, average=None)\n",
    "    precision_per_class = precision.compute(predictions=pred_ints, references=gold_ints, average=None)\n",
    "    recall_per_class = recall.compute(predictions=pred_ints, references=gold_ints, average=None)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy_result['accuracy'],\n",
    "        'f1': f1_result['f1'],\n",
    "        'precision': precision_result['precision'],\n",
    "        'recall': recall_result['recall'],\n",
    "        'f1_per_class': f1_per_class['f1'],\n",
    "        'precision_per_class': precision_per_class['precision'],\n",
    "        'recall_per_class': recall_per_class['recall']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Results for {section_name} (using evaluate package):\")\n",
    "    print(f\"- Samples with reasons: {len(predictions)}\")\n",
    "    print(f\"- Accuracy: {results['accuracy']:.3f}\")\n",
    "    print(f\"- F1 (macro): {results['f1']:.3f}\")\n",
    "    print(f\"- Precision (macro): {results['precision']:.3f}\")\n",
    "    print(f\"- Recall (macro): {results['recall']:.3f}\")\n",
    "    \n",
    "    # Detailed per-class metrics\n",
    "    print(f\"\\n Per-Class Results for {section_name}:\")\n",
    "    class_names = ['entailment', 'neutral', 'contradiction']\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  {class_name:>12}: F1={results['f1_per_class'][i]:.3f}, \"\n",
    "              f\"Precision={results['precision_per_class'][i]:.3f}, \"\n",
    "              f\"Recall={results['recall_per_class'][i]:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 1: Evaluate on test_r1\n",
    "print(f\"\\n Evaluating on test_r1...\")\n",
    "print(f\"Number of samples with non-empty reasons: {len(dataset['test_r1'])}\")\n",
    "pred_test_r1 = evaluate_on_dataset(dataset['test_r1'])\n",
    "results_r1 = compute_metrics_with_evaluate(pred_test_r1, \"test_r1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c15721c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on test_r2...\n",
      "Number of samples with non-empty reasons: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:57<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Results for test_r2 (using evaluate package):\n",
      "- Samples with reasons: 1000\n",
      "- Accuracy: 0.504\n",
      "- F1 (macro): 0.489\n",
      "- Precision (macro): 0.508\n",
      "- Recall (macro): 0.504\n",
      "\n",
      " Per-Class Results for test_r2:\n",
      "    entailment: F1=0.552, Precision=0.538, Recall=0.566\n",
      "       neutral: F1=0.360, Precision=0.508, Recall=0.279\n",
      "  contradiction: F1=0.556, Precision=0.476, Recall=0.667\n",
      "\n",
      " Computing metrics for test_r3...\n",
      "Number of samples with non-empty reasons: 1200\n",
      "\n",
      " Results for test_r3 (using evaluate package):\n",
      "- Samples with reasons: 1200\n",
      "- Accuracy: 0.481\n",
      "- F1 (macro): 0.463\n",
      "- Precision (macro): 0.465\n",
      "- Recall (macro): 0.482\n",
      "\n",
      " Per-Class Results for test_r3:\n",
      "    entailment: F1=0.562, Precision=0.556, Recall=0.567\n",
      "       neutral: F1=0.273, Precision=0.362, Recall=0.219\n",
      "  contradiction: F1=0.554, Precision=0.477, Recall=0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluate on test_r2\n",
    "print(f\"\\n Evaluating on test_r2...\")\n",
    "print(f\"Number of samples with non-empty reasons: {len(dataset['test_r2'])}\")\n",
    "pred_test_r2 = evaluate_on_dataset(dataset['test_r2'])\n",
    "results_r2 = compute_metrics_with_evaluate(pred_test_r2, \"test_r2\")\n",
    "\n",
    "# Step 3: Evaluate on test_r3 (using existing results, computing metrics)\n",
    "print(f\"\\n Computing metrics for test_r3...\")\n",
    "print(f\"Number of samples with non-empty reasons: {len(dataset['test_r3'])}\")\n",
    "results_r3 = compute_metrics_with_evaluate(pred_test_r3, \"test_r3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd1e01",
   "metadata": {},
   "source": [
    "## 1.2. Investigate Errors of the NLI Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f896d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sampled 20 errors for detailed analysis...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Step 1: Collect all incorrect predictions from all test sections\n",
    "def collect_errors(predictions, section_name):\n",
    "    \"\"\"Collect all incorrect predictions from a section\"\"\"\n",
    "    errors = []\n",
    "    for pred in predictions:\n",
    "        if pred['pred_label'] != pred['gold_label']:\n",
    "            pred['section'] = section_name\n",
    "            errors.append(pred)\n",
    "    return errors\n",
    "\n",
    "# Collect errors from all sections\n",
    "errors_r1 = collect_errors(pred_test_r1, \"test_r1\")\n",
    "errors_r2 = collect_errors(pred_test_r2, \"test_r2\")\n",
    "errors_r3 = collect_errors(pred_test_r3, \"test_r3\")\n",
    "\n",
    "all_errors = errors_r1 + errors_r2 + errors_r3\n",
    "\n",
    "# Step 2: Sample 20 errors for detailed analysis\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_errors = random.sample(all_errors, min(20, len(all_errors)))\n",
    "\n",
    "print(f\"\\n Sampled {len(sampled_errors)} errors for detailed analysis...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47041492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigation of each error to understand why the model failed\n",
      "Using evaluate package for metrics computation\n",
      "\n",
      " ERROR INVESTIGATION TABLE:\n",
      "========================================================================================================================\n",
      " error_num section predicted_label    gold_label                                                              investigated_reason\n",
      "         1 test_r3   contradiction       neutral Model failed to understand that listing examples (South Asia, Philippines, Af...\n",
      "         2 test_r1      entailment contradiction Mathematical error - born 1990, album released 2014 = 24 years old, not 18. M...\n",
      "         3 test_r1   contradiction    entailment Mathematical error - born August 23, 1973 would be 45 years old as of 2018-20...\n",
      "         4 test_r2   contradiction    entailment Text processing error - Model overly sensitive to typo \"abum\" instead of \"alb...\n",
      "         5 test_r2      entailment       neutral Temporal assumption error - married 2006-2010 but TV show timing unknown. Mod...\n",
      "         6 test_r2   contradiction       neutral Speculation handling error - Model treated speculation about alternative nami...\n",
      "         7 test_r1   contradiction       neutral Opinion vs fact confusion - Model treated subjective opinion (\"should be call...\n",
      "         8 test_r1      entailment contradiction Mathematical error - first flown March 1990, certified December 1992 = 2 year...\n",
      "         9 test_r3   contradiction    entailment Complex sentence parsing error - confused by semantic mismatch between \"sover...\n",
      "        10 test_r3   contradiction       neutral Missing information bias - premise mentions branches but doesn't specify it's...\n",
      "        11 test_r1   contradiction       neutral Missing temporal information - no start date provided for movement, so 2015 f...\n",
      "        12 test_r3   contradiction       neutral Future prediction error - past preferences don't determine future behavior. M...\n",
      "        13 test_r2   contradiction       neutral Unrelated information handling - premise about vocalist, hypothesis about fun...\n",
      "        14 test_r1   contradiction    entailment Mathematical calculation error - 8th season in 1938 implies starting around 1...\n",
      "        15 test_r1   contradiction       neutral Missing information assumption - no info about where couple met, so Atlanta m...\n",
      "        16 test_r1      entailment       neutral Temporal state assumption - tour began 2007 but unclear if still ongoing. Pas...\n",
      "        17 test_r2   contradiction       neutral Continuation assumption - active in 1993 but unclear about post-1993. Model a...\n",
      "        18 test_r2   contradiction    entailment Counting error - premise clearly lists 8 actors (Marc Warren, Alexander Armst...\n",
      "        19 test_r3      entailment       neutral Temporal state confusion - found \"as kitten\" doesn't indicate current age sta...\n",
      "        20 test_r3      entailment contradiction Semantic mismatch - \"large part of our population\" vs \"tinny population\" are ...\n",
      "\n",
      "========================================================================================================================\n",
      " ERROR PATTERN ANALYSIS\n",
      "========================================================================================================================\n",
      "\n",
      " Mistake Categories (Total: 20 errors):\n",
      "  • Mathematical/Temporal: 4 errors (20.0%) - Errors: [2, 3, 8, 14]\n",
      "  • Neutral Misclassification: 11 errors (55.0%) - Errors: [1, 6, 7, 10, 11, 12, 13, 15, 16, 17, 19]\n",
      "  • Text/Semantic Processing: 4 errors (20.0%) - Errors: [4, 9, 18, 20]\n",
      "  • Missing Information Bias: 4 errors (20.0%) - Errors: [5, 10, 11, 15]\n",
      "  • Assumption Errors: 5 errors (25.0%) - Errors: [5, 12, 16, 17, 19]\n",
      "\n",
      " Errors by Section:\n",
      "  • test_r1: 8 errors (40.0%)\n",
      "  • test_r3: 6 errors (30.0%)\n",
      "  • test_r2: 6 errors (30.0%)\n",
      "\n",
      " KEY INSIGHTS FROM MANUAL ERROR INVESTIGATION:\n",
      "  • Mathematical reasoning consistently fails (errors 2,3,8,14) - age/time calculations are problematic\n",
      "  • Strong bias against neutral predictions - 11/20 errors involve incorrect neutral handling\n",
      "  • Missing information often treated as contradiction rather than neutral (errors 10,11,15)\n",
      "  • Text processing vulnerable to typos and semantic mismatches (errors 4,9,18,20)\n",
      "  • Model makes unwarranted temporal assumptions about ongoing vs completed states\n",
      "  • Opinion/speculation confused with factual claims requiring verification\n"
     ]
    }
   ],
   "source": [
    "print(\"Investigation of each error to understand why the model failed\")\n",
    "print(\"Using evaluate package for metrics computation\")\n",
    "\n",
    "# Manual analysis of each of the 20 sampled errors based on the actual content\n",
    "error_investigations = [\n",
    "    {\n",
    "        'error_num': 1,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Model failed to understand that listing examples (South Asia, Philippines, Africa) does not mean exclusivity - these are examples, not an exhaustive list of ALL countries using this practice'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 2,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'contradiction', \n",
    "        'investigated_reason': 'Mathematical error - born 1990, album released 2014 = 24 years old, not 18. Model failed basic arithmetic calculation'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 3,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'entailment',\n",
    "        'investigated_reason': 'Mathematical error - born August 23, 1973 would be 45 years old as of 2018-2019. Model failed to calculate age from birth date'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 4,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'entailment',\n",
    "        'investigated_reason': 'Text processing error - Model overly sensitive to typo \"abum\" instead of \"album\", failed semantic matching despite clear context'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 5,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Temporal assumption error - married 2006-2010 but TV show timing unknown. Model made unwarranted inference about temporal overlap'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 6,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Speculation handling error - Model treated speculation about alternative naming (\"was going to be called\") as factual claim to verify rather than neutral speculation'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 7,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Opinion vs fact confusion - Model treated subjective opinion (\"should be called\") as objective fact to verify. Opinions are inherently neutral'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 8,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'contradiction',\n",
    "        'investigated_reason': 'Mathematical error - first flown March 1990, certified December 1992 = 2 years 9 months, not 3 years. Temporal calculation mistake'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 9,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'entailment',\n",
    "        'investigated_reason': 'Complex sentence parsing error - confused by semantic mismatch between \"sovereignty issue\" and \"financial publication sovereignty\" but missed core truth about Britain refusing to address sovereignty'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 10,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Missing information bias - premise mentions branches but doesn\\'t specify it\\'s the ONLY way. Model assumes missing info = contradiction instead of neutral'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 11,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Missing temporal information - no start date provided for movement, so 2015 founding cannot be confirmed or denied. Should be neutral due to insufficient info'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 12,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Future prediction error - past preferences don\\'t determine future behavior. Model attempted to predict future actions without evidence'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 13,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Unrelated information handling - premise about vocalist, hypothesis about funeral director. Unrelated info should be neutral, not contradictory'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 14,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'entailment',\n",
    "        'investigated_reason': 'Mathematical calculation error - 8th season in 1938 implies starting around 1930-1931 (1938-8+1). Failed backward calculation from given information'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 15,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Missing information assumption - no info about where couple met, so Atlanta meeting cannot be confirmed or denied. Should be neutral'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 16,\n",
    "        'section': 'test_r1',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Temporal state assumption - tour began 2007 but unclear if still ongoing. Past initiation doesn\\'t determine current existence status'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 17,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Continuation assumption - active in 1993 but unclear about post-1993. Model assumed activity discontinuation without evidence'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 18,\n",
    "        'section': 'test_r2',\n",
    "        'predicted_label': 'contradiction',\n",
    "        'gold_label': 'entailment',\n",
    "        'investigated_reason': 'Counting error - premise clearly lists 8 actors (Marc Warren, Alexander Armstrong, Keeley Hawes, Sarah Alexander, Claire Rushbrook, Emily Joyce, Naomi Bentley, Joshua Sarphie). Basic enumeration failure'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 19,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'neutral',\n",
    "        'investigated_reason': 'Temporal state confusion - found \"as kitten\" doesn\\'t indicate current age status. Model assumed past state determines current state without evidence'\n",
    "    },\n",
    "    {\n",
    "        'error_num': 20,\n",
    "        'section': 'test_r3',\n",
    "        'predicted_label': 'entailment',\n",
    "        'gold_label': 'contradiction',\n",
    "        'investigated_reason': 'Semantic mismatch - \"large part of our population\" vs \"tinny population\" are different concepts. Model failed to recognize lexical/semantic contradiction'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame and display table\n",
    "import pandas as pd\n",
    "error_df = pd.DataFrame(error_investigations)\n",
    "\n",
    "print(\"\\n ERROR INVESTIGATION TABLE:\")\n",
    "print(\"=\"*120)\n",
    "print(error_df.to_string(index=False, max_colwidth=80))\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\" ERROR PATTERN ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Categorize error types for analysis\n",
    "error_categories = {\n",
    "    'Mathematical/Temporal': [2, 3, 8, 14],\n",
    "    'Neutral Misclassification': [1, 6, 7, 10, 11, 12, 13, 15, 16, 17, 19],\n",
    "    'Text/Semantic Processing': [4, 9, 18, 20],\n",
    "    'Missing Information Bias': [5, 10, 11, 15],\n",
    "    'Assumption Errors': [5, 12, 16, 17, 19]\n",
    "}\n",
    "\n",
    "print(f\"\\n Mistake Categories (Total: 20 errors):\")\n",
    "for category, errors in error_categories.items():\n",
    "    count = len(errors)\n",
    "    percentage = (count / 20) * 100\n",
    "    print(f\"  • {category}: {count} errors ({percentage:.1f}%) - Errors: {errors}\")\n",
    "\n",
    "# Section distribution\n",
    "section_counts = error_df['section'].value_counts()\n",
    "print(f\"\\n Errors by Section:\")\n",
    "for section, count in section_counts.items():\n",
    "    percentage = (count / 20) * 100\n",
    "    print(f\"  • {section}: {count} errors ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n KEY INSIGHTS FROM MANUAL ERROR INVESTIGATION:\")\n",
    "insights = [\n",
    "    \"• Mathematical reasoning consistently fails (errors 2,3,8,14) - age/time calculations are problematic\",\n",
    "    \"• Strong bias against neutral predictions - 11/20 errors involve incorrect neutral handling\",\n",
    "    \"• Missing information often treated as contradiction rather than neutral (errors 10,11,15)\",\n",
    "    \"• Text processing vulnerable to typos and semantic mismatches (errors 4,9,18,20)\", \n",
    "    \"• Model makes unwarranted temporal assumptions about ongoing vs completed states\",\n",
    "    \"• Opinion/speculation confused with factual claims requiring verification\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
