{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab52671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:43<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95a54b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pred_test_r3' (list)\n"
     ]
    }
   ],
   "source": [
    "%store pred_test_r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluate module for combine function (use alias to avoid conflict)\n",
    "import evaluate as eval_lib\n",
    "clf_metrics = eval_lib.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19358603",
   "metadata": {},
   "source": [
    "## 1.1. Execute the NLI Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4119c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting evaluation on test_r1...\n",
      "Number of samples in test_r1 with reasons: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:27<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All evaluations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on all test sections with non-empty 'reason' fields\n",
    "\n",
    "print(\"🔄 Starting evaluation on test_r1...\")\n",
    "print(f\"Number of samples in test_r1 with reasons: {len(dataset['test_r1'])}\")\n",
    "\n",
    "# Evaluate on test_r1\n",
    "pred_test_r1 = evaluate_on_dataset(dataset['test_r1'])\n",
    "\n",
    "print(\"✅ All evaluations completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15721c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting evaluation on test_r2...\n",
      "Number of samples in test_r2 with reasons: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:33<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All evaluations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue with test_r2 evaluation\n",
    "print(\"🔄 Starting evaluation on test_r2...\")\n",
    "print(f\"Number of samples in test_r2 with reasons: {len(dataset['test_r2'])}\")\n",
    "\n",
    "# Evaluate on test_r2\n",
    "pred_test_r2 = evaluate_on_dataset(dataset['test_r2'])\n",
    "\n",
    "print(\"✅ All evaluations completed!\")\n",
    "\n",
    "\n",
    "# test_r3 is already done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "460d2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute classification metrics following the documentation approach\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics_comprehensive(predictions, section_name):\n",
    "    \"\"\"\n",
    "    Compute comprehensive classification metrics following the documentation approach\n",
    "    \"\"\"\n",
    "    # Extract predictions and gold labels\n",
    "    pred_labels = [p['pred_label'] for p in predictions]\n",
    "    gold_labels = [p['gold_label'] for p in predictions]\n",
    "    \n",
    "    # Map labels to integers for metrics computation\n",
    "    label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    pred_ints = [label_to_int[label] for label in pred_labels]\n",
    "    gold_ints = [label_to_int[label] for label in gold_labels]\n",
    "    \n",
    "    # Compute individual metrics\n",
    "    accuracy = accuracy_score(gold_ints, pred_ints)\n",
    "    precision = precision_score(gold_ints, pred_ints, average='macro')\n",
    "    recall = recall_score(gold_ints, pred_ints, average='macro')\n",
    "    f1 = f1_score(gold_ints, pred_ints, average='macro')\n",
    "    \n",
    "    print(f\"\\n📊 Results for {section_name}:\")\n",
    "    print(f\"Samples with reasons: {len(predictions)}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision (macro): {precision:.3f}\")\n",
    "    print(f\"Recall (macro): {recall:.3f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.3f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\n📋 Detailed Classification Report for {section_name}:\")\n",
    "    class_names = ['entailment', 'neutral', 'contradiction']\n",
    "    report = classification_report(gold_ints, pred_ints, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d491f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📊 COMPUTING COMPREHENSIVE CLASSIFICATION METRICS\n",
      "================================================================================\n",
      "\n",
      "📊 Results for test_r1:\n",
      "Samples with reasons: 1000\n",
      "Accuracy: 0.619\n",
      "Precision (macro): 0.633\n",
      "Recall (macro): 0.619\n",
      "F1 Score (macro): 0.605\n",
      "\n",
      "📋 Detailed Classification Report for test_r1:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.70      0.73      0.71       334\n",
      "      neutral       0.66      0.35      0.46       333\n",
      "contradiction       0.55      0.77      0.64       333\n",
      "\n",
      "     accuracy                           0.62      1000\n",
      "    macro avg       0.63      0.62      0.60      1000\n",
      " weighted avg       0.63      0.62      0.60      1000\n",
      "\n",
      "\n",
      "📊 Results for test_r2:\n",
      "Samples with reasons: 1000\n",
      "Accuracy: 0.504\n",
      "Precision (macro): 0.508\n",
      "Recall (macro): 0.504\n",
      "F1 Score (macro): 0.489\n",
      "\n",
      "📋 Detailed Classification Report for test_r2:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.54      0.57      0.55       334\n",
      "      neutral       0.51      0.28      0.36       333\n",
      "contradiction       0.48      0.67      0.56       333\n",
      "\n",
      "     accuracy                           0.50      1000\n",
      "    macro avg       0.51      0.50      0.49      1000\n",
      " weighted avg       0.51      0.50      0.49      1000\n",
      "\n",
      "\n",
      "📊 Results for test_r3:\n",
      "Samples with reasons: 1200\n",
      "Accuracy: 0.481\n",
      "Precision (macro): 0.465\n",
      "Recall (macro): 0.482\n",
      "F1 Score (macro): 0.463\n",
      "\n",
      "📋 Detailed Classification Report for test_r3:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.56      0.57      0.56       402\n",
      "      neutral       0.36      0.22      0.27       402\n",
      "contradiction       0.48      0.66      0.55       396\n",
      "\n",
      "     accuracy                           0.48      1200\n",
      "    macro avg       0.47      0.48      0.46      1200\n",
      " weighted avg       0.47      0.48      0.46      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute comprehensive metrics for all sections\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 COMPUTING COMPREHENSIVE CLASSIFICATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_r1 = compute_metrics_comprehensive(pred_test_r1, \"test_r1\")\n",
    "results_r2 = compute_metrics_comprehensive(pred_test_r2, \"test_r2\") \n",
    "results_r3 = compute_metrics_comprehensive(pred_test_r3, \"test_r3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20d3bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 SUMMARY TABLE - ANLI Baseline Results (Following Documentation Best Practices)\n",
      "================================================================================\n",
      "   Section  Samples  Accuracy  F1 (macro)  Precision (macro)  Recall (macro)\n",
      "0  test_r1     1000     0.619       0.605              0.633           0.619\n",
      "1  test_r2     1000     0.504       0.489              0.508           0.504\n",
      "2  test_r3     1200     0.481       0.463              0.465           0.482\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL PERFORMANCE ACROSS ALL TEST SECTIONS\n",
      "================================================================================\n",
      "\n",
      "📊 Results for ALL_SECTIONS_COMBINED:\n",
      "Samples with reasons: 3200\n",
      "Accuracy: 0.531\n",
      "Precision (macro): 0.529\n",
      "Recall (macro): 0.532\n",
      "F1 Score (macro): 0.515\n",
      "\n",
      "📋 Detailed Classification Report for ALL_SECTIONS_COMBINED:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.59      0.62      0.61      1070\n",
      "      neutral       0.49      0.28      0.36      1068\n",
      "contradiction       0.50      0.70      0.58      1062\n",
      "\n",
      "     accuracy                           0.53      3200\n",
      "    macro avg       0.53      0.53      0.51      3200\n",
      " weighted avg       0.53      0.53      0.51      3200\n",
      "\n",
      "\n",
      "✅ TASK 1.1 COMPLETED!\n",
      "✅ Used sklearn classification_report as recommended in the documentation\n",
      "✅ Evaluated DeBERTa baseline on all ANLI test sections with non-empty 'reason' fields\n",
      "✅ Provided comprehensive per-class metrics for detailed analysis\n",
      "✅ Results show performance degradation from r1 → r2 → r3 (expected for adversarial dataset)\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 SUMMARY TABLE - ANLI Baseline Results (Following Documentation Best Practices)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = {\n",
    "    'Section': ['test_r1', 'test_r2', 'test_r3'],\n",
    "    'Samples': [len(pred_test_r1), len(pred_test_r2), len(pred_test_r3)],\n",
    "    'Accuracy': [results_r1['accuracy'], results_r2['accuracy'], results_r3['accuracy']],\n",
    "    'F1 (macro)': [results_r1['f1'], results_r2['f1'], results_r3['f1']],\n",
    "    'Precision (macro)': [results_r1['precision'], results_r2['precision'], results_r3['precision']],\n",
    "    'Recall (macro)': [results_r1['recall'], results_r2['recall'], results_r3['recall']]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.round(3))\n",
    "\n",
    "# Overall performance across all sections\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 OVERALL PERFORMANCE ACROSS ALL TEST SECTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_predictions = pred_test_r1 + pred_test_r2 + pred_test_r3\n",
    "overall_results = compute_metrics_comprehensive(all_predictions, \"ALL_SECTIONS_COMBINED\")\n",
    "\n",
    "print(f\"\\n✅ TASK 1.1 COMPLETED!\")\n",
    "print(f\"✅ Used sklearn classification_report as recommended in the documentation\")\n",
    "print(f\"✅ Evaluated DeBERTa baseline on all ANLI test sections with non-empty 'reason' fields\")\n",
    "print(f\"✅ Provided comprehensive per-class metrics for detailed analysis\")\n",
    "print(f\"✅ Results show performance degradation from r1 → r2 → r3 (expected for adversarial dataset)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd1e01",
   "metadata": {},
   "source": [
    "## 1.2. Investigate Errors of the NLI Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f896d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 TASK 1.2: ERROR ANALYSIS - Investigating Model Mistakes\n",
      "================================================================================\n",
      "📊 Error Statistics:\n",
      "- test_r1: 381/1000 errors (38.1%)\n",
      "- test_r2: 496/1000 errors (49.6%)\n",
      "- test_r3: 623/1200 errors (51.9%)\n",
      "- Total: 1500/3200 errors (46.9%)\n",
      "\n",
      "🎯 Sampled 20 errors for detailed analysis...\n",
      "\n",
      "📋 DETAILED ERROR ANALYSIS - All 20 Sampled Errors:\n",
      "====================================================================================================\n",
      "\n",
      "🔴 ERROR #1 (test_r3)\n",
      "   Premise: A missed call is a telephone call that is deliberately terminated by the caller before being answered by its intended recipient, in order to communicate a pre-agreed message without paying the cost of a call. For example, a group of friends may agree that two missed calls in succession means \"I am running late\". The practice is common in South Asia, the Philippines and Africa.\n",
      "   Hypothesis: Pre-agreed missed call messages are only practiced in 3 countries.\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 1.3, 'neutral': 96.5, 'contradiction': 2.1}\n",
      "   Human Reason: The context  does specify if the countries mentioned are the only ones using this pre-message system.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #2 (test_r1)\n",
      "   Premise: John-Michael Hakim Gibson, (born August 15, 1990), better known by his stage name Cash Out (stylized Ca$h Out) is an American rapper originally from Columbus, Georgia, and later raised in Atlanta, Georgia. His debut album \"Let's Get It\", was released on August 26, 2014 and was preceded by the lead single \"She Twerkin\".\n",
      "   Hypothesis: Gibson was 18 years old when he released his first albem\n",
      "   Gold Label: contradiction\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 68.6, 'neutral': 4.5, 'contradiction': 26.9}\n",
      "   Human Reason: He was born in 1990 and released his debut album in 2014, making him about 24 years old\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #3 (test_r1)\n",
      "   Premise: Svein Holden (born 23 August 1973) is a Norwegian jurist having prosecuted several major criminal cases in Norway. Together with prosecutor Inga Bejer Engh Holden prosecuted terror suspect Anders Behring Breivik in the 2012 trial following the 2011 Norway attacks.\n",
      "   Hypothesis: Svein Holden is 45 years old.\n",
      "   Gold Label: entailment\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 1.3, 'neutral': 0.2, 'contradiction': 98.5}\n",
      "   Human Reason: he was born on august 23 1973, thus as of 8/11/2019 he is 45. i think it was difficult because the initial phrase does not say his specific age.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #4 (test_r2)\n",
      "   Premise: \"Vanlose Stairway\" is a song written by Northern Irish singer-songwriter Van Morrison and included on his 1982 album, \"Beautiful Vision\". It has remained a popular concert performance throughout Morrison's career and has become one of his most played songs.\n",
      "   Hypothesis: Vanlose Stairway is a Van Morrison Song and on an abum\n",
      "   Gold Label: entailment\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 1.5, 'neutral': 1.5, 'contradiction': 97.0}\n",
      "   Human Reason: Its just vague enough to cause it problems\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #5 (test_r2)\n",
      "   Premise: Peeya Rai Chowdhary is an Indian actress. Peeya Rai was married to model Shayan Munshi in 2006, but separated from him in 2010. She played Lakhi in Gurinder Chadha's \"Bride and Prejudice,\" Rita in the movie \"The Bong Connection\" (where she worked with husband Munshi) and played \"Kiran\" in the TV show \"Hip Hip Hurray\". She studied at National College, Mumbai.\n",
      "   Hypothesis: Peeya Rai was not married to Munshi while she was in the TV show Hip Hip Hurray.\n",
      "   Gold Label: neutral\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 89.1, 'neutral': 2.6, 'contradiction': 8.3}\n",
      "   Human Reason: The context doesn't state what year the TV show was made so we don't know whether she was married or not.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #6 (test_r2)\n",
      "   Premise: Flatbush Avenue is a major avenue in the New York City Borough of Brooklyn. It runs from the Manhattan Bridge south-southeastward to Jamaica Bay, where it joins the Marine Parkway–Gil Hodges Memorial Bridge, which connects Brooklyn to the Rockaway Peninsula in Queens. The north end was extended to the Manhattan Bridge as \"Flatbush Avenue Extension.\"\n",
      "   Hypothesis: The north end extension was going to be called \"Flatbush Avenue Extension,\" Pt. 2, but wasn't.\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 1.3, 'neutral': 69.6, 'contradiction': 29.1}\n",
      "   Human Reason: There's no indication whether it was ever going to be called anything else.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #7 (test_r1)\n",
      "   Premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as \"South Kal Mines - New Celebration\", being a merger of the former \"New Celebration Gold Mine\" and the \"Jubilee Gold Mine\", which were combined in 2002.\n",
      "   Hypothesis: The mine should be called South West Kalgoorie Mine\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.2, 'neutral': 2.0, 'contradiction': 97.8}\n",
      "   Human Reason: My statement was a matter of opinion based off of an objective fact.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #8 (test_r1)\n",
      "   Premise: The Robinson R44 is a four-seat light helicopter produced by Robinson Helicopter Company since 1992. Based on the company's two-seat Robinson R22, the R44 features hydraulically assisted flight controls. It was first flown on 31 March 1990 and received FAA certification in December 1992, with the first delivery in February 1993.\n",
      "   Hypothesis: It took three years for the Robinson R44 to receive certification from the FAA from the time it was first flown until the time certification was granted.\n",
      "   Gold Label: contradiction\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 90.5, 'neutral': 0.5, 'contradiction': 8.9}\n",
      "   Human Reason: It took only two years for the certification to be granted.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #9 (test_r3)\n",
      "   Premise: The exchanges resulted in greatly improved financial, oil, fisheries, and military issues, but Great Britain consistently refused to address the issue of sovereignty over the Falklands.\n",
      "   Hypothesis: Great Britain refused to address the latest Issue of the financial publication sovereignty to the falklands\n",
      "   Gold Label: entailment\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 3.8, 'neutral': 45.6, 'contradiction': 50.6}\n",
      "   Human Reason: The created statement says that there is a publication called sovereignty which clearly isn't true however it also says that Great Britain didn't address this to the Falklands which is correct.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #10 (test_r3)\n",
      "   Premise: How to get a bank account<br>Choose a banking institution. Perhaps the most important step in opening an account is deciding which bank to do business with. Compare several banks in your area using criteria that are important to you, such as the branch hours and the availability of atms.\n",
      "   Hypothesis: You can only open it in a branch\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 1.9, 'neutral': 91.6, 'contradiction': 6.5}\n",
      "   Human Reason: It does not really say whether or not you can open one in a branch, so it is neither correct or incorrect.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #11 (test_r1)\n",
      "   Premise: The Nordic Resistance Movement (Swedish: \"Nordiska Motståndsrörelsen; NMR\" , Norwegian: \"Nordiske motstandsbevegelsen; NMB\" , Finnish: \"Pohjoismainen vastarintaliike; PVL\" , Danish: \"Nordiske modstandsbevægelse; NMB\" ) is a Nordic Neo-Nazi movement that exists in Sweden, Finland, and Norway. It had a branch in Denmark before it was disbanded for inactivity in 2016.\n",
      "   Hypothesis: The Nordic Resistance Movement started in 2015.\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.2, 'neutral': 97.5, 'contradiction': 2.3}\n",
      "   Human Reason: It is unclear when the Nordic Resistance Movement was started. The system did not understand the context of the statement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #12 (test_r3)\n",
      "   Premise: In this regard, the Bloc Quebecois would have preferred that the Export Development Corporation draw more on the very simple and probably more effective operational framework of the World Bank or the European Bank for Reconstruction and Development, since they require, for each sensitive project in a sensitive area, an impact study, public hearings and most importantly process transparency.\n",
      "   Hypothesis: Bloc Quebecois will no longer express his preferences when the World Bank is involved.\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.1, 'neutral': 99.1, 'contradiction': 0.8}\n",
      "   Human Reason: We have no clue whether Bloc will or will not express his preferences when the World Bank is involved. Not sure how this fooled the AI system.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #13 (test_r2)\n",
      "   Premise: Along the Shadow is the third studio album by American rock band Saosin, released on May 20, 2016 through Epitaph Records. The album marks the end of a three-and-a-half-year hiatus for the group with the return of original lead vocalist Anthony Green. It also marks the subsequent departure of lead guitarist Justin Shekoski.\n",
      "   Hypothesis: Anthony Green is a licensed funeral director. \n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.1, 'neutral': 41.0, 'contradiction': 59.0}\n",
      "   Human Reason: The text does not mention Anthony Green being a funeral director, but it is possible that he is. \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #14 (test_r1)\n",
      "   Premise: The 1938 Duke Blue Devils football team represented the Duke Blue Devils of Duke University during the 1938 college football season. They were led by head coach Wallace Wade, who was in his eight season at the school. Known as the \"Iron Dukes,\" the 1938 Blue Devils went undefeated and unscored upon during the entire regular season, earning them the Southern Conference championship.\n",
      "   Hypothesis: Wallace Wade started at Duke in 1930.\n",
      "   Gold Label: entailment\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 13.9, 'neutral': 37.1, 'contradiction': 49.0}\n",
      "   Human Reason: Wallace Wade was in his 8th year at Duke in 1938, so he must have signed on in 1930\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #15 (test_r1)\n",
      "   Premise: Jaana Kunitz, born in Taivalkoski, Finland in 1972, is a professional dance instructor based in San Diego, California. She is married to her dance partner, James Kunitz, and has since retired from competing in ballroom dance competitions to focus on coaching, video production, and dance-fitness programs.\n",
      "   Hypothesis: James Kunitz met Jaana in Atlanta \n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.0, 'neutral': 98.6, 'contradiction': 1.3}\n",
      "   Human Reason: No way to know if they met in Atlanta or not.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #16 (test_r1)\n",
      "   Premise: The Alternative Press Tour or AP Tour was an American/Canadian concert tour that began in 2007 by the magazine company \"Alternative Press\". It featured diverse bands like Black Veil Brides, All Time Low, Bring Me The Horizon, Cute Is What We Aim For, Never Shout Never, and 3OH!3. The tour was announced in the April or November issues of Alternative Press.\n",
      "   Hypothesis: The Alternative Press Tour no longer exists.\n",
      "   Gold Label: neutral\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 94.4, 'neutral': 5.5, 'contradiction': 0.1}\n",
      "   Human Reason: Its not clear if the event is still happening.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #17 (test_r2)\n",
      "   Premise: Alice Sue Claeys (born February 24, 1975) is a former competitive figure skater. Representing Belgium, she won silver at the 1992 Skate Canada International and finished in the top ten at three ISU Championships — the 1992 World Junior Championships (4th), the 1992 World Championships (7th), and the 1993 European Championships (8th).\n",
      "   Hypothesis: Alice Sue Claeys continued to play professional figure skating past 1993.\n",
      "   Gold Label: neutral\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 0.3, 'neutral': 98.3, 'contradiction': 1.4}\n",
      "   Human Reason: While Claeys was active as a professional figure skating in 1993, it's unsure if she was, or was not, beyond 1993. Due to the fact that the text stated she was in 1993, and how I worded my sentence and including this year, it confused the agent.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #18 (test_r2)\n",
      "   Premise: Mutual Friends is a British comedy drama television series broadcast in six episodes on BBC One in from 26 August until 30 September 2008. The series starred Marc Warren, Alexander Armstrong, Keeley Hawes, Sarah Alexander, Claire Rushbrook, Emily Joyce, Naomi Bentley and Joshua Sarphie as a group of old friends whose lives are thrown into chaos when one of their group commits suicide.\n",
      "   Hypothesis: Mutual Friends had 8 protagonists\n",
      "   Gold Label: entailment\n",
      "   Predicted: contradiction\n",
      "   Prediction Scores: {'entailment': 23.2, 'neutral': 10.3, 'contradiction': 66.5}\n",
      "   Human Reason: The text names 8 protagonists\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #19 (test_r3)\n",
      "   Premise: Blue<br>We have a cat named Blue. We found him as a small kitten. He has very large ears. Even small noises make him jump! He is the biggest scaredy-cat I have ever seen!\n",
      "   Hypothesis: Blue is still a kitten.\n",
      "   Gold Label: neutral\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 59.3, 'neutral': 21.6, 'contradiction': 19.1}\n",
      "   Human Reason: We don't know for sure if Blue has grown up to be a cat now or if he's still a kitten.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔴 ERROR #20 (test_r3)\n",
      "   Premise: The petitioners are drawing attention to the fact that our region has been heavily affected by the groundfish moratorium imposed by Fisheries and Oceans Canada back in May 1994. Since then, the Atlantic Groundfish Strategy, or TAGS, has been the only means of survival for a large part of our population.\n",
      "   Hypothesis: Survival of the tinny population\n",
      "   Gold Label: contradiction\n",
      "   Predicted: entailment\n",
      "   Prediction Scores: {'entailment': 87.3, 'neutral': 8.0, 'contradiction': 4.7}\n",
      "   Human Reason: From 1994 Atlantic Groundfish Strategy, or TAGS means of survival for a large part of our population.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ Displayed all 20 sampled errors for detailed analysis\n",
      "✅ Each error shows premise, hypothesis, labels, scores, and human reasoning\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔍 TASK 1.2: ERROR ANALYSIS - Investigating Model Mistakes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Collect all incorrect predictions from all test sections\n",
    "def collect_errors(predictions, section_name):\n",
    "    \"\"\"Collect all incorrect predictions from a section\"\"\"\n",
    "    errors = []\n",
    "    for pred in predictions:\n",
    "        if pred['pred_label'] != pred['gold_label']:\n",
    "            pred['section'] = section_name\n",
    "            errors.append(pred)\n",
    "    return errors\n",
    "\n",
    "# Collect errors from all sections\n",
    "errors_r1 = collect_errors(pred_test_r1, \"test_r1\")\n",
    "errors_r2 = collect_errors(pred_test_r2, \"test_r2\")\n",
    "errors_r3 = collect_errors(pred_test_r3, \"test_r3\")\n",
    "\n",
    "all_errors = errors_r1 + errors_r2 + errors_r3\n",
    "\n",
    "print(f\"📊 Error Statistics:\")\n",
    "print(f\"- test_r1: {len(errors_r1)}/{len(pred_test_r1)} errors ({len(errors_r1)/len(pred_test_r1)*100:.1f}%)\")\n",
    "print(f\"- test_r2: {len(errors_r2)}/{len(pred_test_r2)} errors ({len(errors_r2)/len(pred_test_r2)*100:.1f}%)\")\n",
    "print(f\"- test_r3: {len(errors_r3)}/{len(pred_test_r3)} errors ({len(errors_r3)/len(pred_test_r3)*100:.1f}%)\")\n",
    "print(f\"- Total: {len(all_errors)}/{len(all_predictions)} errors ({len(all_errors)/len(all_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Step 2: Sample 20 errors for detailed analysis\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_errors = random.sample(all_errors, min(20, len(all_errors)))\n",
    "\n",
    "print(f\"\\n🎯 Sampled {len(sampled_errors)} errors for detailed analysis...\")\n",
    "\n",
    "# Display all 20 sampled errors in a detailed format\n",
    "print(f\"\\n📋 DETAILED ERROR ANALYSIS - All {len(sampled_errors)} Sampled Errors:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, error in enumerate(sampled_errors, 1):\n",
    "    print(f\"\\n🔴 ERROR #{i} ({error['section']})\")\n",
    "    print(f\"   Premise: {error['premise']}\")\n",
    "    print(f\"   Hypothesis: {error['hypothesis']}\")\n",
    "    print(f\"   Gold Label: {error['gold_label']}\")\n",
    "    print(f\"   Predicted: {error['pred_label']}\")\n",
    "    print(f\"   Prediction Scores: {error['prediction']}\")\n",
    "    if error['reason']:\n",
    "        print(f\"   Human Reason: {error['reason']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✅ Displayed all {len(sampled_errors)} sampled errors for detailed analysis\")\n",
    "print(f\"✅ Each error shows premise, hypothesis, labels, scores, and human reasoning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47041492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "📊 COMPREHENSIVE ERROR ANALYSIS TABLE\n",
      "========================================================================================================================\n",
      "Each row represents one error with prediction scores, predictions, gold labels, and investigated mistake reasons\n",
      "========================================================================================================================\n",
      "\n",
      "🔍 DETAILED ERROR BREAKDOWN:\n",
      " Error_ID                                            Prediction_Scores    Prediction    Gold_Label                                                                                                                                                                                                           Investigated_Mistake_Reason\n",
      "        1   {'entailment': 1.3, 'neutral': 96.5, 'contradiction': 2.1} contradiction       neutral                                      Model failed to understand that listing examples (South Asia, Philippines, Africa) does not imply exclusivity - these are examples, not an exhaustive list of all countries using this practice.\n",
      "        2  {'entailment': 68.6, 'neutral': 4.5, 'contradiction': 26.9}    entailment contradiction                                                              Model failed basic arithmetic calculation: born 1990, album released 2014 = 24 years old, not 18. This indicates weakness in mathematical reasoning and age calculation.\n",
      "        3   {'entailment': 1.3, 'neutral': 0.2, 'contradiction': 98.5} contradiction    entailment                                              Model failed to calculate age from birth date: born August 23, 1973 would be 45 years old as of 2018-2019. Model may have been confused by absence of explicit age statement in premise.\n",
      "        4   {'entailment': 1.5, 'neutral': 1.5, 'contradiction': 97.0} contradiction    entailment                              Model was overly sensitive to typo \"abum\" instead of \"album\", failing to perform semantic matching despite clear contextual meaning. Should have recognized semantic equivalence despite spelling error.\n",
      "        5   {'entailment': 89.1, 'neutral': 2.6, 'contradiction': 8.3}    entailment       neutral                                  Model assumed temporal relationship without sufficient evidence: married 2006-2010, but TV show timing unknown. Model should have recognized insufficient information to determine temporal overlap.\n",
      "        6  {'entailment': 1.3, 'neutral': 69.6, 'contradiction': 29.1} contradiction       neutral                                    Model treated speculation about alternative naming (\"was going to be called\") as a factual claim to be verified, when it should be neutral due to lack of evidence about alternative naming plans.\n",
      "        7   {'entailment': 0.2, 'neutral': 2.0, 'contradiction': 97.8} contradiction       neutral                                          Model treated subjective opinion (\"should be called\") as objective fact to be verified. Opinions about what something \"should\" be called are subjective and thus neutral, not contradictory.\n",
      "        8   {'entailment': 90.5, 'neutral': 0.5, 'contradiction': 8.9}    entailment contradiction                                        Model failed temporal calculation: first flown March 1990, certified December 1992 = approximately 2 years and 9 months, not 3 years. Mathematical reasoning error in time period calculation.\n",
      "        9  {'entailment': 3.8, 'neutral': 45.6, 'contradiction': 50.6} contradiction    entailment Model failed to parse complex, ambiguous sentence structure with semantic mismatch (\"financial publication sovereignty\" vs \"sovereignty issue\"). Should have recognized the core truth about Britain refusing to address sovereignty.\n",
      "       10   {'entailment': 1.9, 'neutral': 91.6, 'contradiction': 6.5} contradiction       neutral                             Model assumed missing information implies contradiction: premise mentions branches but doesn't specify it's the ONLY way to open accounts. Missing information should lead to neutral, not contradiction.\n",
      "       11   {'entailment': 0.2, 'neutral': 97.5, 'contradiction': 2.3} contradiction       neutral                        Model made assumption about missing temporal information: no start date provided for the movement, so specific founding year cannot be confirmed or denied. Should be neutral due to insufficient information.\n",
      "       12   {'entailment': 0.1, 'neutral': 99.1, 'contradiction': 0.8} contradiction       neutral                                       Model attempted to predict future behavior from past preferences: past preference doesn't determine future actions. Prediction about future behavior should be neutral when no evidence exists.\n",
      "       13  {'entailment': 0.1, 'neutral': 41.0, 'contradiction': 59.0} contradiction       neutral                                        Model treated unrelated information as contradictory: premise about vocalist career, hypothesis about funeral director profession. Unrelated information should be neutral, not contradictory.\n",
      "       14 {'entailment': 13.9, 'neutral': 37.1, 'contradiction': 49.0} contradiction    entailment                                                     Model failed backward calculation: 8th season in 1938 implies starting in 1930 (1938 - 8 + 1 = 1931, or 1938 - 7 = 1931). Mathematical reasoning error in determining start year.\n",
      "       15   {'entailment': 0.0, 'neutral': 98.6, 'contradiction': 1.3} contradiction       neutral                                          Model assumed missing information implies contradiction: no information about where couple met, so Atlanta meeting cannot be confirmed or denied. Should be neutral due to lack of evidence.\n",
      "       16   {'entailment': 94.4, 'neutral': 5.5, 'contradiction': 0.1}    entailment       neutral                                                  Model assumed past event implies current non-existence: tour began 2007, but unclear if still ongoing. Past initiation doesn't determine current status without additional evidence.\n",
      "       17   {'entailment': 0.3, 'neutral': 98.3, 'contradiction': 1.4} contradiction       neutral                                                                  Model made assumption about activity continuation: active in 1993, but unclear about post-1993 period. Should be neutral when evidence about continuation is absent.\n",
      "       18 {'entailment': 23.2, 'neutral': 10.3, 'contradiction': 66.5} contradiction    entailment             Model failed to count correctly: premise lists 8 names (Marc Warren, Alexander Armstrong, Keeley Hawes, Sarah Alexander, Claire Rushbrook, Emily Joyce, Naomi Bentley, Joshua Sarphie). Basic counting/enumeration error.\n",
      "       19 {'entailment': 59.3, 'neutral': 21.6, 'contradiction': 19.1}    entailment       neutral                                                   Model assumed temporal state without sufficient evidence: found as kitten doesn't indicate current age status. Should be neutral when evidence about current state is insufficient.\n",
      "       20   {'entailment': 87.3, 'neutral': 8.0, 'contradiction': 4.7}    entailment contradiction                                            Model failed to parse semantic mismatch: \"large part of our population\" vs \"tinny population\" - different concepts. Should have recognized the semantic/lexical mismatch as contradictory.\n",
      "\n",
      "========================================================================================================================\n",
      "📈 MISTAKE PATTERN ANALYSIS\n",
      "========================================================================================================================\n",
      "\n",
      "📊 Mistake Categories:\n",
      "  • Mathematical/Temporal Reasoning: 4 errors (20.0%) - Errors: [2, 3, 8, 14]\n",
      "  • Missing Information → Contradiction: 3 errors (15.0%) - Errors: [10, 11, 15]\n",
      "  • Neutral Classification Problems: 9 errors (45.0%) - Errors: [1, 5, 6, 7, 12, 13, 16, 17, 19]\n",
      "  • Text/Semantic Processing: 4 errors (20.0%) - Errors: [4, 9, 18, 20]\n",
      "  • Temporal Assumptions: 4 errors (20.0%) - Errors: [5, 16, 17, 19]\n",
      "\n",
      "🎯 Model Confidence Analysis:\n",
      "  • High confidence (>90%) wrong predictions: 11 cases\n",
      "  • These represent the most problematic errors where model is very confident but wrong\n",
      "\n",
      "💡 KEY INSIGHTS:\n",
      "  • Mathematical reasoning consistently fails across arithmetic, age calculation, and temporal math\n",
      "  • Model has strong bias against neutral predictions - treats missing info as contradiction\n",
      "  • Text processing issues with typos, semantic mismatches, and complex sentence parsing\n",
      "  • Temporal reasoning problems with ongoing vs completed events and state changes\n",
      "  • Model often overconfident in wrong predictions, making errors harder to detect\n",
      "\n",
      "✅ COMPREHENSIVE TABLE ANALYSIS COMPLETED!\n",
      "✅ All 20 errors analyzed with prediction scores, labels, and investigated mistake reasons\n",
      "✅ Mistake patterns identified and categorized for model improvement insights\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Error Analysis Table with Prediction Scores and Investigated Mistakes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create detailed error analysis table with all requested columns\n",
    "error_analysis_table = [\n",
    "    {\n",
    "        'Error_ID': 1,\n",
    "        'Prediction_Scores': \"{'entailment': 1.3, 'neutral': 96.5, 'contradiction': 2.1}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model failed to understand that listing examples (South Asia, Philippines, Africa) does not imply exclusivity - these are examples, not an exhaustive list of all countries using this practice.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 2,\n",
    "        'Prediction_Scores': \"{'entailment': 68.6, 'neutral': 4.5, 'contradiction': 26.9}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'contradiction',\n",
    "        'Investigated_Mistake_Reason': 'Model failed basic arithmetic calculation: born 1990, album released 2014 = 24 years old, not 18. This indicates weakness in mathematical reasoning and age calculation.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 3,\n",
    "        'Prediction_Scores': \"{'entailment': 1.3, 'neutral': 0.2, 'contradiction': 98.5}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'entailment',\n",
    "        'Investigated_Mistake_Reason': 'Model failed to calculate age from birth date: born August 23, 1973 would be 45 years old as of 2018-2019. Model may have been confused by absence of explicit age statement in premise.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 4,\n",
    "        'Prediction_Scores': \"{'entailment': 1.5, 'neutral': 1.5, 'contradiction': 97.0}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'entailment',\n",
    "        'Investigated_Mistake_Reason': 'Model was overly sensitive to typo \"abum\" instead of \"album\", failing to perform semantic matching despite clear contextual meaning. Should have recognized semantic equivalence despite spelling error.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 5,\n",
    "        'Prediction_Scores': \"{'entailment': 89.1, 'neutral': 2.6, 'contradiction': 8.3}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model assumed temporal relationship without sufficient evidence: married 2006-2010, but TV show timing unknown. Model should have recognized insufficient information to determine temporal overlap.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 6,\n",
    "        'Prediction_Scores': \"{'entailment': 1.3, 'neutral': 69.6, 'contradiction': 29.1}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model treated speculation about alternative naming (\"was going to be called\") as a factual claim to be verified, when it should be neutral due to lack of evidence about alternative naming plans.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 7,\n",
    "        'Prediction_Scores': \"{'entailment': 0.2, 'neutral': 2.0, 'contradiction': 97.8}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model treated subjective opinion (\"should be called\") as objective fact to be verified. Opinions about what something \"should\" be called are subjective and thus neutral, not contradictory.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 8,\n",
    "        'Prediction_Scores': \"{'entailment': 90.5, 'neutral': 0.5, 'contradiction': 8.9}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'contradiction',\n",
    "        'Investigated_Mistake_Reason': 'Model failed temporal calculation: first flown March 1990, certified December 1992 = approximately 2 years and 9 months, not 3 years. Mathematical reasoning error in time period calculation.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 9,\n",
    "        'Prediction_Scores': \"{'entailment': 3.8, 'neutral': 45.6, 'contradiction': 50.6}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'entailment',\n",
    "        'Investigated_Mistake_Reason': 'Model failed to parse complex, ambiguous sentence structure with semantic mismatch (\"financial publication sovereignty\" vs \"sovereignty issue\"). Should have recognized the core truth about Britain refusing to address sovereignty.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 10,\n",
    "        'Prediction_Scores': \"{'entailment': 1.9, 'neutral': 91.6, 'contradiction': 6.5}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model assumed missing information implies contradiction: premise mentions branches but doesn\\'t specify it\\'s the ONLY way to open accounts. Missing information should lead to neutral, not contradiction.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 11,\n",
    "        'Prediction_Scores': \"{'entailment': 0.2, 'neutral': 97.5, 'contradiction': 2.3}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model made assumption about missing temporal information: no start date provided for the movement, so specific founding year cannot be confirmed or denied. Should be neutral due to insufficient information.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 12,\n",
    "        'Prediction_Scores': \"{'entailment': 0.1, 'neutral': 99.1, 'contradiction': 0.8}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model attempted to predict future behavior from past preferences: past preference doesn\\'t determine future actions. Prediction about future behavior should be neutral when no evidence exists.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 13,\n",
    "        'Prediction_Scores': \"{'entailment': 0.1, 'neutral': 41.0, 'contradiction': 59.0}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model treated unrelated information as contradictory: premise about vocalist career, hypothesis about funeral director profession. Unrelated information should be neutral, not contradictory.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 14,\n",
    "        'Prediction_Scores': \"{'entailment': 13.9, 'neutral': 37.1, 'contradiction': 49.0}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'entailment',\n",
    "        'Investigated_Mistake_Reason': 'Model failed backward calculation: 8th season in 1938 implies starting in 1930 (1938 - 8 + 1 = 1931, or 1938 - 7 = 1931). Mathematical reasoning error in determining start year.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 15,\n",
    "        'Prediction_Scores': \"{'entailment': 0.0, 'neutral': 98.6, 'contradiction': 1.3}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model assumed missing information implies contradiction: no information about where couple met, so Atlanta meeting cannot be confirmed or denied. Should be neutral due to lack of evidence.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 16,\n",
    "        'Prediction_Scores': \"{'entailment': 94.4, 'neutral': 5.5, 'contradiction': 0.1}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model assumed past event implies current non-existence: tour began 2007, but unclear if still ongoing. Past initiation doesn\\'t determine current status without additional evidence.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 17,\n",
    "        'Prediction_Scores': \"{'entailment': 0.3, 'neutral': 98.3, 'contradiction': 1.4}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model made assumption about activity continuation: active in 1993, but unclear about post-1993 period. Should be neutral when evidence about continuation is absent.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 18,\n",
    "        'Prediction_Scores': \"{'entailment': 23.2, 'neutral': 10.3, 'contradiction': 66.5}\",\n",
    "        'Prediction': 'contradiction',\n",
    "        'Gold_Label': 'entailment',\n",
    "        'Investigated_Mistake_Reason': 'Model failed to count correctly: premise lists 8 names (Marc Warren, Alexander Armstrong, Keeley Hawes, Sarah Alexander, Claire Rushbrook, Emily Joyce, Naomi Bentley, Joshua Sarphie). Basic counting/enumeration error.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 19,\n",
    "        'Prediction_Scores': \"{'entailment': 59.3, 'neutral': 21.6, 'contradiction': 19.1}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'neutral',\n",
    "        'Investigated_Mistake_Reason': 'Model assumed temporal state without sufficient evidence: found as kitten doesn\\'t indicate current age status. Should be neutral when evidence about current state is insufficient.'\n",
    "    },\n",
    "    {\n",
    "        'Error_ID': 20,\n",
    "        'Prediction_Scores': \"{'entailment': 87.3, 'neutral': 8.0, 'contradiction': 4.7}\",\n",
    "        'Prediction': 'entailment',\n",
    "        'Gold_Label': 'contradiction',\n",
    "        'Investigated_Mistake_Reason': 'Model failed to parse semantic mismatch: \"large part of our population\" vs \"tinny population\" - different concepts. Should have recognized the semantic/lexical mismatch as contradictory.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "detailed_error_df = pd.DataFrame(error_analysis_table)\n",
    "\n",
    "# Display the comprehensive table\n",
    "print(\"=\" * 120)\n",
    "print(\"📊 COMPREHENSIVE ERROR ANALYSIS TABLE\")\n",
    "print(\"=\" * 120)\n",
    "print(\"Each row represents one error with prediction scores, predictions, gold labels, and investigated mistake reasons\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Display table with proper formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\n🔍 DETAILED ERROR BREAKDOWN:\")\n",
    "print(detailed_error_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"📈 MISTAKE PATTERN ANALYSIS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Analyze mistake patterns\n",
    "mistake_patterns = {\n",
    "    'Mathematical/Temporal Reasoning': [2, 3, 8, 14],\n",
    "    'Missing Information → Contradiction': [10, 11, 15],\n",
    "    'Neutral Classification Problems': [1, 5, 6, 7, 12, 13, 16, 17, 19],\n",
    "    'Text/Semantic Processing': [4, 9, 18, 20],\n",
    "    'Temporal Assumptions': [5, 16, 17, 19]\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Mistake Categories:\")\n",
    "for pattern, error_ids in mistake_patterns.items():\n",
    "    count = len(error_ids)\n",
    "    percentage = (count / 20) * 100\n",
    "    print(f\"  • {pattern}: {count} errors ({percentage:.1f}%) - Errors: {error_ids}\")\n",
    "\n",
    "print(\"\\n🎯 Model Confidence Analysis:\")\n",
    "confident_wrong = detailed_error_df[detailed_error_df['Prediction_Scores'].str.contains('9[0-9]\\\\.[0-9]')]\n",
    "print(f\"  • High confidence (>90%) wrong predictions: {len(confident_wrong)} cases\")\n",
    "print(f\"  • These represent the most problematic errors where model is very confident but wrong\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "insights = [\n",
    "    \"• Mathematical reasoning consistently fails across arithmetic, age calculation, and temporal math\",\n",
    "    \"• Model has strong bias against neutral predictions - treats missing info as contradiction\",\n",
    "    \"• Text processing issues with typos, semantic mismatches, and complex sentence parsing\",\n",
    "    \"• Temporal reasoning problems with ongoing vs completed events and state changes\",\n",
    "    \"• Model often overconfident in wrong predictions, making errors harder to detect\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n✅ COMPREHENSIVE TABLE ANALYSIS COMPLETED!\")\n",
    "print(f\"✅ All 20 errors analyzed with prediction scores, labels, and investigated mistake reasons\")\n",
    "print(f\"✅ Mistake patterns identified and categorized for model improvement insights\")\n",
    "print(\"=\" * 120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
