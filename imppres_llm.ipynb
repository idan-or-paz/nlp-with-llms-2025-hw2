{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres with LLM\n",
    "\n",
    "You have to implement in this notebook a better ImpPres classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import dspy\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'], max_tokens=8000, temperature=0.2)\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Paradigm-level Signature: JSON in/out to handle batches of ~19 pairs\n",
    "class ParadigmNLISignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a list of NLI pairs (premise, hypothesis), predict a label for each pair\n",
    "    and provide a brief explanation per pair. Use one of: entailment, neutral, contradiction.\n",
    "    The inputs and outputs are JSON-encoded lists to ensure deterministic parsing.\n",
    "    \"\"\"\n",
    "    pairs_json: str = dspy.InputField()\n",
    "    labels_json: str = dspy.OutputField()\n",
    "    explanations_json: str = dspy.OutputField()\n",
    "\n",
    "# Helper: label mapping\n",
    "LABELS = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Utility: compute per-paradigm accuracy and consistency\n",
    "\n",
    "def compute_paradigm_accuracy(gold_labels: list[int], pred_labels: list[int]) -> float:\n",
    "    correct = sum(1 for g, p in zip(gold_labels, pred_labels) if g == p)\n",
    "    return correct / len(gold_labels) if gold_labels else 0.0\n",
    "\n",
    "\n",
    "def compute_paradigm_consistency(gold_labels: list[int], pred_labels: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Majority-coherence within each gold label group:\n",
    "    - For each gold label value present in the paradigm, find the majority predicted label inside that group.\n",
    "    - Score for the group = fraction of items in that group that match the group's majority predicted label.\n",
    "    - Return the average across gold groups.\n",
    "    Range: [0, 1]. Equals 1.0 when predictions are consistent inside each gold cluster.\n",
    "    \"\"\"\n",
    "    by_gold: defaultdict[int, list[int]] = defaultdict(list)\n",
    "    for g, p in zip(gold_labels, pred_labels):\n",
    "        by_gold[g].append(p)\n",
    "    if not by_gold:\n",
    "        return 0.0\n",
    "    per_group_scores = []\n",
    "    for group, preds in by_gold.items():\n",
    "        if not preds:\n",
    "            continue\n",
    "        counts = Counter(preds)\n",
    "        majority_pred, majority_count = counts.most_common(1)[0]\n",
    "        per_group_scores.append(majority_count / len(preds))\n",
    "    if not per_group_scores:\n",
    "        return 0.0\n",
    "    return sum(per_group_scores) / len(per_group_scores)\n",
    "\n",
    "\n",
    "def combined_reward(gold_labels: list[int], pred_labels: list[int], alpha: float = 0.75) -> float:\n",
    "    acc = compute_paradigm_accuracy(gold_labels, pred_labels)\n",
    "    cons = compute_paradigm_consistency(gold_labels, pred_labels)\n",
    "    return alpha * acc + (1 - alpha) * cons\n",
    "\n",
    "# Parser utilities\n",
    "\n",
    "def parse_labels_json(labels_json: str) -> list[int]:\n",
    "    try:\n",
    "        data = json.loads(labels_json)\n",
    "        # Accept strings or ints\n",
    "        parsed = []\n",
    "        for x in data:\n",
    "            if isinstance(x, int):\n",
    "                parsed.append(x)\n",
    "            else:\n",
    "                parsed.append(label2id.get(str(x).strip().lower(), -1))\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def safe_json_dumps(obj) -> str:\n",
    "    return json.dumps(obj, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11772927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-item signature used for fallback predictions\n",
    "class NLISignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Classify the relationship between the premise and hypothesis to one of:\n",
    "    entailment, neutral, contradiction. Provide a short explanation.\n",
    "    \"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42c652bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build paradigm-grouped DSPy examples with shuffling\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "section_to_split = {\n",
    "    'presupposition_all_n_presupposition': 'all_n_presupposition',\n",
    "    'presupposition_both_presupposition': 'both_presupposition',\n",
    "    'presupposition_change_of_state': 'change_of_state',\n",
    "    'presupposition_cleft_existence': 'cleft_existence',\n",
    "    'presupposition_cleft_uniqueness': 'cleft_uniqueness',\n",
    "    'presupposition_only_presupposition': 'only_presupposition',\n",
    "    'presupposition_possessed_definites_existence': 'possessed_definites_existence',\n",
    "    'presupposition_possessed_definites_uniqueness': 'possessed_definites_uniqueness',\n",
    "    'presupposition_question_presupposition': 'question_presupposition',\n",
    "}\n",
    "\n",
    "# Load data\n",
    "raw = {}\n",
    "for section, split in section_to_split.items():\n",
    "    raw[section] = load_dataset(\"facebook/imppres\", section)[split]\n",
    "\n",
    "# Group items by paradigmID per section, and create one example per paradigm\n",
    "paradigm_examples = []\n",
    "section_index = defaultdict(list)\n",
    "\n",
    "for section, ds in raw.items():\n",
    "    # Build index for this section\n",
    "    pid_to_rows = defaultdict(list)\n",
    "    for row in ds:\n",
    "        pid_to_rows[row[\"paradigmID\"]].append(row)\n",
    "    \n",
    "    # Create a DSPy example per paradigmID\n",
    "    for pid, rows in pid_to_rows.items():\n",
    "        # Shuffle to avoid positional leakage\n",
    "        rows_shuffled = rows[:]\n",
    "        random.shuffle(rows_shuffled)\n",
    "        \n",
    "        pairs = [{\"premise\": r[\"premise\"], \"hypothesis\": r[\"hypothesis\"]} for r in rows_shuffled]\n",
    "        gold = [int(r[\"gold_label\"]) if isinstance(r[\"gold_label\"], (int,)) else int(r[\"gold_label\"]) for r in rows_shuffled]\n",
    "        \n",
    "        ex = dspy.Example(\n",
    "            pairs_json=safe_json_dumps(pairs),\n",
    "            labels_json=safe_json_dumps(gold),\n",
    "            section=section,\n",
    "            paradigm_id=str(pid),\n",
    "        ).with_inputs(\"pairs_json\")\n",
    "        \n",
    "        paradigm_examples.append(ex)\n",
    "        section_index[section].append(len(paradigm_examples) - 1)\n",
    "\n",
    "len(paradigm_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07aaf021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Predictor over Paradigms\n",
    "\n",
    "class ParadigmPredictor(dspy.Module):\n",
    "    def __init__(self, max_items: int = 25):\n",
    "        super().__init__()\n",
    "        # Use ChainOfThought to elicit explanations\n",
    "        self.predict = dspy.ChainOfThought(ParadigmNLISignature)\n",
    "        self.max_items = max_items\n",
    "    \n",
    "    def forward(self, pairs: list[dict]) -> tuple[list[int], list[str]]:\n",
    "        # Safety cap\n",
    "        pairs = pairs[: self.max_items]\n",
    "        prompt_pairs = [\n",
    "            {\n",
    "                \"premise\": p.get(\"premise\", \"\"),\n",
    "                \"hypothesis\": p.get(\"hypothesis\", \"\"),\n",
    "            }\n",
    "            for p in pairs\n",
    "        ]\n",
    "        pairs_json = safe_json_dumps(prompt_pairs)\n",
    "        pred = self.predict(pairs_json=pairs_json)\n",
    "        labels = parse_labels_json(pred.labels_json)\n",
    "        # Fallback: if parsing failed or wrong length, recover with per-item prompts\n",
    "        if not labels or len(labels) != len(prompt_pairs):\n",
    "            labels = []\n",
    "            explanations = []\n",
    "            single = dspy.ChainOfThought(NLISignature)\n",
    "            for p in prompt_pairs:\n",
    "                out = single(premise=p[\"premise\"], hypothesis=p[\"hypothesis\"])\n",
    "                lab = label2id.get(str(out.label).strip().lower(), 1)\n",
    "                labels.append(lab)\n",
    "                explanations.append(str(getattr(out, \"reasoning\", \"\")))\n",
    "            return labels, explanations\n",
    "        # Parse explanations_json if present and aligned\n",
    "        try:\n",
    "            explanations = json.loads(getattr(pred, \"explanations_json\", \"[]\"))\n",
    "            if not isinstance(explanations, list) or len(explanations) != len(labels):\n",
    "                explanations = [\"\"] * len(labels)\n",
    "        except Exception:\n",
    "            explanations = [\"\"] * len(labels)\n",
    "        return labels, explanations\n",
    "\n",
    "# Instantiate a module wrapper for batch processing in evaluation\n",
    "paradigm_predictor = ParadigmPredictor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "316c369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization metric: per-paradigm accuracy + consistency\n",
    "\n",
    "def metric_paradigm(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:\n",
    "    gold = parse_labels_json(example.labels_json)\n",
    "    preds = parse_labels_json(getattr(pred, \"labels_json\", \"[]\"))\n",
    "    if not gold or not preds or len(gold) != len(preds):\n",
    "        return 0.0\n",
    "    return combined_reward(gold, preds, alpha=0.75)\n",
    "\n",
    "# Student module for DSPy compile: produces labels_json from pairs_json\n",
    "class StudentParadigmModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inner = dspy.ChainOfThought(ParadigmNLISignature)\n",
    "    \n",
    "    def forward(self, pairs_json: str) -> dspy.Prediction:\n",
    "        pred = self.inner(pairs_json=pairs_json)\n",
    "        # Ensure labels_json is present; if missing, attempt prompt-robust fallback\n",
    "        labels = parse_labels_json(getattr(pred, \"labels_json\", \"[]\"))\n",
    "        if not labels:\n",
    "            try:\n",
    "                # Last-resort JSON parse from any free-text field\n",
    "                labels = parse_labels_json(getattr(pred, \"labels_json\", \"[]\"))\n",
    "            except Exception:\n",
    "                labels = []\n",
    "        return dspy.Prediction(labels_json=safe_json_dumps(labels), explanations_json=getattr(pred, \"explanations_json\", \"[]\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80b368db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 00:17:35 INFO dspy.teleprompt.copro_optimizer: Iteration Depth: 1/2.\n",
      "2025/08/11 00:17:35 INFO dspy.teleprompt.copro_optimizer: At Depth 1/2, Evaluating Prompt Candidate #1/3 for Predictor 1 of 1.\n",
      "2025/08/11 00:25:05 INFO dspy.evaluate.evaluate: Average Metric: 20.4281067251462 / 45 (45.4%)\n",
      "2025/08/11 00:25:05 INFO dspy.teleprompt.copro_optimizer: At Depth 1/2, Evaluating Prompt Candidate #2/3 for Predictor 1 of 1.\n",
      "2025/08/11 00:36:03 INFO dspy.evaluate.evaluate: Average Metric: 31.61483918128655 / 45 (70.3%)\n",
      "2025/08/11 00:36:03 INFO dspy.teleprompt.copro_optimizer: At Depth 1/2, Evaluating Prompt Candidate #3/3 for Predictor 1 of 1.\n",
      "2025/08/11 00:43:18 INFO dspy.evaluate.evaluate: Average Metric: 29.873062865497065 / 45 (66.4%)\n",
      "2025/08/11 00:45:42 INFO dspy.teleprompt.copro_optimizer: Iteration Depth: 2/2.\n",
      "2025/08/11 00:45:42 INFO dspy.teleprompt.copro_optimizer: At Depth 2/2, Evaluating Prompt Candidate #1/3 for Predictor 1 of 1.\n",
      "2025/08/11 01:13:48 INFO dspy.evaluate.evaluate: Average Metric: 26.74440789473684 / 45 (59.4%)\n",
      "2025/08/11 01:13:48 INFO dspy.teleprompt.copro_optimizer: At Depth 2/2, Evaluating Prompt Candidate #2/3 for Predictor 1 of 1.\n",
      "2025/08/11 01:16:26 INFO dspy.evaluate.evaluate: Average Metric: 19.26315789473684 / 45 (42.8%)\n",
      "2025/08/11 01:16:26 INFO dspy.teleprompt.copro_optimizer: At Depth 2/2, Evaluating Prompt Candidate #3/3 for Predictor 1 of 1.\n",
      "2025/08/11 01:19:10 INFO dspy.evaluate.evaluate: Average Metric: 18.493384502923973 / 45 (41.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inner.predict = Predict(StringSignature(pairs_json -> reasoning, labels_json, explanations_json\n",
       "    instructions='You are an expert in Natural Language Inference (NLI). Your task is to process a JSON-encoded list of pairs, where each pair contains a \"premise\" and a \"hypothesis\". For each pair, determine the relationship between the premise and the hypothesis by selecting one of the following labels: \"entailment\" (the premise logically implies the hypothesis), \"neutral\" (the premise is unrelated or neither supports nor contradicts the hypothesis), or \"contradiction\" (the premise directly conflicts with the hypothesis). Provide a concise, one-sentence explanation for each label to justify your decision. Ensure your output is structured as a JSON-encoded list of objects, where each object includes the fields \"label\" and \"explanation\", corresponding to the input pairs. Focus on accuracy, logical reasoning, and brevity to make your response clear and easy to parse.'\n",
       "    pairs_json = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Pairs Json:', 'desc': '${pairs_json}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    labels_json = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Labels Json:', 'desc': '${labels_json}'})\n",
       "    explanations_json = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Based on the provided input JSON list of pairs, generate the output as a JSON-encoded list:', 'desc': '${explanations_json}'})\n",
       "))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile with DSPy using a small train/eval split of paradigms\n",
    "from dspy.teleprompt import COPRO\n",
    "\n",
    "# Keep budget small; sample paradigms across sections\n",
    "random.seed(123)\n",
    "all_indices = list(range(len(paradigm_examples)))\n",
    "random.shuffle(all_indices)\n",
    "train_k = min(60, int(0.05 * len(all_indices)))  # ~5% of paradigms\n",
    "trainset = [paradigm_examples[i] for i in all_indices[:train_k]]\n",
    "evalset = [paradigm_examples[i] for i in all_indices[train_k:train_k + 2 * train_k]]\n",
    "\n",
    "student = StudentParadigmModule()\n",
    "tele = COPRO(metric=metric_paradigm, max_trials=4, depth=2, breadth=3)\n",
    "optimized_student = tele.compile(student=student, trainset=trainset, eval_kwargs={})\n",
    "\n",
    "optimized_student\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: per-section and per-transformation metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_paradigm_with_model(model: dspy.Module, pairs: list[dict]) -> list[int]:\n",
    "    pred = model(pairs_json=safe_json_dumps(pairs))\n",
    "    return parse_labels_json(getattr(pred, \"labels_json\", \"[]\"))\n",
    "\n",
    "\n",
    "# Build reverse index to the raw rows to recover transformation types (pairID mod 19 often maps to type)\n",
    "# We will report by section and by transformation index t in [0..18]\n",
    "section_pid_to_rows = defaultdict(dict)\n",
    "for section, ds in raw.items():\n",
    "    for row in ds:\n",
    "        section_pid_to_rows[section].setdefault(row[\"paradigmID\"], []).append(row)\n",
    "\n",
    "# Evaluate\n",
    "results_by_section = {}\n",
    "results_by_transform = {t: {\"correct\": 0, \"total\": 0} for t in range(19)}\n",
    "\n",
    "for section, indices in section_index.items():\n",
    "    accs = []\n",
    "    conss = []\n",
    "    totals = 0\n",
    "    corrects = 0\n",
    "    for idx in indices[:]:\n",
    "        ex = paradigm_examples[idx]\n",
    "        pairs = json.loads(ex.pairs_json)\n",
    "        gold = parse_labels_json(ex.labels_json)\n",
    "        preds = predict_paradigm_with_model(optimized_student, pairs)\n",
    "        if not preds or len(preds) != len(gold):\n",
    "            continue\n",
    "        acc = compute_paradigm_accuracy(gold, preds)\n",
    "        cons = compute_paradigm_consistency(gold, preds)\n",
    "        accs.append(acc)\n",
    "        conss.append(cons)\n",
    "        totals += len(gold)\n",
    "        corrects += sum(int(g == p) for g, p in zip(gold, preds))\n",
    "        \n",
    "        # Per-transformation report: best effort via original order using pairID % 19\n",
    "        # If pairID is present and behaves as index, aggregate accuracy per transform\n",
    "        for row in section_pid_to_rows[section][int(ex.paradigm_id)]:\n",
    "            try:\n",
    "                t = int(row[\"pairID\"]) % 19\n",
    "            except Exception:\n",
    "                t = None\n",
    "            # match corresponding prediction by (premise,hypothesis)\n",
    "            try:\n",
    "                i = next(i for i, pr in enumerate(pairs) if pr[\"premise\"] == row[\"premise\"] and pr[\"hypothesis\"] == row[\"hypothesis\"]) \n",
    "                if t is not None:\n",
    "                    results_by_transform[t][\"total\"] += 1\n",
    "                    results_by_transform[t][\"correct\"] += int(preds[i] == int(row[\"gold_label\"]))\n",
    "            except StopIteration:\n",
    "                pass\n",
    "    if accs:\n",
    "        results_by_section[section] = {\n",
    "            \"accuracy\": float(np.mean(accs)),\n",
    "            \"consistency\": float(np.mean(conss)),\n",
    "        }\n",
    "\n",
    "results_by_section, {t: (v[\"correct\"] / v[\"total\"] if v[\"total\"] else None) for t, v in results_by_transform.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ddc7e",
   "metadata": {},
   "source": [
    "\n",
    "## Why this design\n",
    "\n",
    "- We group by `paradigmID` and shuffle items within each paradigm to prevent position leakage and to compute a meaningful reward over the whole paradigm.\n",
    "- The optimization metric is a weighted combination of accuracy and intra-paradigm consistency, aligning with the assignment. Consistency is measured via majority-coherence within gold-label clusters.\n",
    "- We use a JSON-IO `Signature` so the model returns aligned lists of labels and explanations; this reduces parsing errors for batched predictions.\n",
    "- We adopt a CoT predictor to elicit short explanations that often stabilize classification, with a fallback to per-item CoT if the batched JSON output is malformed.\n",
    "- During evaluation, we report per-section averages for accuracy and consistency, and approximate er-transformation accuracy using `pairID % 19` mapping to transformation indices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
