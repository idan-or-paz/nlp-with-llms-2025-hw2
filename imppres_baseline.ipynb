{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres Baseline\n",
    "\n",
    "This notebook illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ImpPres dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': DatasetDict({\n",
       "     all_n_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_both_presupposition': DatasetDict({\n",
       "     both_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_change_of_state': DatasetDict({\n",
       "     change_of_state: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_existence': DatasetDict({\n",
       "     cleft_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': DatasetDict({\n",
       "     cleft_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_only_presupposition': DatasetDict({\n",
       "     only_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': DatasetDict({\n",
       "     possessed_definites_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': DatasetDict({\n",
       "     possessed_definites_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_question_presupposition': DatasetDict({\n",
       "     question_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ImpPres dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['gold_label']],\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as eval_lib\n",
    "\n",
    "accuracy = eval_lib.load(\"accuracy\")\n",
    "precision = eval_lib.load(\"precision\")\n",
    "recall = eval_lib.load(\"recall\")\n",
    "f1 = eval_lib.load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = eval_lib.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ImpPres dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e53914fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š TASK 2.2: EVALUATING DEBERTA BASELINE ON IMPPRES PRESUPPOSITION DATA\n",
      "================================================================================\n",
      "ðŸ“‹ Model: DeBERTa-v3-base-mnli-fever-anli\n",
      "ðŸ“‹ Sections: 9 presupposition types\n",
      "ðŸ“‹ Metrics: Accuracy, Precision, Recall, F1 (using evaluate package)\n",
      "================================================================================\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_all_n_presupposition...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [02:40<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_all_n_presupposition:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.463\n",
      "- F1 (macro): 0.420\n",
      "- Precision (macro): 0.433\n",
      "- Recall (macro): 0.475\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_both_presupposition...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [02:43<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_both_presupposition:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.397\n",
      "- F1 (macro): 0.316\n",
      "- Precision (macro): 0.275\n",
      "- Recall (macro): 0.394\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_change_of_state...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [02:45<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_change_of_state:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.308\n",
      "- F1 (macro): 0.316\n",
      "- Precision (macro): 0.334\n",
      "- Recall (macro): 0.324\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_cleft_existence...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [02:43<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_cleft_existence:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.641\n",
      "- F1 (macro): 0.630\n",
      "- Precision (macro): 0.678\n",
      "- Recall (macro): 0.694\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_cleft_uniqueness...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [02:39<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_cleft_uniqueness:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.195\n",
      "- F1 (macro): 0.191\n",
      "- Precision (macro): 0.214\n",
      "- Recall (macro): 0.186\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_only_presupposition...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [07:55<00:00,  4.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_only_presupposition:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.583\n",
      "- F1 (macro): 0.565\n",
      "- Precision (macro): 0.646\n",
      "- Recall (macro): 0.640\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_possessed_definites_existence...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [10:03<00:00,  3.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_possessed_definites_existence:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.671\n",
      "- F1 (macro): 0.664\n",
      "- Precision (macro): 0.799\n",
      "- Recall (macro): 0.737\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_possessed_definites_uniqueness...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [17:53<00:00,  1.77it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_possessed_definites_uniqueness:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.390\n",
      "- F1 (macro): 0.293\n",
      "- Precision (macro): 0.252\n",
      "- Recall (macro): 0.383\n",
      "\n",
      "ðŸ”„ Evaluating on presupposition_question_presupposition...\n",
      "Number of samples: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [18:22<00:00,  1.72it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Results for presupposition_question_presupposition:\n",
      "- Samples: 1900\n",
      "- Accuracy: 0.625\n",
      "- F1 (macro): 0.599\n",
      "- Precision (macro): 0.699\n",
      "- Recall (macro): 0.695\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPUTING OVERALL METRICS ACROSS ALL SECTIONS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“Š TASK 2.2: EVALUATING DEBERTA BASELINE ON IMPPRES PRESUPPOSITION DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“‹ Model: DeBERTa-v3-base-mnli-fever-anli\")\n",
    "print(\"ðŸ“‹ Sections: 9 presupposition types\")\n",
    "print(\"ðŸ“‹ Metrics: Accuracy, Precision, Recall, F1 (using evaluate package)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_metrics_on_section(dataset_section, section_name):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for a specific presupposition section\n",
    "    \"\"\"\n",
    "    # Get the actual split name for this section\n",
    "    split_name = list(dataset_section.keys())[0]\n",
    "    section_data = dataset_section[split_name]\n",
    "    \n",
    "    print(f\"\\nðŸ”„ Evaluating on {section_name}...\")\n",
    "    print(f\"Number of samples: {len(section_data)}\")\n",
    "    \n",
    "    # Run evaluation on the section\n",
    "    predictions = evaluate_on_dataset(section_data)\n",
    "    \n",
    "    # Extract predictions and gold labels\n",
    "    pred_labels = [p['pred_label'] for p in predictions]\n",
    "    gold_labels = [p['gold_label'] for p in predictions]\n",
    "    \n",
    "    # Map labels to integers for metrics computation\n",
    "    label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    pred_ints = [label_to_int[label] for label in pred_labels]\n",
    "    gold_ints = [label_to_int[label] for label in gold_labels]\n",
    "    \n",
    "    # Compute metrics using evaluate package with macro averaging for multiclass\n",
    "    accuracy_result = accuracy.compute(predictions=pred_ints, references=gold_ints)\n",
    "    f1_result = f1.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    precision_result = precision.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    recall_result = recall.compute(predictions=pred_ints, references=gold_ints, average='macro')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_result['accuracy'],\n",
    "        'f1': f1_result['f1'],\n",
    "        'precision': precision_result['precision'],\n",
    "        'recall': recall_result['recall']\n",
    "    }\n",
    "    \n",
    "    # Add section information\n",
    "    metrics['section'] = section_name\n",
    "    metrics['num_samples'] = len(predictions)\n",
    "    \n",
    "    print(f\"ðŸ“Š Results for {section_name}:\")\n",
    "    print(f\"- Samples: {metrics['num_samples']}\")\n",
    "    print(f\"- Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"- F1 (macro): {metrics['f1']:.3f}\")\n",
    "    print(f\"- Precision (macro): {metrics['precision']:.3f}\")\n",
    "    print(f\"- Recall (macro): {metrics['recall']:.3f}\")\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "# Evaluate each presupposition section\n",
    "all_results = {}\n",
    "all_predictions = []\n",
    "\n",
    "for section in sections:\n",
    "    metrics, predictions = compute_metrics_on_section(dataset[section], section)\n",
    "    all_results[section] = metrics\n",
    "    all_predictions.extend(predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š COMPUTING OVERALL METRICS ACROSS ALL SECTIONS\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18372c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall metrics across all sections\n",
    "print(f\"Computing overall metrics on {len(all_predictions)} total samples...\")\n",
    "\n",
    "# Extract overall predictions and gold labels\n",
    "overall_pred_labels = [p['pred_label'] for p in all_predictions]\n",
    "overall_gold_labels = [p['gold_label'] for p in all_predictions]\n",
    "\n",
    "# Map labels to integers\n",
    "label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "overall_pred_ints = [label_to_int[label] for label in overall_pred_labels]\n",
    "overall_gold_ints = [label_to_int[label] for label in overall_gold_labels]\n",
    "\n",
    "# Compute overall metrics with macro averaging for multiclass\n",
    "overall_accuracy_result = accuracy.compute(predictions=overall_pred_ints, references=overall_gold_ints)\n",
    "overall_f1_result = f1.compute(predictions=overall_pred_ints, references=overall_gold_ints, average='macro')\n",
    "overall_precision_result = precision.compute(predictions=overall_pred_ints, references=overall_gold_ints, average='macro')\n",
    "overall_recall_result = recall.compute(predictions=overall_pred_ints, references=overall_gold_ints, average='macro')\n",
    "\n",
    "overall_metrics = {\n",
    "    'accuracy': overall_accuracy_result['accuracy'],\n",
    "    'f1': overall_f1_result['f1'],\n",
    "    'precision': overall_precision_result['precision'],\n",
    "    'recall': overall_recall_result['recall']\n",
    "}\n",
    "overall_metrics['section'] = 'ALL_SECTIONS'\n",
    "overall_metrics['num_samples'] = len(all_predictions)\n",
    "\n",
    "print(f\"ðŸ“Š Overall Results across all presupposition sections:\")\n",
    "print(f\"- Total Samples: {overall_metrics['num_samples']}\")\n",
    "print(f\"- Accuracy: {overall_metrics['accuracy']:.3f}\")\n",
    "print(f\"- F1 (macro): {overall_metrics['f1']:.3f}\")\n",
    "print(f\"- Precision (macro): {overall_metrics['precision']:.3f}\")\n",
    "print(f\"- Recall (macro): {overall_metrics['recall']:.3f}\")\n",
    "\n",
    "# Add overall results to the results dictionary\n",
    "all_results['ALL_SECTIONS'] = overall_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“‹ SUMMARY TABLE: DEBERTA BASELINE PERFORMANCE ON IMPPRES PRESUPPOSITIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create results table\n",
    "print(f\"{'Section':<45} {'Samples':<8} {'Accuracy':<9} {'F1':<6} {'Precision':<10} {'Recall':<6}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Print individual section results\n",
    "for section in sections:\n",
    "    r = all_results[section]\n",
    "    print(f\"{section:<45} {r['num_samples']:<8} {r['accuracy']:<9.3f} {r['f1']:<6.3f} {r['precision']:<10.3f} {r['recall']:<6.3f}\")\n",
    "\n",
    "# Print overall results\n",
    "print(\"-\" * 90)\n",
    "r = all_results['ALL_SECTIONS']\n",
    "print(f\"{'ALL_SECTIONS':<45} {r['num_samples']:<8} {r['accuracy']:<9.3f} {r['f1']:<6.3f} {r['precision']:<10.3f} {r['recall']:<6.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… TASK 2.2 COMPLETED\")\n",
    "print(\"ðŸ“Š DeBERTa baseline evaluation completed on all 9 presupposition sections\")\n",
    "print(\"ðŸ“‹ Results show performance metrics for each section and overall performance\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
