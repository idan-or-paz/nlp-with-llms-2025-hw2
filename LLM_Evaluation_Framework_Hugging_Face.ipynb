{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "LLM Evaluation Framework - Hugging Face",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-10T19:12:23.502463Z",
          "iopub.execute_input": "2024-05-10T19:12:23.50369Z",
          "iopub.status.idle": "2024-05-10T19:12:23.511002Z",
          "shell.execute_reply.started": "2024-05-10T19:12:23.503643Z",
          "shell.execute_reply": "2024-05-10T19:12:23.510005Z"
        },
        "trusted": true,
        "id": "kX-HU8_MpiGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Evaluation Framework**\n",
        "This notebook describes how you build LLM Evaluation Framework using **\"EVALUATE\"** Library in Hugging Face.\n",
        "![](https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png)\n",
        "\n",
        "A library for easily evaluating machine learning models and datasets.\n",
        "\n",
        "With a single line of code, you get access to dozens of evaluation methods for different domains. Be it on your local machine or in a distributed training setup, you can evaluate your models in a consistent and reproducible way!\n"
      ],
      "metadata": {
        "id": "1UnShyncpiGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:13:16.391354Z",
          "iopub.execute_input": "2024-05-10T19:13:16.392109Z",
          "iopub.status.idle": "2024-05-10T19:13:41.330706Z",
          "shell.execute_reply.started": "2024-05-10T19:13:16.392073Z",
          "shell.execute_reply": "2024-05-10T19:13:41.329286Z"
        },
        "trusted": true,
        "id": "l7MYE2dipiGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:13:41.333063Z",
          "iopub.execute_input": "2024-05-10T19:13:41.333661Z",
          "iopub.status.idle": "2024-05-10T19:13:41.343136Z",
          "shell.execute_reply.started": "2024-05-10T19:13:41.33362Z",
          "shell.execute_reply": "2024-05-10T19:13:41.34207Z"
        },
        "trusted": true,
        "id": "dBZXDEeYpiGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Evaluate"
      ],
      "metadata": {
        "id": "5o_M_NvKpiGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hell']))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:13:41.344303Z",
          "iopub.execute_input": "2024-05-10T19:13:41.344597Z",
          "iopub.status.idle": "2024-05-10T19:14:02.052255Z",
          "shell.execute_reply.started": "2024-05-10T19:13:41.344571Z",
          "shell.execute_reply": "2024-05-10T19:14:02.051206Z"
        },
        "trusted": true,
        "id": "5Adtyvs5piGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of evaluations\n",
        "\n",
        "There are different aspects of a typical machine learning pipeline that can be evaluated and for each aspect 🤗 Evaluate provides a tool:\n",
        "\n",
        "- **Metric**: A metric is used to evaluate a model’s performance and usually involves the model’s predictions as well as some ground truth labels. You can find all integrated metrics at evaluate-metric.\n",
        "- **Comparison**: A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at evaluate-comparison.\n",
        "- **Measurement**: The dataset is as important as the model trained on it. With measurements one can investigate a dataset’s properties. You can find all integrated measurements at evaluate-measurement.\n",
        "\n",
        "Each metric, comparison, and measurement is a separate Python module, but for using any of them, there is a single entry point: **evaluate.load()**\n",
        "\n",
        "![](https://i1.wp.com/dataaspirant.com/wp-content/uploads/2020/08/2_6_classification_evaluation_metrics.png?resize=1536%2C1099&ssl=1)"
      ],
      "metadata": {
        "id": "Gs1feICopiGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "word_length = evaluate.load(\"word_length\", module_type=\"measurement\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:02.054227Z",
          "iopub.execute_input": "2024-05-10T19:14:02.054538Z",
          "iopub.status.idle": "2024-05-10T19:14:03.942169Z",
          "shell.execute_reply.started": "2024-05-10T19:14:02.054512Z",
          "shell.execute_reply": "2024-05-10T19:14:03.941204Z"
        },
        "trusted": true,
        "id": "MfMhTvr7piGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"hello world\"]\n",
        "results = word_length.compute(data=data)\n",
        "print(results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:03.943586Z",
          "iopub.execute_input": "2024-05-10T19:14:03.944336Z",
          "iopub.status.idle": "2024-05-10T19:14:03.965991Z",
          "shell.execute_reply.started": "2024-05-10T19:14:03.944282Z",
          "shell.execute_reply": "2024-05-10T19:14:03.965122Z"
        },
        "trusted": true,
        "id": "M9NKNVaDpiG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:03.966994Z",
          "iopub.execute_input": "2024-05-10T19:14:03.967273Z",
          "iopub.status.idle": "2024-05-10T19:14:03.989341Z",
          "shell.execute_reply.started": "2024-05-10T19:14:03.967249Z",
          "shell.execute_reply": "2024-05-10T19:14:03.988453Z"
        },
        "trusted": true,
        "id": "rVXKQvyzpiG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you have model Actual Results and Predictions, you can calculate the Accuracy as below-"
      ],
      "metadata": {
        "id": "PLxQST3wpiG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
        "    accuracy.add_batch(references=refs, predictions=preds)\n",
        "accuracy.compute()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:03.990491Z",
          "iopub.execute_input": "2024-05-10T19:14:03.990783Z",
          "iopub.status.idle": "2024-05-10T19:14:04.004714Z",
          "shell.execute_reply.started": "2024-05-10T19:14:03.990758Z",
          "shell.execute_reply": "2024-05-10T19:14:04.003662Z"
        },
        "trusted": true,
        "id": "JWS9jOmipiG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining several evaluations"
      ],
      "metadata": {
        "id": "kh5u0qA6piG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:04.428579Z",
          "iopub.execute_input": "2024-05-10T19:14:04.429462Z",
          "iopub.status.idle": "2024-05-10T19:14:05.987332Z",
          "shell.execute_reply.started": "2024-05-10T19:14:04.429425Z",
          "shell.execute_reply": "2024-05-10T19:14:05.986602Z"
        },
        "trusted": true,
        "id": "iBG85dF8piG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:10.577974Z",
          "iopub.execute_input": "2024-05-10T19:14:10.578711Z",
          "iopub.status.idle": "2024-05-10T19:14:10.618387Z",
          "shell.execute_reply.started": "2024-05-10T19:14:10.57868Z",
          "shell.execute_reply": "2024-05-10T19:14:10.617347Z"
        },
        "trusted": true,
        "id": "6D4INI55piG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Result in File"
      ],
      "metadata": {
        "id": "jqibCnAQpiG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])\n",
        "\n",
        "hyperparams = {\"model\": \"bert-base-uncased\"}\n",
        "evaluate.save(\"/kaggle/working/\",experiment=\"run 42\", **result, **hyperparams)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:12.520394Z",
          "iopub.execute_input": "2024-05-10T19:14:12.521153Z",
          "iopub.status.idle": "2024-05-10T19:14:12.547751Z",
          "shell.execute_reply.started": "2024-05-10T19:14:12.521116Z",
          "shell.execute_reply": "2024-05-10T19:14:12.546211Z"
        },
        "trusted": true,
        "id": "GqCv0UGupiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "import evaluate\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\", device=0)\n",
        "data = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000))\n",
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:13.765373Z",
          "iopub.execute_input": "2024-05-10T19:14:13.766217Z",
          "iopub.status.idle": "2024-05-10T19:14:28.18666Z",
          "shell.execute_reply.started": "2024-05-10T19:14:13.766179Z",
          "shell.execute_reply": "2024-05-10T19:14:28.185849Z"
        },
        "trusted": true,
        "id": "6lKVUHpDpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_evaluator = evaluator(\"text-classification\")\n",
        "results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,\n",
        "                       label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1},)\n",
        "print(results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:14:43.356311Z",
          "iopub.execute_input": "2024-05-10T19:14:43.356697Z",
          "iopub.status.idle": "2024-05-10T19:14:54.550169Z",
          "shell.execute_reply.started": "2024-05-10T19:14:43.356666Z",
          "shell.execute_reply": "2024-05-10T19:14:54.549147Z"
        },
        "trusted": true,
        "id": "YZ7MgczcpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the value of the metric alone is often not enough to know if a model performs significantly better than another one. With bootstrapping evaluate computes confidence intervals and the standard error which helps estimate how stable a score is:"
      ],
      "metadata": {
        "id": "zLwty_tPpiG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,\n",
        "                       label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
        "                       strategy=\"bootstrap\", n_resamples=200)\n",
        "print(results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:31.482128Z",
          "iopub.execute_input": "2024-05-10T19:15:31.482519Z",
          "iopub.status.idle": "2024-05-10T19:15:57.505098Z",
          "shell.execute_reply.started": "2024-05-10T19:15:31.482492Z",
          "shell.execute_reply": "2024-05-10T19:15:57.503995Z"
        },
        "trusted": true,
        "id": "HgBF_i4-piG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluator expects a \"text\" and \"label\" column for the data input. If your dataset differs you can provide the columns with the keywords input_column=\"text\" and label_column=\"label\". Currently only \"text-classification\" is supported with more tasks being added in the future."
      ],
      "metadata": {
        "id": "odYOqFndpiG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the Evaluation of Different Models"
      ],
      "metadata": {
        "id": "Oljg215RpiG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from evaluate.visualization import radar_plot\n",
        "\n",
        "data = [\n",
        "   {\"accuracy\": 0.99, \"precision\": 0.8, \"f1\": 0.95, \"latency_in_seconds\": 33.6},\n",
        "   {\"accuracy\": 0.98, \"precision\": 0.87, \"f1\": 0.91, \"latency_in_seconds\": 11.2},\n",
        "   {\"accuracy\": 0.98, \"precision\": 0.78, \"f1\": 0.88, \"latency_in_seconds\": 87.6},\n",
        "   {\"accuracy\": 0.88, \"precision\": 0.78, \"f1\": 0.81, \"latency_in_seconds\": 101.6}\n",
        "   ]\n",
        "model_names = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"]\n",
        "plot = radar_plot(data=data, model_names=model_names)\n",
        "plot.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:57.507063Z",
          "iopub.execute_input": "2024-05-10T19:15:57.507406Z",
          "iopub.status.idle": "2024-05-10T19:15:58.371385Z",
          "shell.execute_reply.started": "2024-05-10T19:15:57.507375Z",
          "shell.execute_reply": "2024-05-10T19:15:58.369415Z"
        },
        "trusted": true,
        "id": "ASa-yjpPpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing a Metric for your task\n",
        "So you’ve trained your model and want to see how well it’s doing on a dataset of your choice. Where do you start?\n",
        "\n",
        "There is no “one size fits all” approach to choosing an evaluation metric, but some good guidelines to keep in mind are:\n",
        "\n",
        "![](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/how-to-evaluate-llm-blog/dimensions_of_llm_evaluations.webp)"
      ],
      "metadata": {
        "id": "-kh3g2r9piG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categories of Metrics\n",
        "There are 3 high-level categories of metrics:\n",
        "\n",
        "- **Generic metrics**, which can be applied to a variety of situations and datasets, such as precision and accuracy.\n",
        "- **Task-specific metrics**, which are limited to a given task, such as Machine Translation (often evaluated using metrics BLEU or ROUGE) or Named Entity Recognition (often evaluated with seqeval).\n",
        "- **Dataset-specific metrics**, which aim to measure model performance on specific benchmarks: for instance, the GLUE benchmark has a dedicated evaluation metric."
      ],
      "metadata": {
        "id": "X4Swtr7QpiG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic metrics\n",
        "Many of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets.\n",
        "\n",
        "This is the case for metrics like accuracy and precision, which can be used for evaluating labeled (supervised) datasets, as well as perplexity, which can be used for evaluating different kinds of (unsupervised) generative tasks."
      ],
      "metadata": {
        "id": "qfFHfDi2piG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision_metric = evaluate.load(\"precision\")\n",
        "results = precision_metric.compute(references=[0, 1], predictions=[0, 1])\n",
        "print(results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:58.373159Z",
          "iopub.execute_input": "2024-05-10T19:15:58.373686Z",
          "iopub.status.idle": "2024-05-10T19:15:58.725128Z",
          "shell.execute_reply.started": "2024-05-10T19:15:58.373645Z",
          "shell.execute_reply": "2024-05-10T19:15:58.724194Z"
        },
        "trusted": true,
        "id": "zt-KDdCvpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-specific metrics\n",
        "Popular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models.\n",
        "\n",
        "- For example, a series of different metrics have been proposed for text generation, ranging from BLEU and its derivatives such as GoogleBLEU and GLEU, but also ROUGE, MAUVE, etc.\n",
        "\n",
        "You can find the right metric for your task by:\n",
        "\n",
        "- Looking at the Task pages to see what metrics can be used for evaluating models for a given task.\n",
        "- Checking out leaderboards on sites like Papers With Code (you can search by task and by dataset).\n",
        "- Reading the metric cards for the relevant metrics and see which ones are a good fit for your use case. For example, see the BLEU metric card or SQuaD metric card.\n",
        "- Looking at papers and blog posts published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!"
      ],
      "metadata": {
        "id": "WRnln-ImpiG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_machine_translation(hypotheses, references):\n",
        "    \"\"\"\n",
        "    Calculates BLEU score for machine translation evaluation.\n",
        "    Args:\n",
        "    hypotheses (List[str]): List of translated sentences.\n",
        "    references (List[List[str]]): List of reference translations for each input sentence.\n",
        "\n",
        "    Returns:\n",
        "    float: BLEU score.\n",
        "    \"\"\"\n",
        "    # Use HF Eval's `load` function to get the BLEU evaluator\n",
        "    bleu_evaluator = evaluate.load(\"bleu\")\n",
        "\n",
        "    # Calculate BLEU score using the evaluator\n",
        "    results = bleu_evaluator.compute(predictions=hypotheses, references=references)\n",
        "\n",
        "    # Extract BLEU score (average across references)\n",
        "    bleu_score = results[\"bleu\"]\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:58.727408Z",
          "iopub.execute_input": "2024-05-10T19:15:58.727729Z",
          "iopub.status.idle": "2024-05-10T19:15:58.733912Z",
          "shell.execute_reply.started": "2024-05-10T19:15:58.7277Z",
          "shell.execute_reply": "2024-05-10T19:15:58.732905Z"
        },
        "trusted": true,
        "id": "-naPgbqSpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example hypotheses (translated sentences)\n",
        "hypotheses = [\"The cat sat on mat.\", \"The dog played in garden.\"]\n",
        "\n",
        "# Example references (reference translations for each input sentence)\n",
        "references = [[\"The cat sat on the mat.\"], [\"The dog played in the garden.\"]]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = evaluate_machine_translation(hypotheses, references)\n",
        "\n",
        "# Print the BLEU score\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:58.734824Z",
          "iopub.execute_input": "2024-05-10T19:15:58.735122Z",
          "iopub.status.idle": "2024-05-10T19:15:59.759065Z",
          "shell.execute_reply.started": "2024-05-10T19:15:58.735096Z",
          "shell.execute_reply": "2024-05-10T19:15:59.758235Z"
        },
        "trusted": true,
        "id": "NvY4M3w-piG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indicates the overall BLEU score. It reflects the cumulative n-gram precision of the generated translations or summaries compared to the reference translations or summaries, where n typically ranges from 1 to 4. A score of approximately 0.51 indicates that around 51.15% of the n-grams (typically up to 4-grams) in the generated translations or summaries match those in the reference translations or summaries."
      ],
      "metadata": {
        "id": "WCNn7AG_piG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_ner(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a Named Entity Recognition system.\n",
        "\n",
        "    Args:\n",
        "    true_labels (list of lists): True labels for each sentence in the dataset.\n",
        "    predicted_labels (list of lists): Predicted labels for each sentence in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    classification_report (str): Text summary of precision, recall, and F1 score for each class.\n",
        "    \"\"\"\n",
        "    # Flatten the lists of labels\n",
        "    true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
        "    predicted_labels_flat = [label for sublist in predicted_labels for label in sublist]\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(true_labels_flat, predicted_labels_flat)\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:59.760171Z",
          "iopub.execute_input": "2024-05-10T19:15:59.76045Z",
          "iopub.status.idle": "2024-05-10T19:15:59.765972Z",
          "shell.execute_reply.started": "2024-05-10T19:15:59.760425Z",
          "shell.execute_reply": "2024-05-10T19:15:59.765012Z"
        },
        "trusted": true,
        "id": "a1cRMHFFpiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = [['N', 'B-PER', 'I-PER', 'O'], ['B-LOC', 'I-LOC', 'N']]\n",
        "predicted_labels = [['O', 'B-PER', 'I-PER', 'O'], ['B-LOC', 'I-LOC', 'O']]\n",
        "\n",
        "evaluation_result = evaluate_ner(true_labels, predicted_labels)\n",
        "print(evaluation_result)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:59.767253Z",
          "iopub.execute_input": "2024-05-10T19:15:59.767885Z",
          "iopub.status.idle": "2024-05-10T19:15:59.788764Z",
          "shell.execute_reply.started": "2024-05-10T19:15:59.767839Z",
          "shell.execute_reply": "2024-05-10T19:15:59.787823Z"
        },
        "trusted": true,
        "id": "JwjmDPH9piG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def simple_summarizer(text):\n",
        "    sentences = text.split(\".\")\n",
        "    return sentences[0]\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge_scorer = evaluate.load(\"rouge\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"Today is a beautiful day. The sun is shining and the birds are singing. I am going for a walk in the park.\"\n",
        "reference = \"The weather is pleasant today.\"\n",
        "\n",
        "# Generate summary using the function\n",
        "prediction = simple_summarizer(text)\n",
        "\n",
        "# Compute ROUGE score\n",
        "rouge_results = rouge_scorer.compute(predictions=[prediction], references=[reference])\n",
        "\n",
        "# Print ROUGE score (might be very low due to the simplistic summarizer)\n",
        "print(\"ROUGE-L score:\", rouge_results[\"rougeL\"])\n",
        "print(\"ROUGE-1 score:\", rouge_results[\"rouge1\"])\n",
        "print(\"ROUGE-2 score:\", rouge_results[\"rouge2\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:59.790075Z",
          "iopub.execute_input": "2024-05-10T19:15:59.790647Z",
          "iopub.status.idle": "2024-05-10T19:16:01.235031Z",
          "shell.execute_reply.started": "2024-05-10T19:15:59.790621Z",
          "shell.execute_reply": "2024-05-10T19:16:01.233991Z"
        },
        "trusted": true,
        "id": "ACRc7IEUpiG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ROUGE-L score:This indicates the ROUGE-L score, which measures the overlap of Longest Common Subsequences (LCS) between the generated summaries and the reference summaries. The score of 0.20000000000000004 suggests that approximately 20% of the content in the generated summaries matches the content in the reference summaries, but there's significant room for improvement.\n",
        "\n",
        "- ROUGE-1 score: This is the ROUGE-1 score, which measures the overlap of unigrams (individual words) between the generated summaries and the reference summaries. The score of 0.4000000000000001 suggests that approximately 40% of the unigrams in the generated summaries match those in the reference summaries.\n",
        "\n",
        "- ROUGE-2 score: This is the ROUGE-2 score, which measures the overlap of bigrams (pairs of adjacent words) between the generated summaries and the reference summaries. The score of 0.0 suggests that there is no overlap of bigrams between the generated and reference summaries, indicating that the generated summaries did not contain any two-word sequences that matched those in the reference summaries."
      ],
      "metadata": {
        "id": "BNo5u4CFpiG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset-specific metrics\n",
        "Some datasets have specific metrics associated with them — this is especially in the case of popular benchmarks like GLUE and SQuAD.\n",
        "\n",
        "**GLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the \"Natural Language Inference\" NLI task, such as mnli, which is described as “crowdsourced collection of sentence pairs with textual entailment annotations”**\n"
      ],
      "metadata": {
        "id": "R1IvwbcMpiG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad\")\n",
        "predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
        "references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
        "results = squad_metric.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:16:01.236281Z",
          "iopub.execute_input": "2024-05-10T19:16:01.236641Z",
          "iopub.status.idle": "2024-05-10T19:16:01.842575Z",
          "shell.execute_reply.started": "2024-05-10T19:16:01.23661Z",
          "shell.execute_reply": "2024-05-10T19:16:01.841679Z"
        },
        "trusted": true,
        "id": "cUo0Nw75piG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Evaluator\n",
        "The Evaluator classes allow to evaluate a triplet of model, dataset, and metric. The models wrapped in a pipeline, responsible for handling all preprocessing and post-processing and out-of-the-box, Evaluators support transformers pipelines for the supported tasks."
      ],
      "metadata": {
        "id": "FZCn8sgXpiG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "from transformers import AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\n",
        "task_evaluator = evaluator(\"text-classification\")\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
        "\n",
        "eval_results = task_evaluator.compute(\n",
        "    model_or_pipeline=pipe,\n",
        "    data=data,\n",
        "    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        ")\n",
        "print(eval_results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:16:01.84498Z",
          "iopub.execute_input": "2024-05-10T19:16:01.84526Z",
          "iopub.status.idle": "2024-05-10T19:18:41.119264Z",
          "shell.execute_reply.started": "2024-05-10T19:16:01.845235Z",
          "shell.execute_reply": "2024-05-10T19:18:41.118182Z"
        },
        "trusted": true,
        "id": "FAwz028BpiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Evaluation Suit on GPT2 LLM"
      ],
      "metadata": {
        "id": "hzCjcVhApiG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import EvaluationSuite\n",
        "suite = EvaluationSuite.load('mathemakitten/glue-evaluation-suite')\n",
        "results = suite.run(\"gpt2\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:18:41.120771Z",
          "iopub.execute_input": "2024-05-10T19:18:41.121113Z",
          "iopub.status.idle": "2024-05-10T19:19:35.316882Z",
          "shell.execute_reply.started": "2024-05-10T19:18:41.121083Z",
          "shell.execute_reply": "2024-05-10T19:19:35.316066Z"
        },
        "trusted": true,
        "id": "OGAZIFdApiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:19:35.318627Z",
          "iopub.execute_input": "2024-05-10T19:19:35.318927Z",
          "iopub.status.idle": "2024-05-10T19:19:35.326547Z",
          "shell.execute_reply.started": "2024-05-10T19:19:35.318901Z",
          "shell.execute_reply": "2024-05-10T19:19:35.325616Z"
        },
        "trusted": true,
        "id": "bdC45Z7bpiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J92hmIW6piG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}